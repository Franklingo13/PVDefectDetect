{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuaderno de prueba para testear la capacidad de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías\n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.nn import DataParallel\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50, DeepLabV3_ResNet50_Weights\n",
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
    "from torchvision.utils import draw_segmentation_masks\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "import copy\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from imutils.paths import list_images\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar el manejador de modelo: ModelHandler\n",
    "from pv_vision.nn import ModelHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se definió una clase para el conjunto de datos solar, que hereda de la clase VisionDataset de PyTorch.\n",
    "class SolarDataset(VisionDataset):\n",
    "    \"\"\"Un conjunto de datos que lee directamente las imágenes y las máscaras desde una carpeta.\"\"\"\n",
    "    \n",
    "    # Se definió el método de inicialización para la clase.\n",
    "    def __init__(self, \n",
    "                 root, \n",
    "                 image_folder, \n",
    "                 mask_folder,\n",
    "                 transforms,\n",
    "                 mode = \"train\",\n",
    "                 random_seed=42):\n",
    "        # Se llamó al método de inicialización de la clase padre.\n",
    "        super().__init__(root, transforms)\n",
    "        # Se establecieron las rutas a las carpetas de imágenes y máscaras.\n",
    "        self.image_path = Path(self.root) / image_folder\n",
    "        self.mask_path = Path(self.root) / mask_folder\n",
    "\n",
    "        # Se verificó que las carpetas de imágenes y máscaras existan.\n",
    "        if not os.path.exists(self.image_path):\n",
    "            raise OSError(f\"{self.image_path} no encontrado.\")\n",
    "\n",
    "        if not os.path.exists(self.mask_path):\n",
    "            raise OSError(f\"{self.mask_path} no encontrado.\")\n",
    "\n",
    "        # Se obtuvieron las listas de imágenes y máscaras y se ordenaron.\n",
    "        self.image_list = sorted(list(list_images(self.image_path)))\n",
    "        self.mask_list = sorted(list(list_images(self.mask_path)))\n",
    "\n",
    "        # Se convirtieron las listas de imágenes y máscaras a arrays de numpy.\n",
    "        self.image_list = np.array(self.image_list)\n",
    "        self.mask_list = np.array(self.mask_list)\n",
    "\n",
    "        # Se estableció la semilla para la generación de números aleatorios y se mezclaron las imágenes y las máscaras.\n",
    "        np.random.seed(random_seed)\n",
    "        index = np.arange(len(self.image_list))\n",
    "        np.random.shuffle(index)\n",
    "        self.image_list = self.image_list[index]\n",
    "        self.mask_list = self.mask_list[index]\n",
    "\n",
    "    # Se definió el método para obtener la longitud del conjunto de datos.\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    # Se definió un método para obtener el nombre de una imagen o máscara.\n",
    "    def __getname__(self, index):\n",
    "        image_name = os.path.splitext(os.path.split(self.image_list[index])[-1])[0]\n",
    "        mask_name = os.path.splitext(os.path.split(self.mask_list[index])[-1])[0]\n",
    "\n",
    "        if image_name == mask_name:\n",
    "            return image_name\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    # Se definió un método para obtener una imagen y su máscara correspondiente.\n",
    "    def __getraw__(self, index):\n",
    "        if not self.__getname__(index):\n",
    "            raise ValueError(\"{}: La imagen no coincide con la máscara\".format(os.path.split(self.image_list[index])[-1]))\n",
    "        image = Image.open(self.image_list[index])\n",
    "        mask = Image.open(self.mask_list[index]).convert('L')\n",
    "        mask = np.array(mask)\n",
    "        mask = Image.fromarray(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    # Se definió el método para obtener un elemento del conjunto de datos.\n",
    "    def __getitem__(self, index):\n",
    "        image, mask = self.__getraw__(index)\n",
    "        image, mask = self.transforms(image, mask)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "El código anterior define una clase para un conjunto de datos de imágenes solares. La clase tiene métodos para obtener la longitud del conjunto de datos, obtener el nombre de una imagen o máscara, obtener una imagen y su máscara correspondiente, y obtener un elemento del conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se definió una clase para componer varias transformaciones.\n",
    "class Compose:\n",
    "    def __init__(self, transforms):\n",
    "        \"\"\"\n",
    "        transforms: una lista de transformaciones\n",
    "        \"\"\"\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    # Se definió el método para aplicar las transformaciones a la imagen y la máscara.\n",
    "    def __call__(self, image, target):\n",
    "        \"\"\"\n",
    "        image: imagen de entrada\n",
    "        target: máscara de entrada\n",
    "        \"\"\"\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "# Se definió una clase para redimensionar la imagen y la máscara a un tamaño fijo.\n",
    "class FixResize:\n",
    "    # UNet requiere que el tamaño de entrada sea múltiplo de 16\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    # Se definió el método para redimensionar la imagen y la máscara.\n",
    "    def __call__(self, image, target):\n",
    "        image = F.resize(image, (self.size, self.size), interpolation=transforms.InterpolationMode.BILINEAR)\n",
    "        target = F.resize(target, (self.size, self.size), interpolation=transforms.InterpolationMode.NEAREST)\n",
    "        return image, target\n",
    "\n",
    "# Se definió una clase para transformar la imagen y la máscara a tensores.\n",
    "class ToTensor:\n",
    "    \"\"\"Transforma la imagen a tensor. Escala la imagen a [0,1] float32.\n",
    "    Transforma la máscara a tensor.\n",
    "    \"\"\"\n",
    "    def __call__(self, image, target):\n",
    "        image = transforms.ToTensor()(image)\n",
    "        target = torch.as_tensor(np.array(target), dtype=torch.int64)\n",
    "        return image, target\n",
    "\n",
    "# Se definió una clase para transformar la imagen a tensor manteniendo el tipo original.\n",
    "class PILToTensor:\n",
    "    \"\"\"Transforma la imagen a tensor. Mantiene el tipo original.\"\"\"\n",
    "    def __call__(self, image, target):\n",
    "        image = F.pil_to_tensor(image)\n",
    "        target = torch.as_tensor(np.array(target), dtype=torch.int64)\n",
    "        return image, target\n",
    "\n",
    "# Se definió una clase para normalizar la imagen.\n",
    "class Normalize:\n",
    "    def __init__(self, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "    \n",
    "    # Se definió el método para normalizar la imagen.\n",
    "    def __call__(self, image, target):\n",
    "        image = F.normalize(image, mean=self.mean, std=self.std)\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "El código anterior define varias clases para transformaciones de imágenes y máscaras, incluyendo la composición de varias transformaciones, el redimensionamiento a un tamaño fijo, la transformación a tensores y la normalización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Las imágenes y etiquetas son solo para demostración tutorial.\n",
    "# El conjunto de datos completo que usamos para el desarrollo del modelo se puede encontrar aquí:\n",
    "# https://datahub.duramat.org/dataset/00b29daf-239c-47b6-bd96-bfb0875179a8/resource/c6626a05-e82f-4732-ade9-ec5441b83e46/download/crack_segmentation.zip\n",
    "\n",
    "# Se establece la ruta al directorio raíz que contiene las imágenes y las etiquetas.\n",
    "root = Path('/home/franklin/PV_vision/pv-vision/examples/crack_segmentation/img_label_for_training')\n",
    "\n",
    "\n",
    "# Se definen las transformaciones a aplicar a las imágenes y las etiquetas.\n",
    "transformers = Compose([FixResize(256), ToTensor(), Normalize()])\n",
    "\n",
    "# Se crean los conjuntos de datos de entrenamiento, validación y prueba.\n",
    "trainset = SolarDataset(root, image_folder=\"train/img\", \n",
    "        mask_folder=\"train/ann\", transforms=transformers)\n",
    "\n",
    "valset = SolarDataset(root, image_folder=\"val/img\", \n",
    "        mask_folder=\"val/ann\", transforms=transformers)\n",
    "\n",
    "testset = SolarDataset(root, image_folder=\"testset/img\", \n",
    "        mask_folder=\"testset/ann\", transforms=transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Este código define la ruta al directorio raíz que contiene las imágenes y las etiquetas, las transformaciones a aplicar a las imágenes y las etiquetas, y crea los conjuntos de datos de entrenamiento, validación y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El conjunto de datos de entrenamiento contiene 972 elementos.\n"
     ]
    }
   ],
   "source": [
    "# Verificación de que la carpeta haya sido establecida correctamente\n",
    "print(f\"El conjunto de datos de entrenamiento contiene {len(trainset)} elementos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define una función para crear un modelo DeepLab preentrenado.\n",
    "def DeepLab_pretrained(num_classes):\n",
    "    # Se carga el modelo DeepLab con una arquitectura ResNet50 preentrenada.\n",
    "    deeplab = deeplabv3_resnet50(weights=DeepLabV3_ResNet50_Weights.DEFAULT)\n",
    "    \n",
    "    # Se reemplaza el clasificador del modelo con un nuevo clasificador DeepLabHead.\n",
    "    # El nuevo clasificador tiene 2048 características de entrada y 'num_classes' características de salida.\n",
    "    deeplab.classifier = DeepLabHead(2048, num_classes)\n",
    "    \n",
    "    # Se devuelve el modelo modificado.\n",
    "    return deeplab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Este código define una función para crear un modelo DeepLab preentrenado con una arquitectura ResNet50 y reemplazar su clasificador con un nuevo clasificador que tiene un número específico de características de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/franklin/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo utilizado: cpu\n"
     ]
    }
   ],
   "source": [
    "# Se define el dispositivo en el que se ejecutará el modelo. Si hay una GPU disponible, se usará. De lo contrario, se usará la CPU.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Se imprime el dispositivo utilizado.\n",
    "print(f\"Dispositivo utilizado: {device}\")\n",
    "\n",
    "# Se crea el modelo utilizando la función DeepLab_pretrained definida anteriormente. \n",
    "# El modelo se envuelve en un objeto DataParallel para permitir el entrenamiento en múltiples GPUs si están disponibles.\n",
    "model = DataParallel(DeepLab_pretrained(5))\n",
    "\n",
    "# Se define la función de pérdida a utilizar durante el entrenamiento. En este caso, se utiliza la pérdida de entropía cruzada.\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# Se define el optimizador a utilizar durante el entrenamiento. En este caso, se utiliza Adam con una tasa de aprendizaje de 0.01.\n",
    "optimizer = Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Se define el programador de la tasa de aprendizaje a utilizar durante el entrenamiento. \n",
    "# En este caso, se utiliza un programador de paso que disminuye la tasa de aprendizaje en un factor de 0.2 cada 5 épocas.\n",
    "lr_scheduler = StepLR(optimizer, step_size=5, gamma=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "El código anterior define el dispositivo en el que se ejecutará el modelo, crea el modelo, define la función de pérdida, el optimizador y el programador de la tasa de aprendizaje a utilizar durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se inicializa el manejador del modelo.\n",
    "# La salida se almacena en la carpeta de salida.\n",
    "modelhandler = ModelHandler(\n",
    "    # Se pasa el modelo que se va a entrenar.\n",
    "    model=model,\n",
    "    \n",
    "    # Se especifica el nombre de la carpeta de salida.\n",
    "    model_output='out',\n",
    "    \n",
    "    # Se pasan los conjuntos de datos de entrenamiento, validación y prueba.\n",
    "    train_dataset=trainset,\n",
    "    val_dataset=valset,\n",
    "    test_dataset=testset,\n",
    "    \n",
    "    # Se especifica el tamaño del lote para el entrenamiento y la validación.\n",
    "    batch_size_train=32,\n",
    "    batch_size_val=32,\n",
    "    \n",
    "    # Se pasa el programador de la tasa de aprendizaje.\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    \n",
    "    # Se especifica el número de épocas para el entrenamiento.\n",
    "    num_epochs=10,\n",
    "    \n",
    "    # Se pasa la función de pérdida y el optimizador.\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    \n",
    "    # Se pasa el dispositivo en el que se ejecutará el entrenamiento.\n",
    "    device=device,\n",
    "    \n",
    "    # Se especifica el directorio donde se guardarán los puntos de control del modelo.\n",
    "    save_dir='checkpoints',\n",
    "    \n",
    "    # Se especifica el nombre del archivo de punto de control.\n",
    "    save_name='deeplab.pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Este código inicializa un manejador de modelo con varios parámetros, incluyendo el modelo a entrenar, los conjuntos de datos de entrenamiento, validación y prueba, el tamaño del lote, el programador de la tasa de aprendizaje, el número de épocas, la función de pérdida, el optimizador, el dispositivo, el directorio de guardado y el nombre del archivo de punto de control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/31 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "modelhandler.batch_size_train = 8  # Reduce the batch size to 16\n",
    "modelhandler.train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Este código entrena el modelo utilizando el manejador de modelo. El número de épocas se especificó al inicializar el manejador de modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Se visualiza el proceso de entrenamiento.\n",
    "# Esta función traza la pérdida del modelo durante el entrenamiento.\n",
    "modelhandler.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Este código traza la pérdida del modelo durante el entrenamiento utilizando el manejador de modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "attempt to get argmin of an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Se busca la pérdida mínima en la validación, que corresponde al mejor modelo.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# 'np.argmin' devuelve el índice de la pérdida mínima en el conjunto de validación.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Se suma 1 porque los índices en Python comienzan en 0, pero las épocas comienzan en 1.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelhandler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_record\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:1325\u001b[0m, in \u001b[0;36margmin\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;124;03mReturns the indices of the minimum values along an axis.\u001b[39;00m\n\u001b[1;32m   1240\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m-> 1325\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:56\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, method, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bound \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:45\u001b[0m, in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     wrap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, mu\u001b[38;5;241m.\u001b[39mndarray):\n",
      "\u001b[0;31mValueError\u001b[0m: attempt to get argmin of an empty sequence"
     ]
    }
   ],
   "source": [
    "# Se busca la pérdida mínima en la validación, que corresponde al mejor modelo.\n",
    "# 'np.argmin' devuelve el índice de la pérdida mínima en el conjunto de validación.\n",
    "# Se suma 1 porque los índices en Python comienzan en 0, pero las épocas comienzan en 1.\n",
    "np.argmin(modelhandler.running_record['val']['loss'])+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Este código busca la pérdida mínima en el conjunto de validación, que corresponde al mejor modelo. Se suma 1 al índice devuelto por 'np.argmin' porque los índices en Python comienzan en 0, pero las épocas comienzan en 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Podemos cargar el mejor modelo y verificar su rendimiento en el conjunto de prueba.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# 'load_model' es una función del manejador de modelo que carga un modelo desde un archivo de punto de control.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodelhandler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcheckpoints/epoch_10/deeplab.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pv_vision/nn/modelhandler.py:344\u001b[0m, in \u001b[0;36mModelHandler.load_model\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[1;32m    343\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Load model from path \"\"\"\u001b[39;00m\n\u001b[0;32m--> 344\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoaded model from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:1014\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1013\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1014\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1021\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1022\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:1422\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1420\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1421\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1422\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1424\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1425\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1427\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:1392\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1391\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1392\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:1366\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1361\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[1;32m   1363\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1364\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1365\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1366\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1367\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1368\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1371\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:381\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 381\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    383\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:279\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mUntypedStorage(obj\u001b[38;5;241m.\u001b[39mnbytes(), device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(location))\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_utils.py:117\u001b[0m, in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     untyped_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mUntypedStorage(\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize(), device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    116\u001b[0m     )\n\u001b[0;32m--> 117\u001b[0m     \u001b[43muntyped_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m untyped_storage\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Podemos cargar el mejor modelo y verificar su rendimiento en el conjunto de prueba.\n",
    "# 'load_model' es una función del manejador de modelo que carga un modelo desde un archivo de punto de control.\n",
    "modelhandler.load_model('checkpoints/epoch_10/deeplab.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Este código carga el mejor modelo desde un archivo de punto de control y lo prepara para verificar su rendimiento en el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se prueba el modelo en el conjunto de prueba y se almacena la salida en 'testset_output'.\n",
    "# 'test_model' es una función del manejador de modelo que prueba el modelo en el conjunto de prueba.\n",
    "_ = modelhandler.test_model(cache_output='testset_output')\n",
    "\n",
    "# La salida del modelo se almacena en self.cache['testset_output']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Este código prueba el modelo en el conjunto de prueba y almacena la salida en 'testset_output'. También se hace un comentario sobre la puntuación de la prueba y la puntuación de la validación."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

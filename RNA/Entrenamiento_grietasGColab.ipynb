{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Franklingo13/PVDefectDetect/blob/main/RNA/Entrenamiento_grietasGColab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psMYf9fJG09O"
      },
      "source": [
        "Notebook para entrenamiento de redes neuronales convolucionales para clasificación de defectos en imágenes de celdas fotovoltaicas.\n",
        "Pensado para correr en Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbQ5zjRCG09Q",
        "outputId": "c8aaddf5-1d1d-4a73-f4bf-c6ccfe849a46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Conexión con Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OhRFEtnDGxpJ"
      },
      "outputs": [],
      "source": [
        "# SPDX-License-Identifier: Apache-2.0\n",
        "#\n",
        "# Copyright (C) 2021 Supervisely\n",
        "#\n",
        "# This file is part of the Supervisely project and has been taken\n",
        "# from the Supervisely repository (https://github.com/supervisely/supervisely/blob/master/plugins/nn/unet_v2/src/unet.py).\n",
        "# It is being redistributed under the Apache License 2.0.\n",
        "#\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.models.vgg import vgg16_bn\n",
        "\n",
        "\n",
        "class ConvBNAct(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.seq = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels,\n",
        "                      kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return self.seq(inputs)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, src_channels, dst_channels):\n",
        "        super().__init__()\n",
        "        self.seq1 = ConvBNAct(src_channels, dst_channels)\n",
        "        self.seq2 = ConvBNAct(dst_channels, dst_channels)\n",
        "        self.seq3 = ConvBNAct(dst_channels, dst_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        result = self.seq1(x)\n",
        "        result = self.seq2(result)\n",
        "        result = self.seq3(result)\n",
        "        return result\n",
        "\n",
        "\n",
        "class UNetUp(nn.Module):\n",
        "    def __init__(self, down_channels,  right_channels):\n",
        "        super().__init__()\n",
        "        self.bottom_up = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.conv = nn.Conv2d(down_channels, right_channels, kernel_size=1, stride=1)\n",
        "\n",
        "    def forward(self, left, bottom):\n",
        "        from_bottom = self.bottom_up(bottom)\n",
        "        from_bottom = self.conv(from_bottom)\n",
        "        result = torch.cat([left, from_bottom], 1)\n",
        "        return result\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), dilation=2, padding=2)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(3, 3), dilation=2, padding=2)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = self.bn(out)\n",
        "        out = self.conv2(self.relu(out))\n",
        "        out = self.bn2(out)\n",
        "        return torch.cat((x, self.relu2(out)), dim=1)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder_blocks,  encoder_channels, n_cls):\n",
        "        self.encoder_channels = encoder_channels\n",
        "        self.depth = len(self.encoder_channels)\n",
        "        assert len(encoder_blocks) == self.depth\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder_blocks = nn.ModuleList(encoder_blocks)\n",
        "\n",
        "        self.blocks = nn.ModuleList()\n",
        "        # add bottleneck\n",
        "        self.blocks.append(Block(\n",
        "            self.encoder_channels[-1],\n",
        "            self.encoder_channels[-1]\n",
        "        ))\n",
        "\n",
        "        self.ups = nn.ModuleList()\n",
        "        for i in range(1, self.depth):\n",
        "            bottom_channels = self.encoder_channels[self.depth - i]\n",
        "            left_channels = self.encoder_channels[self.depth - i - 1]\n",
        "            right_channels = left_channels\n",
        "            self.ups.append(UNetUp(bottom_channels,  right_channels))\n",
        "            self.blocks.append(Block(\n",
        "                left_channels + right_channels,\n",
        "                right_channels\n",
        "            ))\n",
        "        self.last_conv = nn.Conv2d(encoder_channels[0], n_cls, 1)\n",
        "        # self.dropout = nn.Dropout2d(p=0.1)\n",
        "        self.bottle = Bottleneck(512, 512)\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoder_outputs = []\n",
        "        for encoder_block in self.encoder_blocks:\n",
        "            x = encoder_block(x)\n",
        "            encoder_outputs.append(x)\n",
        "        x = self.bottle(encoder_outputs[self.depth - 1])\n",
        "        for i in range(self.depth):\n",
        "            if i > 0:\n",
        "                encoder_output = encoder_outputs[self.depth - i - 1]\n",
        "                x = self.ups[i - 1](encoder_output, x)\n",
        "                x = self.blocks[i](x)\n",
        "        # x = self.dropout(x)\n",
        "        x = self.last_conv(x)\n",
        "        return x  # no softmax or log_softmax\n",
        "\n",
        "\n",
        "def _get_encoder_blocks(model):\n",
        "    # last modules (ReLUs) of VGG blocks\n",
        "    layers_last_module_names = ['5', '12', '22', '32', '42']\n",
        "    result = []\n",
        "    cur_block = nn.Sequential()\n",
        "    for name, child in model.named_children():\n",
        "        if name == 'features':\n",
        "            for name2, child2 in child.named_children():\n",
        "                cur_block.add_module(name2, child2)\n",
        "                if name2 in layers_last_module_names:\n",
        "                    result.append(cur_block)\n",
        "                    cur_block = nn.Sequential()\n",
        "            break\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def construct_unet(n_cls, pretrain=False):  # no weights inited\n",
        "    model = vgg16_bn(weights='DEFAULT')\n",
        "    encoder_blocks = _get_encoder_blocks(model)\n",
        "    encoder_channels = [64, 128, 256, 512, 1024]  # vgg16 channels\n",
        "    # prev_channels = encoder_channels[-1]\n",
        "\n",
        "    return UNet(encoder_blocks, encoder_channels, n_cls)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "U_8l2-gnG09S"
      },
      "outputs": [],
      "source": [
        "# Importación de librerías\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torch.nn import DataParallel\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torchvision.io import read_image, ImageReadMode\n",
        "from torchvision.datasets.vision import VisionDataset\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50, DeepLabV3_ResNet50_Weights\n",
        "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
        "from torchvision.utils import draw_segmentation_masks\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "\n",
        "import requests\n",
        "import copy\n",
        "#from unet_model import construct_unet\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from imutils.paths import list_images\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importación de la librería de pv-vision\n",
        "!pip install pv-vision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YVtXGzixG09T"
      },
      "outputs": [],
      "source": [
        "# Importar el manejador de modelo: ModelHandler\n",
        "from pv_vision.nn import ModelHandler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ia6yr7DDG09T"
      },
      "outputs": [],
      "source": [
        "# Definición de una clase para el conjunto de datos solar,\n",
        "# que hereda de la clase VisionDataset de PyTorch.\n",
        "class SolarDataset(VisionDataset):\n",
        "    \"\"\"Un conjunto de datos que lee directamente las imágenes y las máscaras desde una carpeta.\"\"\"\n",
        "\n",
        "    # Se definió el método de inicialización para la clase.\n",
        "    def __init__(self,\n",
        "                 root,\n",
        "                 image_folder,\n",
        "                 mask_folder,\n",
        "                 transforms,\n",
        "                 mode = \"train\",\n",
        "                 random_seed=42):\n",
        "        # Se llamó al método de inicialización de la clase padre.\n",
        "        super().__init__(root, transforms)\n",
        "        # Se establecieron las rutas a las carpetas de imágenes y máscaras.\n",
        "        self.image_path = Path(self.root) / image_folder\n",
        "        self.mask_path = Path(self.root) / mask_folder\n",
        "\n",
        "        # Se verificó que las carpetas de imágenes y máscaras existan.\n",
        "        if not os.path.exists(self.image_path):\n",
        "            raise OSError(f\"{self.image_path} no encontrado.\")\n",
        "\n",
        "        if not os.path.exists(self.mask_path):\n",
        "            raise OSError(f\"{self.mask_path} no encontrado.\")\n",
        "\n",
        "        # Se obtuvieron las listas de imágenes y máscaras y se ordenaron.\n",
        "        self.image_list = sorted(list(list_images(self.image_path)))\n",
        "        self.mask_list = sorted(list(list_images(self.mask_path)))\n",
        "\n",
        "        # Se convirtieron las listas de imágenes y máscaras a arrays de numpy.\n",
        "        self.image_list = np.array(self.image_list)\n",
        "        self.mask_list = np.array(self.mask_list)\n",
        "\n",
        "        # Se estableció la semilla para la generación de números aleatorios y se mezclaron las imágenes y las máscaras.\n",
        "        np.random.seed(random_seed)\n",
        "        index = np.arange(len(self.image_list))\n",
        "        np.random.shuffle(index)\n",
        "        self.image_list = self.image_list[index]\n",
        "        self.mask_list = self.mask_list[index]\n",
        "\n",
        "    # Se definió el método para obtener la longitud del conjunto de datos.\n",
        "    def __len__(self):\n",
        "        return len(self.image_list)\n",
        "\n",
        "    # Se definió un método para obtener el nombre de una imagen o máscara.\n",
        "    def __getname__(self, index):\n",
        "        image_name = os.path.splitext(os.path.split(self.image_list[index])[-1])[0]\n",
        "        mask_name = os.path.splitext(os.path.split(self.mask_list[index])[-1])[0]\n",
        "\n",
        "        if image_name == mask_name:\n",
        "            return image_name\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    # Se definió un método para obtener una imagen y su máscara correspondiente.\n",
        "    def __getraw__(self, index):\n",
        "        if not self.__getname__(index):\n",
        "            raise ValueError(\"{}: La imagen no coincide con la máscara\".format(os.path.split(self.image_list[index])[-1]))\n",
        "        image = Image.open(self.image_list[index])\n",
        "        mask = Image.open(self.mask_list[index]).convert('L')\n",
        "        mask = np.array(mask)\n",
        "        mask = Image.fromarray(mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "    # Se definió el método para obtener un elemento del conjunto de datos.\n",
        "    def __getitem__(self, index):\n",
        "        image, mask = self.__getraw__(index)\n",
        "        image, mask = self.transforms(image, mask)\n",
        "\n",
        "        return image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "t1nDW9d6G09T"
      },
      "outputs": [],
      "source": [
        "# Definición de una clase para componer varias transformaciones.\n",
        "class Compose:\n",
        "    def __init__(self, transforms):\n",
        "        \"\"\"\n",
        "        transforms: una lista de transformaciones\n",
        "        \"\"\"\n",
        "        self.transforms = transforms\n",
        "\n",
        "    # Se definió el método para aplicar las transformaciones a la imagen y la máscara.\n",
        "    def __call__(self, image, target):\n",
        "        \"\"\"\n",
        "        image: imagen de entrada\n",
        "        target: máscara de entrada\n",
        "        \"\"\"\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target\n",
        "\n",
        "# Se definió una clase para redimensionar la imagen y la máscara a un tamaño fijo.\n",
        "class FixResize:\n",
        "    # UNet requiere que el tamaño de entrada sea múltiplo de 16\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "\n",
        "    # Se definió el método para redimensionar la imagen y la máscara.\n",
        "    def __call__(self, image, target):\n",
        "        image = F.resize(image, (self.size, self.size), interpolation=transforms.InterpolationMode.BILINEAR)\n",
        "        target = F.resize(target, (self.size, self.size), interpolation=transforms.InterpolationMode.NEAREST)\n",
        "        return image, target\n",
        "\n",
        "# Se definió una clase para transformar la imagen y la máscara a tensores.\n",
        "class ToTensor:\n",
        "    \"\"\"Transforma la imagen a tensor. Escala la imagen a [0,1] float32.\n",
        "    Transforma la máscara a tensor.\n",
        "    \"\"\"\n",
        "    def __call__(self, image, target):\n",
        "        image = transforms.ToTensor()(image)\n",
        "        target = torch.as_tensor(np.array(target), dtype=torch.int64)\n",
        "        return image, target\n",
        "\n",
        "# Se definió una clase para transformar la imagen a tensor manteniendo el tipo original.\n",
        "class PILToTensor:\n",
        "    \"\"\"Transforma la imagen a tensor. Mantiene el tipo original.\"\"\"\n",
        "    def __call__(self, image, target):\n",
        "        image = F.pil_to_tensor(image)\n",
        "        target = torch.as_tensor(np.array(target), dtype=torch.int64)\n",
        "        return image, target\n",
        "\n",
        "# Se definió una clase para normalizar la imagen.\n",
        "class Normalize:\n",
        "    def __init__(self, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        # Verifica si la imagen es en escala de grises (1 canal) y la convierte a RGB (3 canales) si es necesario\n",
        "        if image.shape[0] == 1:\n",
        "            image = image.repeat(3, 1, 1)  # Repite el canal existente 3 veces\n",
        "        image = F.normalize(image, mean=self.mean, std=self.std)\n",
        "        return image, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRAdQ8o1G09U",
        "outputId": "ac4a6979-ee74-4162-e69b-6fe0560f99b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El conjunto de datos de entrenamiento contiene 1012 elementos.\n"
          ]
        }
      ],
      "source": [
        "# Ruta al directorio que contiene las imágenes y las máscaras.\n",
        "root = Path(\n",
        "    '/content/drive/MyDrive/Trabajo de titulación/PV_vision/Entrenamiento')\n",
        "\n",
        "# Se definen las transformaciones a aplicar a las imágenes y las etiquetas.\n",
        "transformers = Compose([FixResize(256), ToTensor(), Normalize()])\n",
        "#/content/drive/MyDrive/Trabajo de titulación/PV_vision/Entrenamiento/train/annotations\n",
        "#/content/drive/MyDrive/Trabajo de titulación/PV_vision/Entrenamiento/img_label_for_training/train\n",
        "# Se crean los conjuntos de datos de entrenamiento, validación y prueba.\n",
        "trainset = SolarDataset(root, image_folder=\"train/img\",\n",
        "        mask_folder=\"train/ann\", transforms=transformers)\n",
        "\n",
        "valset = SolarDataset(root, image_folder=\"val2/img\",\n",
        "        mask_folder=\"val2/ann\", transforms=transformers)\n",
        "\n",
        "testset = SolarDataset(root, image_folder=\"test/img\",\n",
        "        mask_folder=\"test/ann\", transforms=transformers)\n",
        "\n",
        "# Verificación de que la carpeta haya sido establecida correctamente\n",
        "print(f\"El conjunto de datos de entrenamiento contiene {len(trainset)} elementos.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Accuracy:\n",
        "    def __call__(self, outputs, targets):\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        return (preds == targets).float().mean().item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaZs0hwDG09U"
      },
      "outputs": [],
      "source": [
        "# Se define una función para crear un modelo DeepLab preentrenado.\n",
        "def DeepLab_pretrained(num_classes):\n",
        "    # Se carga el modelo DeepLab con una arquitectura ResNet50 preentrenada.\n",
        "    deeplab = deeplabv3_resnet50(weights=DeepLabV3_ResNet50_Weights.DEFAULT)\n",
        "\n",
        "    # Se reemplaza el clasificador del modelo con un nuevo clasificador DeepLabHead.\n",
        "    # El nuevo clasificador tiene 2048 características de entrada y 'num_classes' características de salida.\n",
        "    deeplab.classifier = DeepLabHead(2048, num_classes)\n",
        "\n",
        "    # Se devuelve el modelo modificado.\n",
        "    return deeplab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TZFPZp57F3wK"
      },
      "outputs": [],
      "source": [
        "# Crea una instancia del modelo U-Net con 5 canales de salida.\n",
        "# Número de canales de salida = al número de clases\n",
        "unet = construct_unet(5)\n",
        "# Se \"envuelve\" el modelo en un objeto DataParallel.\n",
        "# Esto permite que el modelo se ejecute en paralelo en múltiples GPUs, si están disponibles.\n",
        "unet = DataParallel(unet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qnmr0nyOG09U",
        "outputId": "b9b8b7af-4fb2-4b03-db1e-350dd4013848"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dispositivo utilizado: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Se define el dispositivo en el que se ejecutará el modelo.\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Se imprime el dispositivo utilizado.\n",
        "print(f\"Dispositivo utilizado: {device}\")\n",
        "\n",
        "# Se crea el modelo utilizando la función DeepLab_pretrained definida anteriormente.\n",
        "# El modelo se envuelve en un objeto DataParallel para permitir el entrenamiento en múltiples GPUs si están disponibles.\n",
        "#model = DataParallel(DeepLab_pretrained(5))\n",
        "\n",
        "# Se define la función de pérdida a utilizar durante el entrenamiento.\n",
        "# En este caso, se utiliza la pérdida de entropía cruzada.\n",
        "criterion = CrossEntropyLoss()\n",
        "\n",
        "# Se define el optimizador a utilizar durante el entrenamiento. En este caso, se utiliza Adam con una tasa de aprendizaje de 0.01.\n",
        "#optimizer = Adam(model.parameters(), lr=0.01)\n",
        "optimizer = Adam(unet.parameters(), lr=0.01)\n",
        "\n",
        "# Se define el programador de la tasa de aprendizaje a utilizar durante el entrenamiento.\n",
        "# En este caso, se utiliza un programador de paso que disminuye la tasa de aprendizaje en un factor de 0.2 cada 5 épocas.\n",
        "lr_scheduler = StepLR(optimizer, step_size=5, gamma=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjJv6uo4G09V",
        "outputId": "a7a1865a-f5a5-4f91-fe4f-72ab315027ef"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:pv_vision.nn.modelhandler:ModelHandler initialized.\n"
          ]
        }
      ],
      "source": [
        "# Se inicializa el manejador del modelo.\n",
        "# La salida se almacena en la carpeta de salida.\n",
        "modelhandler = ModelHandler(\n",
        "    # Se pasa el modelo que se va a entrenar.\n",
        "    #model=model,\n",
        "    model = unet,\n",
        "    # Se especifica el nombre de la carpeta de salida.\n",
        "    #model_output='out_unet',\n",
        "    # Se pasan los conjuntos de datos de entrenamiento, validación y prueba.\n",
        "    train_dataset=trainset,\n",
        "    val_dataset=valset,\n",
        "    test_dataset=testset,\n",
        "    # Se especifica el tamaño del lote para el entrenamiento y la validación.\n",
        "    batch_size_train=16,\n",
        "    batch_size_val=16,\n",
        "    # Se pasa el programador de la tasa de aprendizaje.\n",
        "    lr_scheduler=lr_scheduler,\n",
        "    # Se especifica el número de épocas para el entrenamiento.\n",
        "    num_epochs=20,\n",
        "    # Se pasa la función de pérdida y el optimizador.\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    # Se pasa el dispositivo en el que se ejecutará el entrenamiento.\n",
        "    device=device,\n",
        "    evaluate_metric = Accuracy,\n",
        "    # Se especifica el directorio donde se guardarán los puntos de control del modelo.\n",
        "    save_dir='/content/drive/MyDrive/Trabajo de titulación/PV_vision/Entrenamiento/PuntosControl/checkpoints',\n",
        "    # Se especifica el nombre del archivo de punto de control.\n",
        "    save_name='unetv5.pt'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1SfRwQCG09V",
        "outputId": "0f5bb9e6-1f76-4f93-ec88-261d245a2ff2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 / 20\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/64 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 1 [0/1012 (0%)]\tLoss: 0.038848\n",
            " 16%|█▌        | 10/64 [00:09<00:54,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 1 [160/1012 (16%)]\tLoss: 0.032154\n",
            " 31%|███▏      | 20/64 [00:20<00:46,  1.05s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 1 [320/1012 (31%)]\tLoss: 0.043614\n",
            " 47%|████▋     | 30/64 [00:31<00:35,  1.05s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 1 [480/1012 (47%)]\tLoss: 0.034632\n",
            " 62%|██████▎   | 40/64 [00:41<00:24,  1.03s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 1 [640/1012 (62%)]\tLoss: 0.043071\n",
            " 78%|███████▊  | 50/64 [00:52<00:14,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 1 [800/1012 (78%)]\tLoss: 0.049263\n",
            " 94%|█████████▍| 60/64 [01:02<00:04,  1.01s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 1 [960/1012 (94%)]\tLoss: 0.056294\n",
            "100%|██████████| 64/64 [01:06<00:00,  1.04s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 1\n",
            "100%|██████████| 13/13 [00:06<00:00,  2.02it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 1 \tAverage loss: 0.1577\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 0.0456 (train) | 0.1577 (val)\n",
            "Epoch 2 / 20\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/64 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 2 [0/1012 (0%)]\tLoss: 0.035654\n",
            " 16%|█▌        | 10/64 [00:09<00:54,  1.01s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 2 [160/1012 (16%)]\tLoss: 0.041282\n",
            " 31%|███▏      | 20/64 [00:20<00:44,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 2 [320/1012 (31%)]\tLoss: 0.063022\n",
            " 47%|████▋     | 30/64 [00:30<00:35,  1.03s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 2 [480/1012 (47%)]\tLoss: 0.048774\n",
            " 62%|██████▎   | 40/64 [00:41<00:24,  1.03s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 2 [640/1012 (62%)]\tLoss: 0.028748\n",
            " 78%|███████▊  | 50/64 [00:51<00:14,  1.03s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 2 [800/1012 (78%)]\tLoss: 0.040730\n",
            " 94%|█████████▍| 60/64 [01:02<00:04,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 2 [960/1012 (94%)]\tLoss: 0.043309\n",
            "100%|██████████| 64/64 [01:06<00:00,  1.03s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 2\n",
            "100%|██████████| 13/13 [00:07<00:00,  1.84it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 2 \tAverage loss: 0.1572\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 0.0452 (train) | 0.1572 (val)\n",
            "Epoch 3 / 20\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/64 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 3 [0/1012 (0%)]\tLoss: 0.031178\n",
            " 16%|█▌        | 10/64 [00:09<00:54,  1.01s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 3 [160/1012 (16%)]\tLoss: 0.048081\n",
            " 31%|███▏      | 20/64 [00:20<00:44,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 3 [320/1012 (31%)]\tLoss: 0.053353\n",
            " 47%|████▋     | 30/64 [00:30<00:34,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 3 [480/1012 (47%)]\tLoss: 0.036519\n",
            " 62%|██████▎   | 40/64 [00:41<00:24,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 3 [640/1012 (62%)]\tLoss: 0.063605\n",
            " 78%|███████▊  | 50/64 [00:51<00:14,  1.03s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 3 [800/1012 (78%)]\tLoss: 0.031455\n",
            " 94%|█████████▍| 60/64 [01:01<00:04,  1.03s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 3 [960/1012 (94%)]\tLoss: 0.081479\n",
            "100%|██████████| 64/64 [01:06<00:00,  1.03s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 3\n",
            "100%|██████████| 13/13 [00:06<00:00,  2.05it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 3 \tAverage loss: 0.1562\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 0.0456 (train) | 0.1562 (val)\n",
            "Epoch 4 / 20\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/64 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 4 [0/1012 (0%)]\tLoss: 0.065927\n",
            " 16%|█▌        | 10/64 [00:09<00:54,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 4 [160/1012 (16%)]\tLoss: 0.044476\n",
            " 31%|███▏      | 20/64 [00:20<00:45,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 4 [320/1012 (31%)]\tLoss: 0.045227\n",
            " 47%|████▋     | 30/64 [00:30<00:34,  1.03s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 4 [480/1012 (47%)]\tLoss: 0.041337\n",
            " 62%|██████▎   | 40/64 [00:41<00:24,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 4 [640/1012 (62%)]\tLoss: 0.034336\n",
            " 78%|███████▊  | 50/64 [00:51<00:14,  1.03s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 4 [800/1012 (78%)]\tLoss: 0.059120\n",
            " 94%|█████████▍| 60/64 [01:02<00:04,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 4 [960/1012 (94%)]\tLoss: 0.072252\n",
            "100%|██████████| 64/64 [01:06<00:00,  1.03s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 4\n",
            "100%|██████████| 13/13 [00:07<00:00,  1.80it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 4 \tAverage loss: 0.1509\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 0.0454 (train) | 0.1509 (val)\n",
            "Epoch 5 / 20\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/64 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 5 [0/1012 (0%)]\tLoss: 0.051695\n",
            " 16%|█▌        | 10/64 [00:09<00:55,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 5 [160/1012 (16%)]\tLoss: 0.033955\n",
            " 31%|███▏      | 20/64 [00:20<00:44,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 5 [320/1012 (31%)]\tLoss: 0.053162\n",
            " 47%|████▋     | 30/64 [00:30<00:34,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 5 [480/1012 (47%)]\tLoss: 0.069383\n",
            " 62%|██████▎   | 40/64 [00:41<00:24,  1.03s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 5 [640/1012 (62%)]\tLoss: 0.026963\n",
            " 78%|███████▊  | 50/64 [00:51<00:14,  1.03s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 5 [800/1012 (78%)]\tLoss: 0.052594\n",
            " 94%|█████████▍| 60/64 [01:02<00:04,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 5 [960/1012 (94%)]\tLoss: 0.056126\n",
            "100%|██████████| 64/64 [01:06<00:00,  1.03s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 5\n",
            "100%|██████████| 13/13 [00:06<00:00,  1.96it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 5 \tAverage loss: 0.1584\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 0.0452 (train) | 0.1584 (val)\n",
            "Epoch 6 / 20\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/64 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 6 [0/1012 (0%)]\tLoss: 0.034989\n",
            " 16%|█▌        | 10/64 [00:09<00:54,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 6 [160/1012 (16%)]\tLoss: 0.050877\n",
            " 31%|███▏      | 20/64 [00:20<00:44,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 6 [320/1012 (31%)]\tLoss: 0.035595\n",
            " 47%|████▋     | 30/64 [00:30<00:34,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 6 [480/1012 (47%)]\tLoss: 0.042183\n",
            " 62%|██████▎   | 40/64 [00:41<00:24,  1.03s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 6 [640/1012 (62%)]\tLoss: 0.044077\n",
            " 78%|███████▊  | 50/64 [00:51<00:14,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 6 [800/1012 (78%)]\tLoss: 0.060277\n",
            " 94%|█████████▍| 60/64 [01:02<00:04,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 6 [960/1012 (94%)]\tLoss: 0.039185\n",
            "100%|██████████| 64/64 [01:06<00:00,  1.03s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 6\n",
            "100%|██████████| 13/13 [00:08<00:00,  1.56it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 6 \tAverage loss: 0.1636\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 0.0453 (train) | 0.1636 (val)\n",
            "Epoch 7 / 20\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/64 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 7 [0/1012 (0%)]\tLoss: 0.046421\n",
            " 16%|█▌        | 10/64 [00:10<00:54,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 7 [160/1012 (16%)]\tLoss: 0.057142\n",
            " 31%|███▏      | 20/64 [00:20<00:45,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 7 [320/1012 (31%)]\tLoss: 0.026831\n",
            " 47%|████▋     | 30/64 [00:31<00:34,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 7 [480/1012 (47%)]\tLoss: 0.040936\n",
            " 62%|██████▎   | 40/64 [00:41<00:24,  1.03s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 7 [640/1012 (62%)]\tLoss: 0.053688\n",
            " 78%|███████▊  | 50/64 [00:51<00:14,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 7 [800/1012 (78%)]\tLoss: 0.080737\n",
            " 94%|█████████▍| 60/64 [01:02<00:04,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 7 [960/1012 (94%)]\tLoss: 0.071565\n",
            "100%|██████████| 64/64 [01:06<00:00,  1.04s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 7\n",
            "100%|██████████| 13/13 [00:06<00:00,  1.99it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 7 \tAverage loss: 0.1543\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 0.0452 (train) | 0.1543 (val)\n",
            "Epoch 8 / 20\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/64 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 8 [0/1012 (0%)]\tLoss: 0.053852\n",
            " 16%|█▌        | 10/64 [00:09<00:54,  1.01s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 8 [160/1012 (16%)]\tLoss: 0.053915\n",
            " 31%|███▏      | 20/64 [00:20<00:44,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 8 [320/1012 (31%)]\tLoss: 0.038462\n",
            " 47%|████▋     | 30/64 [00:30<00:34,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 8 [480/1012 (47%)]\tLoss: 0.054477\n",
            " 62%|██████▎   | 40/64 [00:40<00:24,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 8 [640/1012 (62%)]\tLoss: 0.045842\n",
            " 78%|███████▊  | 50/64 [00:51<00:14,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 8 [800/1012 (78%)]\tLoss: 0.040757\n",
            " 94%|█████████▍| 60/64 [01:01<00:04,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 8 [960/1012 (94%)]\tLoss: 0.045092\n",
            "100%|██████████| 64/64 [01:05<00:00,  1.03s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 8\n",
            "100%|██████████| 13/13 [00:08<00:00,  1.49it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 8 \tAverage loss: 0.1666\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 0.0451 (train) | 0.1666 (val)\n",
            "Epoch 9 / 20\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/64 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 9 [0/1012 (0%)]\tLoss: 0.040102\n",
            " 16%|█▌        | 10/64 [00:09<00:54,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 9 [160/1012 (16%)]\tLoss: 0.044735\n",
            " 31%|███▏      | 20/64 [00:20<00:45,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 9 [320/1012 (31%)]\tLoss: 0.044332\n",
            " 47%|████▋     | 30/64 [00:30<00:34,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 9 [480/1012 (47%)]\tLoss: 0.054550\n",
            " 62%|██████▎   | 40/64 [00:41<00:24,  1.03s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 9 [640/1012 (62%)]\tLoss: 0.053668\n",
            " 78%|███████▊  | 50/64 [00:51<00:14,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 9 [800/1012 (78%)]\tLoss: 0.050669\n",
            " 94%|█████████▍| 60/64 [01:02<00:04,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 9 [960/1012 (94%)]\tLoss: 0.049832\n",
            "100%|██████████| 64/64 [01:06<00:00,  1.03s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 9\n",
            "100%|██████████| 13/13 [00:06<00:00,  1.93it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 9 \tAverage loss: 0.1547\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 0.0449 (train) | 0.1547 (val)\n",
            "Epoch 10 / 20\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/64 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 10 [0/1012 (0%)]\tLoss: 0.054689\n",
            " 16%|█▌        | 10/64 [00:09<00:54,  1.01s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 10 [160/1012 (16%)]\tLoss: 0.050006\n",
            " 31%|███▏      | 20/64 [00:20<00:44,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 10 [320/1012 (31%)]\tLoss: 0.035328\n",
            " 47%|████▋     | 30/64 [00:30<00:34,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 10 [480/1012 (47%)]\tLoss: 0.039292\n",
            " 62%|██████▎   | 40/64 [00:41<00:24,  1.03s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 10 [640/1012 (62%)]\tLoss: 0.042229\n",
            " 78%|███████▊  | 50/64 [00:51<00:14,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 10 [800/1012 (78%)]\tLoss: 0.052976\n",
            " 94%|█████████▍| 60/64 [01:01<00:04,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 10 [960/1012 (94%)]\tLoss: 0.035052\n",
            "100%|██████████| 64/64 [01:05<00:00,  1.03s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 10\n",
            "100%|██████████| 13/13 [00:07<00:00,  1.63it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 10 \tAverage loss: 0.1603\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 0.0452 (train) | 0.1603 (val)\n",
            "Epoch 11 / 20\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/64 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 11 [0/1012 (0%)]\tLoss: 0.039180\n",
            " 16%|█▌        | 10/64 [00:10<00:54,  1.01s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 11 [160/1012 (16%)]\tLoss: 0.051145\n",
            " 31%|███▏      | 20/64 [00:20<00:44,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 11 [320/1012 (31%)]\tLoss: 0.066922\n",
            " 47%|████▋     | 30/64 [00:31<00:34,  1.03s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 11 [480/1012 (47%)]\tLoss: 0.058454\n",
            " 62%|██████▎   | 40/64 [00:41<00:24,  1.03s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 11 [640/1012 (62%)]\tLoss: 0.038917\n",
            " 78%|███████▊  | 50/64 [00:51<00:14,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 11 [800/1012 (78%)]\tLoss: 0.054560\n",
            " 94%|█████████▍| 60/64 [01:02<00:04,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 11 [960/1012 (94%)]\tLoss: 0.045972\n",
            "100%|██████████| 64/64 [01:06<00:00,  1.04s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 11\n",
            "100%|██████████| 13/13 [00:06<00:00,  1.87it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 11 \tAverage loss: 0.1665\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 0.0450 (train) | 0.1665 (val)\n",
            "Epoch 12 / 20\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/64 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 12 [0/1012 (0%)]\tLoss: 0.053955\n",
            " 16%|█▌        | 10/64 [00:09<00:54,  1.01s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 12 [160/1012 (16%)]\tLoss: 0.047701\n",
            " 31%|███▏      | 20/64 [00:20<00:45,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 12 [320/1012 (31%)]\tLoss: 0.052368\n",
            " 47%|████▋     | 30/64 [00:30<00:34,  1.03s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 12 [480/1012 (47%)]\tLoss: 0.035928\n",
            " 62%|██████▎   | 40/64 [00:41<00:24,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 12 [640/1012 (62%)]\tLoss: 0.038222\n",
            " 78%|███████▊  | 50/64 [00:51<00:14,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 12 [800/1012 (78%)]\tLoss: 0.041714\n",
            " 94%|█████████▍| 60/64 [01:01<00:04,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 12 [960/1012 (94%)]\tLoss: 0.046576\n",
            "100%|██████████| 64/64 [01:06<00:00,  1.03s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 12\n",
            "100%|██████████| 13/13 [00:07<00:00,  1.70it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 12 \tAverage loss: 0.1516\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 0.0452 (train) | 0.1516 (val)\n",
            "Epoch 13 / 20\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/64 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 13 [0/1012 (0%)]\tLoss: 0.058751\n",
            " 16%|█▌        | 10/64 [00:10<00:54,  1.01s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 13 [160/1012 (16%)]\tLoss: 0.041475\n",
            " 31%|███▏      | 20/64 [00:20<00:45,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 13 [320/1012 (31%)]\tLoss: 0.038400\n",
            " 47%|████▋     | 30/64 [00:31<00:34,  1.03s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 13 [480/1012 (47%)]\tLoss: 0.069077\n",
            " 62%|██████▎   | 40/64 [00:41<00:24,  1.03s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 13 [640/1012 (62%)]\tLoss: 0.031249\n",
            " 78%|███████▊  | 50/64 [00:52<00:14,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 13 [800/1012 (78%)]\tLoss: 0.032797\n",
            " 94%|█████████▍| 60/64 [01:02<00:04,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 13 [960/1012 (94%)]\tLoss: 0.066440\n",
            "100%|██████████| 64/64 [01:06<00:00,  1.04s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 13\n",
            "100%|██████████| 13/13 [00:06<00:00,  1.86it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 13 \tAverage loss: 0.1641\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 0.0454 (train) | 0.1641 (val)\n",
            "Epoch 14 / 20\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/64 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 14 [0/1012 (0%)]\tLoss: 0.058753\n",
            " 16%|█▌        | 10/64 [00:09<00:54,  1.01s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 14 [160/1012 (16%)]\tLoss: 0.030891\n",
            " 31%|███▏      | 20/64 [00:20<00:44,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 14 [320/1012 (31%)]\tLoss: 0.041638\n",
            " 47%|████▋     | 30/64 [00:30<00:34,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 14 [480/1012 (47%)]\tLoss: 0.035655\n",
            " 62%|██████▎   | 40/64 [00:41<00:24,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 14 [640/1012 (62%)]\tLoss: 0.058327\n",
            " 78%|███████▊  | 50/64 [00:51<00:14,  1.03s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 14 [800/1012 (78%)]\tLoss: 0.060270\n",
            " 94%|█████████▍| 60/64 [01:01<00:04,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 14 [960/1012 (94%)]\tLoss: 0.039580\n",
            "100%|██████████| 64/64 [01:05<00:00,  1.03s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 14\n",
            "100%|██████████| 13/13 [00:06<00:00,  1.91it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 14 \tAverage loss: 0.1593\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 0.0451 (train) | 0.1593 (val)\n",
            "Epoch 15 / 20\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/64 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 15 [0/1012 (0%)]\tLoss: 0.058878\n",
            " 16%|█▌        | 10/64 [00:09<00:54,  1.01s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 15 [160/1012 (16%)]\tLoss: 0.033333\n",
            " 31%|███▏      | 20/64 [00:20<00:45,  1.03s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 15 [320/1012 (31%)]\tLoss: 0.046429\n",
            " 47%|████▋     | 30/64 [00:30<00:34,  1.03s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 15 [480/1012 (47%)]\tLoss: 0.049140\n",
            " 62%|██████▎   | 40/64 [00:41<00:24,  1.03s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 15 [640/1012 (62%)]\tLoss: 0.025895\n",
            " 78%|███████▊  | 50/64 [00:51<00:14,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 15 [800/1012 (78%)]\tLoss: 0.051567\n",
            " 94%|█████████▍| 60/64 [01:02<00:04,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 15 [960/1012 (94%)]\tLoss: 0.068804\n",
            "100%|██████████| 64/64 [01:06<00:00,  1.03s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 15\n",
            "100%|██████████| 13/13 [00:06<00:00,  1.95it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 15 \tAverage loss: 0.1598\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 0.0452 (train) | 0.1598 (val)\n",
            "Epoch 16 / 20\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/64 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 16 [0/1012 (0%)]\tLoss: 0.035787\n",
            " 16%|█▌        | 10/64 [00:09<00:54,  1.01s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 16 [160/1012 (16%)]\tLoss: 0.044103\n",
            " 31%|███▏      | 20/64 [00:20<00:44,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 16 [320/1012 (31%)]\tLoss: 0.067250\n",
            " 47%|████▋     | 30/64 [00:30<00:34,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 16 [480/1012 (47%)]\tLoss: 0.043116\n",
            " 62%|██████▎   | 40/64 [00:41<00:24,  1.03s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 16 [640/1012 (62%)]\tLoss: 0.024310\n",
            " 78%|███████▊  | 50/64 [00:51<00:14,  1.03s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 16 [800/1012 (78%)]\tLoss: 0.042552\n",
            " 94%|█████████▍| 60/64 [01:01<00:04,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 16 [960/1012 (94%)]\tLoss: 0.057415\n",
            "100%|██████████| 64/64 [01:06<00:00,  1.03s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 16\n",
            "100%|██████████| 13/13 [00:06<00:00,  1.90it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 16 \tAverage loss: 0.1612\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 0.0449 (train) | 0.1612 (val)\n",
            "Epoch 17 / 20\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/64 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 17 [0/1012 (0%)]\tLoss: 0.045279\n",
            " 16%|█▌        | 10/64 [00:09<00:54,  1.01s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 17 [160/1012 (16%)]\tLoss: 0.062564\n",
            " 31%|███▏      | 20/64 [00:20<00:44,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 17 [320/1012 (31%)]\tLoss: 0.048943\n",
            " 47%|████▋     | 30/64 [00:30<00:34,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 17 [480/1012 (47%)]\tLoss: 0.042492\n",
            " 62%|██████▎   | 40/64 [00:41<00:24,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 17 [640/1012 (62%)]\tLoss: 0.041427\n",
            " 78%|███████▊  | 50/64 [00:51<00:14,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 17 [800/1012 (78%)]\tLoss: 0.043300\n",
            " 94%|█████████▍| 60/64 [01:01<00:04,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 17 [960/1012 (94%)]\tLoss: 0.039061\n",
            "100%|██████████| 64/64 [01:05<00:00,  1.03s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 17\n",
            "100%|██████████| 13/13 [00:06<00:00,  1.96it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 17 \tAverage loss: 0.1663\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 0.0452 (train) | 0.1663 (val)\n",
            "Epoch 18 / 20\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/64 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 18 [0/1012 (0%)]\tLoss: 0.042638\n",
            " 16%|█▌        | 10/64 [00:09<00:54,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 18 [160/1012 (16%)]\tLoss: 0.037767\n",
            " 31%|███▏      | 20/64 [00:20<00:45,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 18 [320/1012 (31%)]\tLoss: 0.038253\n",
            " 47%|████▋     | 30/64 [00:30<00:34,  1.03s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 18 [480/1012 (47%)]\tLoss: 0.038412\n",
            " 62%|██████▎   | 40/64 [00:41<00:24,  1.03s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 18 [640/1012 (62%)]\tLoss: 0.073911\n",
            " 78%|███████▊  | 50/64 [00:51<00:14,  1.03s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 18 [800/1012 (78%)]\tLoss: 0.041360\n",
            " 94%|█████████▍| 60/64 [01:02<00:04,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 18 [960/1012 (94%)]\tLoss: 0.051419\n",
            "100%|██████████| 64/64 [01:06<00:00,  1.03s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 18\n",
            "100%|██████████| 13/13 [00:06<00:00,  1.93it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 18 \tAverage loss: 0.1556\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 0.0449 (train) | 0.1556 (val)\n",
            "Epoch 19 / 20\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/64 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 19 [0/1012 (0%)]\tLoss: 0.060424\n",
            " 16%|█▌        | 10/64 [00:09<00:54,  1.01s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 19 [160/1012 (16%)]\tLoss: 0.045328\n",
            " 31%|███▏      | 20/64 [00:20<00:45,  1.03s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 19 [320/1012 (31%)]\tLoss: 0.039130\n",
            " 47%|████▋     | 30/64 [00:30<00:34,  1.03s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 19 [480/1012 (47%)]\tLoss: 0.048338\n",
            " 62%|██████▎   | 40/64 [00:41<00:24,  1.03s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 19 [640/1012 (62%)]\tLoss: 0.048149\n",
            " 78%|███████▊  | 50/64 [00:51<00:14,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 19 [800/1012 (78%)]\tLoss: 0.024389\n",
            " 94%|█████████▍| 60/64 [01:02<00:04,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 19 [960/1012 (94%)]\tLoss: 0.050228\n",
            "100%|██████████| 64/64 [01:06<00:00,  1.03s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 19\n",
            "100%|██████████| 13/13 [00:06<00:00,  2.01it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 19 \tAverage loss: 0.1534\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 0.0455 (train) | 0.1534 (val)\n",
            "Epoch 20 / 20\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/64 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 20 [0/1012 (0%)]\tLoss: 0.057602\n",
            " 16%|█▌        | 10/64 [00:09<00:54,  1.01s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 20 [160/1012 (16%)]\tLoss: 0.022999\n",
            " 31%|███▏      | 20/64 [00:20<00:44,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 20 [320/1012 (31%)]\tLoss: 0.035567\n",
            " 47%|████▋     | 30/64 [00:30<00:34,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 20 [480/1012 (47%)]\tLoss: 0.053268\n",
            " 62%|██████▎   | 40/64 [00:41<00:24,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 20 [640/1012 (62%)]\tLoss: 0.042925\n",
            " 78%|███████▊  | 50/64 [00:51<00:14,  1.02s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 20 [800/1012 (78%)]\tLoss: 0.046497\n",
            " 94%|█████████▍| 60/64 [01:02<00:04,  1.03s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 20 [960/1012 (94%)]\tLoss: 0.043064\n",
            "100%|██████████| 64/64 [01:06<00:00,  1.03s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 20\n",
            "100%|██████████| 13/13 [00:06<00:00,  1.87it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 20 \tAverage loss: 0.1683\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 0.0451 (train) | 0.1683 (val)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'train': {'loss': [0.045579425445776214,\n",
              "   0.045218858058037964,\n",
              "   0.045552201274591,\n",
              "   0.04538224128041814,\n",
              "   0.04521193283171993,\n",
              "   0.0452519010737715,\n",
              "   0.04522868405600546,\n",
              "   0.04509589229176638,\n",
              "   0.04487413387348058,\n",
              "   0.045203853788759865,\n",
              "   0.045018766502382256,\n",
              "   0.0452057121417268,\n",
              "   0.04542887539201575,\n",
              "   0.04512357286254879,\n",
              "   0.04515742101216976,\n",
              "   0.04490010534586172,\n",
              "   0.04516341283976325,\n",
              "   0.044860710798516105,\n",
              "   0.04549189718815649,\n",
              "   0.0451428030613854]},\n",
              " 'val': {'loss': [0.15773860368786788,\n",
              "   0.15718154027694609,\n",
              "   0.15617273711576696,\n",
              "   0.1508663542750405,\n",
              "   0.15842616514461796,\n",
              "   0.16364167536177288,\n",
              "   0.1543310747277446,\n",
              "   0.1665763637641581,\n",
              "   0.15469902826518547,\n",
              "   0.1602682385502792,\n",
              "   0.16650566226098595,\n",
              "   0.15157318467774042,\n",
              "   0.16406119451290224,\n",
              "   0.1593305314459452,\n",
              "   0.15977670717530135,\n",
              "   0.16120752197940175,\n",
              "   0.16629557754935287,\n",
              "   0.15557563788280254,\n",
              "   0.15338686435687832,\n",
              "   0.16828955397373294]}}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Se inicializa el entrenamiento del modelo.\n",
        "modelhandler.train_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "k55JhgMyG09V",
        "outputId": "98770658-ff66-466e-b342-ac37c8e529dd"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Se visualiza el proceso de entrenamiento.\n",
        "# Esta función traza la pérdida del modelo durante el entrenamiento.\n",
        "modelhandler.plot_loss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E52bTEXnG09W",
        "outputId": "d72f2e0f-d187-4067-aaa0-e1c6125d2e26"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Se busca la pérdida mínima en la validación, que corresponde al mejor modelo.\n",
        "# 'np.argmin' devuelve el índice de la pérdida mínima en el conjunto de validación.\n",
        "# Se suma 1 porque los índices en Python comienzan en 0, pero las épocas comienzan en 1.\n",
        "np.argmin(modelhandler.running_record['val']['loss'])+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kH5xVXQyG09W",
        "outputId": "d900217c-984f-40f4-c995-301ba8d7a76f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:pv_vision.nn.modelhandler:Loaded model from /content/drive/MyDrive/Trabajo de titulación/PV_vision/Entrenamiento/PuntosControl/unetv3.pt\n"
          ]
        }
      ],
      "source": [
        "# Se carga el mejor modelo entrenado y se verifica su rendimiento en el conjunto de prueba.\n",
        "# Se emplea `load_model` para cargar el modelo entrenado. Este método toma el nombre del archivo de punto de control.\n",
        "modelhandler.load_model('/content/drive/MyDrive/Trabajo de titulación/PV_vision/Entrenamiento/PuntosControl/checkpoints/epoch_4/unetv5.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xa-Fdu8ZG09W"
      },
      "source": [
        "El siguiente código prueba el modelo en el conjunto de prueba y almacena la salida en 'testset_output'. También se hace un comentario sobre la puntuación de la prueba y la puntuación de la validación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1q3LEUNaG09W",
        "outputId": "54fd117a-b457-4e4b-b1ba-9b2ebf682a45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing mode\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 23/23 [05:01<00:00, 13.09s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Test set: Average loss: 0.1434\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set: Average loss: 0.1434\n"
          ]
        }
      ],
      "source": [
        "# Se evalúa el modelo en el conjunto de prueba. `test_model` es una función de ModelHandler\n",
        "# que evalúa el modelo en el conjunto de prueba y almacena la salida en la caché.\n",
        "_ = modelhandler.test_model(cache_output='testset_output')\n",
        "\n",
        "# La salida del modelo se almacena en self.cache['testset_output']"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Franklingo13/PVDefectDetect/blob/main/RNA/Entrenamiento_grietasGColab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psMYf9fJG09O"
      },
      "source": [
        "Notebook para entrenamiento de redes neuronales convolucionales para clasificación de defectos en imágenes de celdas fotovoltaicas.\n",
        "Pensado para correr en Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbQ5zjRCG09Q",
        "outputId": "44f57862-9011-4c56-887c-d0bc9bf31bf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Conexión con Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OhRFEtnDGxpJ"
      },
      "outputs": [],
      "source": [
        "# SPDX-License-Identifier: Apache-2.0\n",
        "#\n",
        "# Copyright (C) 2021 Supervisely\n",
        "#\n",
        "# This file is part of the Supervisely project and has been taken\n",
        "# from the Supervisely repository (https://github.com/supervisely/supervisely/blob/master/plugins/nn/unet_v2/src/unet.py).\n",
        "# It is being redistributed under the Apache License 2.0.\n",
        "#\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.models.vgg import vgg16_bn\n",
        "\n",
        "\n",
        "class ConvBNAct(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.seq = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels,\n",
        "                      kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return self.seq(inputs)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, src_channels, dst_channels):\n",
        "        super().__init__()\n",
        "        self.seq1 = ConvBNAct(src_channels, dst_channels)\n",
        "        self.seq2 = ConvBNAct(dst_channels, dst_channels)\n",
        "        self.seq3 = ConvBNAct(dst_channels, dst_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        result = self.seq1(x)\n",
        "        result = self.seq2(result)\n",
        "        result = self.seq3(result)\n",
        "        return result\n",
        "\n",
        "\n",
        "class UNetUp(nn.Module):\n",
        "    def __init__(self, down_channels,  right_channels):\n",
        "        super().__init__()\n",
        "        self.bottom_up = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.conv = nn.Conv2d(down_channels, right_channels, kernel_size=1, stride=1)\n",
        "\n",
        "    def forward(self, left, bottom):\n",
        "        from_bottom = self.bottom_up(bottom)\n",
        "        from_bottom = self.conv(from_bottom)\n",
        "        result = torch.cat([left, from_bottom], 1)\n",
        "        return result\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), dilation=2, padding=2)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(3, 3), dilation=2, padding=2)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = self.bn(out)\n",
        "        out = self.conv2(self.relu(out))\n",
        "        out = self.bn2(out)\n",
        "        return torch.cat((x, self.relu2(out)), dim=1)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder_blocks,  encoder_channels, n_cls):\n",
        "        self.encoder_channels = encoder_channels\n",
        "        self.depth = len(self.encoder_channels)\n",
        "        assert len(encoder_blocks) == self.depth\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder_blocks = nn.ModuleList(encoder_blocks)\n",
        "\n",
        "        self.blocks = nn.ModuleList()\n",
        "        # add bottleneck\n",
        "        self.blocks.append(Block(\n",
        "            self.encoder_channels[-1],\n",
        "            self.encoder_channels[-1]\n",
        "        ))\n",
        "\n",
        "        self.ups = nn.ModuleList()\n",
        "        for i in range(1, self.depth):\n",
        "            bottom_channels = self.encoder_channels[self.depth - i]\n",
        "            left_channels = self.encoder_channels[self.depth - i - 1]\n",
        "            right_channels = left_channels\n",
        "            self.ups.append(UNetUp(bottom_channels,  right_channels))\n",
        "            self.blocks.append(Block(\n",
        "                left_channels + right_channels,\n",
        "                right_channels\n",
        "            ))\n",
        "        self.last_conv = nn.Conv2d(encoder_channels[0], n_cls, 1)\n",
        "        # self.dropout = nn.Dropout2d(p=0.1)\n",
        "        self.bottle = Bottleneck(512, 512)\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoder_outputs = []\n",
        "        for encoder_block in self.encoder_blocks:\n",
        "            x = encoder_block(x)\n",
        "            encoder_outputs.append(x)\n",
        "        x = self.bottle(encoder_outputs[self.depth - 1])\n",
        "        for i in range(self.depth):\n",
        "            if i > 0:\n",
        "                encoder_output = encoder_outputs[self.depth - i - 1]\n",
        "                x = self.ups[i - 1](encoder_output, x)\n",
        "                x = self.blocks[i](x)\n",
        "        # x = self.dropout(x)\n",
        "        x = self.last_conv(x)\n",
        "        return x  # no softmax or log_softmax\n",
        "\n",
        "\n",
        "def _get_encoder_blocks(model):\n",
        "    # last modules (ReLUs) of VGG blocks\n",
        "    layers_last_module_names = ['5', '12', '22', '32', '42']\n",
        "    result = []\n",
        "    cur_block = nn.Sequential()\n",
        "    for name, child in model.named_children():\n",
        "        if name == 'features':\n",
        "            for name2, child2 in child.named_children():\n",
        "                cur_block.add_module(name2, child2)\n",
        "                if name2 in layers_last_module_names:\n",
        "                    result.append(cur_block)\n",
        "                    cur_block = nn.Sequential()\n",
        "            break\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def construct_unet(n_cls, pretrain=False):  # no weights inited\n",
        "    model = vgg16_bn(weights='DEFAULT')\n",
        "    encoder_blocks = _get_encoder_blocks(model)\n",
        "    encoder_channels = [64, 128, 256, 512, 1024]  # vgg16 channels\n",
        "    # prev_channels = encoder_channels[-1]\n",
        "\n",
        "    return UNet(encoder_blocks, encoder_channels, n_cls)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "U_8l2-gnG09S"
      },
      "outputs": [],
      "source": [
        "# Importación de librerías\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torch.nn import DataParallel\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torchvision.io import read_image, ImageReadMode\n",
        "from torchvision.datasets.vision import VisionDataset\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50, DeepLabV3_ResNet50_Weights\n",
        "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
        "from torchvision.utils import draw_segmentation_masks\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "\n",
        "import requests\n",
        "import copy\n",
        "#from unet_model import construct_unet\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from imutils.paths import list_images\n",
        "import os\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "u-13tOJejCxA",
        "outputId": "aef8d1e4-6e3b-4179-90cb-f6b6505379bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pv-vision in /usr/local/lib/python3.10/dist-packages (0.2.8)\n",
            "Requirement already satisfied: imutils>=0.5.4 in /usr/local/lib/python3.10/dist-packages (from pv-vision) (0.5.4)\n",
            "Requirement already satisfied: ipywidgets>=8.1.2 in /usr/local/lib/python3.10/dist-packages (from pv-vision) (8.1.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pv-vision) (1.4.2)\n",
            "Requirement already satisfied: matplotlib>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from pv-vision) (3.9.1)\n",
            "Requirement already satisfied: opencv-python>=4.7.0.72 in /usr/local/lib/python3.10/dist-packages (from pv-vision) (4.10.0.84)\n",
            "Requirement already satisfied: scikit-learn>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pv-vision) (1.3.2)\n",
            "Requirement already satisfied: setuptools>=68.2.2 in /usr/local/lib/python3.10/dist-packages (from pv-vision) (71.0.4)\n",
            "Requirement already satisfied: torch>=2.2.0.post100 in /usr/local/lib/python3.10/dist-packages (from pv-vision) (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.15.2a0 in /usr/local/lib/python3.10/dist-packages (from pv-vision) (0.18.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from pv-vision) (4.66.4)\n",
            "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.1.2->pv-vision) (0.2.2)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.1.2->pv-vision) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.1.2->pv-vision) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.11 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.1.2->pv-vision) (4.0.11)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.1.2->pv-vision) (3.0.11)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.8.0->pv-vision) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.8.0->pv-vision) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.8.0->pv-vision) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.8.0->pv-vision) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.8.0->pv-vision) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.8.0->pv-vision) (24.1)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.8.0->pv-vision) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.8.0->pv-vision) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.8.0->pv-vision) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->pv-vision) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->pv-vision) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0.post100->pv-vision) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0.post100->pv-vision) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0.post100->pv-vision) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0.post100->pv-vision) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0.post100->pv-vision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0.post100->pv-vision) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0.post100->pv-vision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0.post100->pv-vision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0.post100->pv-vision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0.post100->pv-vision) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0.post100->pv-vision) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0.post100->pv-vision) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0.post100->pv-vision) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0.post100->pv-vision) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0.post100->pv-vision) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0.post100->pv-vision) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0.post100->pv-vision) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0.post100->pv-vision) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.2.0.post100->pv-vision) (12.5.82)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->pv-vision) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->pv-vision) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->pv-vision) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->pv-vision) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->pv-vision) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->pv-vision) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->pv-vision) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->pv-vision) (4.9.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.8.0->pv-vision) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.2.0.post100->pv-vision) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.2.0.post100->pv-vision) (1.3.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.1.2->pv-vision) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.1.2->pv-vision) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.1.2->pv-vision) (0.2.13)\n"
          ]
        }
      ],
      "source": [
        "# Importación de la librería de pv-vision\n",
        "!pip install pv-vision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YVtXGzixG09T"
      },
      "outputs": [],
      "source": [
        "# Importar el manejador de modelo: ModelHandler\n",
        "from pv_vision.nn import ModelHandler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ia6yr7DDG09T"
      },
      "outputs": [],
      "source": [
        "# Definición de una clase para el conjunto de datos solar,\n",
        "# que hereda de la clase VisionDataset de PyTorch.\n",
        "class SolarDataset(VisionDataset):\n",
        "    \"\"\"Un conjunto de datos que lee directamente las imágenes y las máscaras desde una carpeta.\"\"\"\n",
        "\n",
        "    # Se definió el método de inicialización para la clase.\n",
        "    def __init__(self,\n",
        "                 root,\n",
        "                 image_folder,\n",
        "                 mask_folder,\n",
        "                 transforms,\n",
        "                 mode = \"train\",\n",
        "                 random_seed=42):\n",
        "        # Se llamó al método de inicialización de la clase padre.\n",
        "        super().__init__(root, transforms)\n",
        "        # Se establecieron las rutas a las carpetas de imágenes y máscaras.\n",
        "        self.image_path = Path(self.root) / image_folder\n",
        "        self.mask_path = Path(self.root) / mask_folder\n",
        "\n",
        "        # Se verificó que las carpetas de imágenes y máscaras existan.\n",
        "        if not os.path.exists(self.image_path):\n",
        "            raise OSError(f\"{self.image_path} no encontrado.\")\n",
        "\n",
        "        if not os.path.exists(self.mask_path):\n",
        "            raise OSError(f\"{self.mask_path} no encontrado.\")\n",
        "\n",
        "        # Se obtuvieron las listas de imágenes y máscaras y se ordenaron.\n",
        "        self.image_list = sorted(list(list_images(self.image_path)))\n",
        "        self.mask_list = sorted(list(list_images(self.mask_path)))\n",
        "\n",
        "        # Se convirtieron las listas de imágenes y máscaras a arrays de numpy.\n",
        "        self.image_list = np.array(self.image_list)\n",
        "        self.mask_list = np.array(self.mask_list)\n",
        "\n",
        "        # Se estableció la semilla para la generación de números aleatorios y se mezclaron las imágenes y las máscaras.\n",
        "        np.random.seed(random_seed)\n",
        "        index = np.arange(len(self.image_list))\n",
        "        np.random.shuffle(index)\n",
        "        self.image_list = self.image_list[index]\n",
        "        self.mask_list = self.mask_list[index]\n",
        "\n",
        "    # Se definió el método para obtener la longitud del conjunto de datos.\n",
        "    def __len__(self):\n",
        "        return len(self.image_list)\n",
        "\n",
        "    # Se definió un método para obtener el nombre de una imagen o máscara.\n",
        "    def __getname__(self, index):\n",
        "        image_name = os.path.splitext(os.path.split(self.image_list[index])[-1])[0]\n",
        "        mask_name = os.path.splitext(os.path.split(self.mask_list[index])[-1])[0]\n",
        "\n",
        "        if image_name == mask_name:\n",
        "            return image_name\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    # Se definió un método para obtener una imagen y su máscara correspondiente.\n",
        "    def __getraw__(self, index):\n",
        "        if not self.__getname__(index):\n",
        "            raise ValueError(\"{}: La imagen no coincide con la máscara\".format(os.path.split(self.image_list[index])[-1]))\n",
        "        image = Image.open(self.image_list[index])\n",
        "        mask = Image.open(self.mask_list[index]).convert('L')\n",
        "        mask = np.array(mask)\n",
        "        mask = Image.fromarray(mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "    # Se definió el método para obtener un elemento del conjunto de datos.\n",
        "    def __getitem__(self, index):\n",
        "        image, mask = self.__getraw__(index)\n",
        "        image, mask = self.transforms(image, mask)\n",
        "\n",
        "        return image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "t1nDW9d6G09T"
      },
      "outputs": [],
      "source": [
        "# Definición de una clase para componer varias transformaciones.\n",
        "class Compose:\n",
        "    def __init__(self, transforms):\n",
        "        \"\"\"\n",
        "        transforms: una lista de transformaciones\n",
        "        \"\"\"\n",
        "        self.transforms = transforms\n",
        "\n",
        "    # Se definió el método para aplicar las transformaciones a la imagen y la máscara.\n",
        "    def __call__(self, image, target):\n",
        "        \"\"\"\n",
        "        image: imagen de entrada\n",
        "        target: máscara de entrada\n",
        "        \"\"\"\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target\n",
        "\n",
        "# Se definió una clase para redimensionar la imagen y la máscara a un tamaño fijo.\n",
        "class FixResize:\n",
        "    # UNet requiere que el tamaño de entrada sea múltiplo de 16\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "\n",
        "    # Se definió el método para redimensionar la imagen y la máscara.\n",
        "    def __call__(self, image, target):\n",
        "        image = F.resize(image, (self.size, self.size), interpolation=transforms.InterpolationMode.BILINEAR)\n",
        "        target = F.resize(target, (self.size, self.size), interpolation=transforms.InterpolationMode.NEAREST)\n",
        "        return image, target\n",
        "\n",
        "# Se definió una clase para transformar la imagen y la máscara a tensores.\n",
        "class ToTensor:\n",
        "    \"\"\"Transforma la imagen a tensor. Escala la imagen a [0,1] float32.\n",
        "    Transforma la máscara a tensor.\n",
        "    \"\"\"\n",
        "    def __call__(self, image, target):\n",
        "        image = transforms.ToTensor()(image)\n",
        "        target = torch.as_tensor(np.array(target), dtype=torch.int64)\n",
        "        return image, target\n",
        "\n",
        "# Se definió una clase para transformar la imagen a tensor manteniendo el tipo original.\n",
        "class PILToTensor:\n",
        "    \"\"\"Transforma la imagen a tensor. Mantiene el tipo original.\"\"\"\n",
        "    def __call__(self, image, target):\n",
        "        image = F.pil_to_tensor(image)\n",
        "        target = torch.as_tensor(np.array(target), dtype=torch.int64)\n",
        "        return image, target\n",
        "\n",
        "# Se definió una clase para normalizar la imagen.\n",
        "class Normalize:\n",
        "    def __init__(self, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        # Verifica si la imagen es en escala de grises (1 canal) y la convierte a RGB (3 canales) si es necesario\n",
        "        if image.shape[0] == 1:\n",
        "            image = image.repeat(3, 1, 1)  # Repite el canal existente 3 veces\n",
        "        image = F.normalize(image, mean=self.mean, std=self.std)\n",
        "        return image, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRAdQ8o1G09U",
        "outputId": "49298859-60f3-42ab-e184-b8e0520a53bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El conjunto de datos de entrenamiento contiene 1453 elementos.\n"
          ]
        }
      ],
      "source": [
        "# Ruta al directorio que contiene las imágenes y las máscaras.\n",
        "# root = Path(\n",
        "#     '/content/drive/MyDrive/Trabajo de titulación/PV_vision/Entrenamiento')\n",
        "\n",
        "root = Path(\n",
        "    '/content/drive/MyDrive/Entrenamiento')\n",
        "\n",
        "# Se definen las transformaciones a aplicar a las imágenes y las etiquetas.\n",
        "transformers = Compose([FixResize(256), ToTensor(), Normalize()])\n",
        "#/content/drive/MyDrive/Trabajo de titulación/PV_vision/Entrenamiento/train/annotations\n",
        "#/content/drive/MyDrive/Trabajo de titulación/PV_vision/Entrenamiento/img_label_for_training/train\n",
        "# Se crean los conjuntos de datos de entrenamiento, validación y prueba.\n",
        "trainset = SolarDataset(root, image_folder=\"train/img\",\n",
        "        mask_folder=\"train/ann\", transforms=transformers)\n",
        "\n",
        "valset = SolarDataset(root, image_folder=\"val/img\",\n",
        "        mask_folder=\"val/ann\", transforms=transformers)\n",
        "\n",
        "testset = SolarDataset(root, image_folder=\"test/img\",\n",
        "        mask_folder=\"test/ann\", transforms=transformers)\n",
        "\n",
        "# Verificación de que la carpeta haya sido establecida correctamente\n",
        "print(f\"El conjunto de datos de entrenamiento contiene {len(trainset)} elementos.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhN5cKIpjCxD"
      },
      "outputs": [],
      "source": [
        "class Accuracy:\n",
        "    \"\"\"Calcular la precisión de un modelo\"\"\"\n",
        "    def __init__(self):\n",
        "        self.__name__ = \"accuracy\"\n",
        "        self._base = [0, 0]\n",
        "\n",
        "    def calc(self, outputs, targets, reduction='mean'):\n",
        "        \"\"\" Calcular la precisión.\n",
        "        Argumentos:\n",
        "        -----------\n",
        "        outputs: torch.Tensor\n",
        "        La salida del modelo, forma (batch_size, num_classes, H, W)\n",
        "\n",
        "        targets: torch.Tensor\n",
        "        La etiqueta verdadera, forma (batch_size, H, W)\n",
        "\n",
        "        reduction: str\n",
        "        El método de reducción, 'mean' o 'sum'\n",
        "        Si es 'mean', devuelve la precisión media del lote\n",
        "        Si es 'sum', devuelve la suma de predicciones correctas del lote\n",
        "\n",
        "        Retorna:\n",
        "        --------\n",
        "        accuracy: torch.Tensor\n",
        "        \"\"\"\n",
        "        # Asegúrate de que las dimensiones de outputs y targets sean compatibles\n",
        "        if outputs.dim() == 4 and targets.dim() == 3:\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct = torch.sum(preds == targets)\n",
        "\n",
        "            if reduction == 'mean':\n",
        "                return correct.float() / targets.numel()\n",
        "            elif reduction == 'sum':\n",
        "                return correct\n",
        "            else:\n",
        "                raise ValueError(\"reduction debe ser 'mean' o 'sum'\")\n",
        "        else:\n",
        "            raise ValueError(\"Las dimensiones de outputs y targets no son compatibles\")\n",
        "\n",
        "    def accumulate(self, outputs, targets):\n",
        "        \"\"\" Acumular la métrica a lo largo de varios lotes.\"\"\"\n",
        "        if outputs.dim() == 4 and targets.dim() == 3:\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct = torch.sum(preds == targets)\n",
        "            self._base[0] += correct\n",
        "            self._base[1] += targets.numel()\n",
        "        else:\n",
        "            raise ValueError(\"Las dimensiones de outputs y targets no son compatibles\")\n",
        "\n",
        "    def reset(self):\n",
        "        self._base = [0, 0]\n",
        "\n",
        "    def accumulated_score(self):\n",
        "        \"\"\" Devolver la puntuación acumulada en una época.\"\"\"\n",
        "        if self._base[1] == 0:\n",
        "            # advertencia de división por cero\n",
        "            warnings.warn(\"El denominador es cero, devuelve 0\", RuntimeWarning)\n",
        "            return 0\n",
        "        return self._base[0].float() / self._base[1]\n",
        "\n",
        "    def __call__(self, outputs, targets, reduction='mean'):\n",
        "        return self.calc(outputs, targets, reduction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaZs0hwDG09U"
      },
      "outputs": [],
      "source": [
        "# Se define una función para crear un modelo DeepLab preentrenado.\n",
        "def DeepLab_pretrained(num_classes):\n",
        "    # Se carga el modelo DeepLab con una arquitectura ResNet50 preentrenada.\n",
        "    deeplab = deeplabv3_resnet50(weights=DeepLabV3_ResNet50_Weights.DEFAULT)\n",
        "\n",
        "    # Se reemplaza el clasificador del modelo con un nuevo clasificador DeepLabHead.\n",
        "    # El nuevo clasificador tiene 2048 características de entrada y 'num_classes' características de salida.\n",
        "    deeplab.classifier = DeepLabHead(2048, num_classes)\n",
        "\n",
        "    # Se devuelve el modelo modificado.\n",
        "    return deeplab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TZFPZp57F3wK"
      },
      "outputs": [],
      "source": [
        "# Crea una instancia del modelo U-Net con 5 canales de salida.\n",
        "# Número de canales de salida = al número de clases\n",
        "unet = construct_unet(5)\n",
        "# Se \"envuelve\" el modelo en un objeto DataParallel.\n",
        "# Esto permite que el modelo se ejecute en paralelo en múltiples GPUs, si están disponibles.\n",
        "unet = DataParallel(unet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qnmr0nyOG09U",
        "outputId": "77c60b05-b1ff-4af3-fd82-7e041ea0f46a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dispositivo utilizado: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Se define el dispositivo en el que se ejecutará el modelo.\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Se imprime el dispositivo utilizado.\n",
        "print(f\"Dispositivo utilizado: {device}\")\n",
        "\n",
        "# Se crea el modelo utilizando la función DeepLab_pretrained definida anteriormente.\n",
        "# El modelo se envuelve en un objeto DataParallel para permitir el entrenamiento en múltiples GPUs si están disponibles.\n",
        "#model = DataParallel(DeepLab_pretrained(5))\n",
        "\n",
        "# Se define la función de pérdida a utilizar durante el entrenamiento.\n",
        "# En este caso, se utiliza la pérdida de entropía cruzada.\n",
        "criterion = CrossEntropyLoss()\n",
        "\n",
        "# Se define el optimizador a utilizar durante el entrenamiento. En este caso, se utiliza Adam con una tasa de aprendizaje de 0.01.\n",
        "#optimizer = Adam(model.parameters(), lr=0.01)\n",
        "optimizer = Adam(unet.parameters(), lr=0.001)\n",
        "\n",
        "# Se define el programador de la tasa de aprendizaje a utilizar durante el entrenamiento.\n",
        "# En este caso, se utiliza un programador de paso que disminuye la tasa de aprendizaje en un factor de 0.2 cada 5 épocas.\n",
        "lr_scheduler = StepLR(optimizer, step_size=5, gamma=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qouTmOWmA8ng",
        "outputId": "40e9ee1b-b1b7-4a68-b6ca-b95d799a4346"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# Cargar los pesos del modelo preentrenado\n",
        "\n",
        "weight_path = '/content/drive/MyDrive/Entrenamiento/unetv11.pt'\n",
        "unet.load_state_dict(torch.load(weight_path, map_location=device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjJv6uo4G09V",
        "outputId": "9bc0bbad-4c19-4c3e-c661-dfa6c53a8242"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pv_vision.nn.modelhandler:ModelHandler initialized.\n"
          ]
        }
      ],
      "source": [
        "# Se inicializa el manejador del modelo.\n",
        "# La salida se almacena en la carpeta de salida.\n",
        "modelhandler = ModelHandler(\n",
        "    # Se pasa el modelo que se va a entrenar.\n",
        "    #model=model,\n",
        "    model = unet,\n",
        "    # Se especifica el nombre de la carpeta de salida.\n",
        "    #model_output='out_unet',\n",
        "    # Se pasan los conjuntos de datos de entrenamiento, validación y prueba.\n",
        "    train_dataset=trainset,\n",
        "    val_dataset=valset,\n",
        "    test_dataset=testset,\n",
        "    # Se especifica el tamaño del lote para el entrenamiento y la validación.\n",
        "    batch_size_train=32,\n",
        "    batch_size_val=32,\n",
        "    # Se pasa el programador de la tasa de aprendizaje.\n",
        "    lr_scheduler=lr_scheduler,\n",
        "    # Se especifica el número de épocas para el entrenamiento.\n",
        "    num_epochs=35,\n",
        "    # Se pasa la función de pérdida y el optimizador.\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    # Se pasa el dispositivo en el que se ejecutará el entrenamiento.\n",
        "    device=device,\n",
        "    #evaluate_metric= Precision,\n",
        "    # Se especifica el directorio donde se guardarán los puntos de control del modelo.\n",
        "    save_dir='/content/drive/MyDrive/Entrenamiento/checkpoints',\n",
        "    # Se especifica el nombre del archivo de punto de control.\n",
        "    save_name='unetv13.pt'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1SfRwQCG09V",
        "outputId": "001a3d7f-6687-40fe-8c66-1210bc69532a",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 35\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 1 [0/1453 (0%)]\tLoss: 2.089597\n",
            " 22%|██▏       | 10/46 [00:21<00:44,  1.24s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 1 [320/1453 (22%)]\tLoss: 0.955458\n",
            " 43%|████▎     | 20/46 [00:33<00:29,  1.12s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 1 [640/1453 (43%)]\tLoss: 0.782049\n",
            " 65%|██████▌   | 30/46 [00:44<00:17,  1.12s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 1 [960/1453 (65%)]\tLoss: 0.607395\n",
            " 87%|████████▋ | 40/46 [00:56<00:06,  1.12s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 1 [1280/1453 (87%)]\tLoss: 0.519559\n",
            "100%|██████████| 46/46 [01:02<00:00,  1.37s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 1\n",
            "100%|██████████| 3/3 [00:01<00:00,  1.51it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 1 \tAverage loss: 0.6058\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.8295 (train) | 0.6058 (val)\n",
            "Epoch 2 / 35\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 2 [0/1453 (0%)]\tLoss: 0.450244\n",
            " 22%|██▏       | 10/46 [00:11<00:40,  1.12s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 2 [320/1453 (22%)]\tLoss: 0.369152\n",
            " 43%|████▎     | 20/46 [00:22<00:29,  1.13s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 2 [640/1453 (43%)]\tLoss: 0.293407\n",
            " 65%|██████▌   | 30/46 [00:34<00:18,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 2 [960/1453 (65%)]\tLoss: 0.227933\n",
            " 87%|████████▋ | 40/46 [00:45<00:06,  1.13s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 2 [1280/1453 (87%)]\tLoss: 0.210229\n",
            "100%|██████████| 46/46 [00:52<00:00,  1.14s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 2\n",
            "100%|██████████| 3/3 [00:01<00:00,  1.51it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 2 \tAverage loss: 0.3150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.3026 (train) | 0.3150 (val)\n",
            "Epoch 3 / 35\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 3 [0/1453 (0%)]\tLoss: 0.219684\n",
            " 22%|██▏       | 10/46 [00:11<00:40,  1.13s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 3 [320/1453 (22%)]\tLoss: 0.188496\n",
            " 43%|████▎     | 20/46 [00:22<00:29,  1.13s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 3 [640/1453 (43%)]\tLoss: 0.191296\n",
            " 65%|██████▌   | 30/46 [00:34<00:18,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 3 [960/1453 (65%)]\tLoss: 0.157065\n",
            " 87%|████████▋ | 40/46 [00:45<00:06,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 3 [1280/1453 (87%)]\tLoss: 0.166193\n",
            "100%|██████████| 46/46 [00:52<00:00,  1.15s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 3\n",
            "100%|██████████| 3/3 [00:01<00:00,  1.51it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 3 \tAverage loss: 0.2265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.1640 (train) | 0.2265 (val)\n",
            "Epoch 4 / 35\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 4 [0/1453 (0%)]\tLoss: 0.124637\n",
            " 22%|██▏       | 10/46 [00:11<00:40,  1.13s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 4 [320/1453 (22%)]\tLoss: 0.143085\n",
            " 43%|████▎     | 20/46 [00:22<00:29,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 4 [640/1453 (43%)]\tLoss: 0.119544\n",
            " 65%|██████▌   | 30/46 [00:34<00:18,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 4 [960/1453 (65%)]\tLoss: 0.117113\n",
            " 87%|████████▋ | 40/46 [00:46<00:06,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 4 [1280/1453 (87%)]\tLoss: 0.122255\n",
            "100%|██████████| 46/46 [00:52<00:00,  1.15s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 4\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.50it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 4 \tAverage loss: 0.1960\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.1181 (train) | 0.1960 (val)\n",
            "Epoch 5 / 35\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 5 [0/1453 (0%)]\tLoss: 0.097777\n",
            " 22%|██▏       | 10/46 [00:11<00:40,  1.13s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 5 [320/1453 (22%)]\tLoss: 0.113777\n",
            " 43%|████▎     | 20/46 [00:22<00:29,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 5 [640/1453 (43%)]\tLoss: 0.109031\n",
            " 65%|██████▌   | 30/46 [00:34<00:18,  1.16s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 5 [960/1453 (65%)]\tLoss: 0.092815\n",
            " 87%|████████▋ | 40/46 [00:46<00:06,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 5 [1280/1453 (87%)]\tLoss: 0.102796\n",
            "100%|██████████| 46/46 [00:53<00:00,  1.15s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 5\n",
            "100%|██████████| 3/3 [00:01<00:00,  1.50it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 5 \tAverage loss: 0.1608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0953 (train) | 0.1608 (val)\n",
            "Epoch 6 / 35\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 6 [0/1453 (0%)]\tLoss: 0.092136\n",
            " 22%|██▏       | 10/46 [00:11<00:40,  1.13s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 6 [320/1453 (22%)]\tLoss: 0.072424\n",
            " 43%|████▎     | 20/46 [00:22<00:29,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 6 [640/1453 (43%)]\tLoss: 0.090808\n",
            " 65%|██████▌   | 30/46 [00:34<00:18,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 6 [960/1453 (65%)]\tLoss: 0.070029\n",
            " 87%|████████▋ | 40/46 [00:46<00:06,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 6 [1280/1453 (87%)]\tLoss: 0.086085\n",
            "100%|██████████| 46/46 [00:52<00:00,  1.15s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 6\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.50it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 6 \tAverage loss: 0.1367\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0829 (train) | 0.1367 (val)\n",
            "Epoch 7 / 35\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 7 [0/1453 (0%)]\tLoss: 0.073822\n",
            " 22%|██▏       | 10/46 [00:11<00:40,  1.13s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 7 [320/1453 (22%)]\tLoss: 0.070909\n",
            " 43%|████▎     | 20/46 [00:22<00:29,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 7 [640/1453 (43%)]\tLoss: 0.064590\n",
            " 65%|██████▌   | 30/46 [00:34<00:18,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 7 [960/1453 (65%)]\tLoss: 0.070017\n",
            " 87%|████████▋ | 40/46 [00:46<00:06,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 7 [1280/1453 (87%)]\tLoss: 0.069951\n",
            "100%|██████████| 46/46 [00:53<00:00,  1.15s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 7\n",
            "100%|██████████| 3/3 [00:01<00:00,  1.53it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 7 \tAverage loss: 0.1387\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0771 (train) | 0.1387 (val)\n",
            "Epoch 8 / 35\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 8 [0/1453 (0%)]\tLoss: 0.060617\n",
            " 22%|██▏       | 10/46 [00:11<00:40,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 8 [320/1453 (22%)]\tLoss: 0.076545\n",
            " 43%|████▎     | 20/46 [00:22<00:29,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 8 [640/1453 (43%)]\tLoss: 0.076688\n",
            " 65%|██████▌   | 30/46 [00:34<00:18,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 8 [960/1453 (65%)]\tLoss: 0.054473\n",
            " 87%|████████▋ | 40/46 [00:46<00:06,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 8 [1280/1453 (87%)]\tLoss: 0.058438\n",
            "100%|██████████| 46/46 [00:53<00:00,  1.15s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 8\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.48it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 8 \tAverage loss: 0.1335\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0746 (train) | 0.1335 (val)\n",
            "Epoch 9 / 35\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 9 [0/1453 (0%)]\tLoss: 0.070188\n",
            " 22%|██▏       | 10/46 [00:11<00:40,  1.13s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 9 [320/1453 (22%)]\tLoss: 0.102281\n",
            " 43%|████▎     | 20/46 [00:22<00:29,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 9 [640/1453 (43%)]\tLoss: 0.058803\n",
            " 65%|██████▌   | 30/46 [00:34<00:18,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 9 [960/1453 (65%)]\tLoss: 0.071266\n",
            " 87%|████████▋ | 40/46 [00:46<00:06,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 9 [1280/1453 (87%)]\tLoss: 0.103181\n",
            "100%|██████████| 46/46 [00:52<00:00,  1.15s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 9\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.50it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 9 \tAverage loss: 0.1326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0721 (train) | 0.1326 (val)\n",
            "Epoch 10 / 35\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 10 [0/1453 (0%)]\tLoss: 0.089212\n",
            " 22%|██▏       | 10/46 [00:11<00:40,  1.13s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 10 [320/1453 (22%)]\tLoss: 0.084708\n",
            " 43%|████▎     | 20/46 [00:22<00:29,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 10 [640/1453 (43%)]\tLoss: 0.058751\n",
            " 65%|██████▌   | 30/46 [00:34<00:18,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 10 [960/1453 (65%)]\tLoss: 0.074510\n",
            " 87%|████████▋ | 40/46 [00:46<00:06,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 10 [1280/1453 (87%)]\tLoss: 0.058625\n",
            "100%|██████████| 46/46 [00:53<00:00,  1.16s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 10\n",
            "100%|██████████| 3/3 [00:01<00:00,  1.51it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 10 \tAverage loss: 0.1236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0701 (train) | 0.1236 (val)\n",
            "Epoch 11 / 35\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 11 [0/1453 (0%)]\tLoss: 0.083979\n",
            " 22%|██▏       | 10/46 [00:11<00:40,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 11 [320/1453 (22%)]\tLoss: 0.071693\n",
            " 43%|████▎     | 20/46 [00:22<00:29,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 11 [640/1453 (43%)]\tLoss: 0.047907\n",
            " 65%|██████▌   | 30/46 [00:34<00:18,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 11 [960/1453 (65%)]\tLoss: 0.058046\n",
            " 87%|████████▋ | 40/46 [00:46<00:06,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 11 [1280/1453 (87%)]\tLoss: 0.056529\n",
            "100%|██████████| 46/46 [00:53<00:00,  1.15s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 11\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.50it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 11 \tAverage loss: 0.1166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0661 (train) | 0.1166 (val)\n",
            "Epoch 12 / 35\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 12 [0/1453 (0%)]\tLoss: 0.061472\n",
            " 22%|██▏       | 10/46 [00:11<00:40,  1.13s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 12 [320/1453 (22%)]\tLoss: 0.052382\n",
            " 43%|████▎     | 20/46 [00:22<00:29,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 12 [640/1453 (43%)]\tLoss: 0.066918\n",
            " 65%|██████▌   | 30/46 [00:34<00:18,  1.16s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 12 [960/1453 (65%)]\tLoss: 0.061310\n",
            " 87%|████████▋ | 40/46 [00:46<00:06,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 12 [1280/1453 (87%)]\tLoss: 0.061872\n",
            "100%|██████████| 46/46 [00:53<00:00,  1.15s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 12\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.48it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 12 \tAverage loss: 0.1172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0652 (train) | 0.1172 (val)\n",
            "Epoch 13 / 35\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 13 [0/1453 (0%)]\tLoss: 0.075208\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 13 [320/1453 (22%)]\tLoss: 0.061540\n",
            " 43%|████▎     | 20/46 [00:22<00:29,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 13 [640/1453 (43%)]\tLoss: 0.064038\n",
            " 65%|██████▌   | 30/46 [00:34<00:18,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 13 [960/1453 (65%)]\tLoss: 0.056519\n",
            " 87%|████████▋ | 40/46 [00:46<00:06,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 13 [1280/1453 (87%)]\tLoss: 0.069546\n",
            "100%|██████████| 46/46 [00:53<00:00,  1.16s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 13\n",
            "100%|██████████| 3/3 [00:01<00:00,  1.50it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 13 \tAverage loss: 0.1144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0643 (train) | 0.1144 (val)\n",
            "Epoch 14 / 35\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 14 [0/1453 (0%)]\tLoss: 0.070722\n",
            " 22%|██▏       | 10/46 [00:11<00:40,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 14 [320/1453 (22%)]\tLoss: 0.065300\n",
            " 43%|████▎     | 20/46 [00:22<00:29,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 14 [640/1453 (43%)]\tLoss: 0.059099\n",
            " 65%|██████▌   | 30/46 [00:34<00:18,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 14 [960/1453 (65%)]\tLoss: 0.061141\n",
            " 87%|████████▋ | 40/46 [00:46<00:06,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 14 [1280/1453 (87%)]\tLoss: 0.063849\n",
            "100%|██████████| 46/46 [00:53<00:00,  1.15s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 14\n",
            "100%|██████████| 3/3 [00:01<00:00,  1.51it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 14 \tAverage loss: 0.1145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0640 (train) | 0.1145 (val)\n",
            "Epoch 15 / 35\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 15 [0/1453 (0%)]\tLoss: 0.052135\n",
            " 22%|██▏       | 10/46 [00:11<00:40,  1.13s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 15 [320/1453 (22%)]\tLoss: 0.051601\n",
            " 43%|████▎     | 20/46 [00:22<00:29,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 15 [640/1453 (43%)]\tLoss: 0.068210\n",
            " 65%|██████▌   | 30/46 [00:34<00:18,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 15 [960/1453 (65%)]\tLoss: 0.064669\n",
            " 87%|████████▋ | 40/46 [00:46<00:06,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 15 [1280/1453 (87%)]\tLoss: 0.045507\n",
            "100%|██████████| 46/46 [00:53<00:00,  1.15s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 15\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.48it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 15 \tAverage loss: 0.1131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0636 (train) | 0.1131 (val)\n",
            "Epoch 16 / 35\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 16 [0/1453 (0%)]\tLoss: 0.065990\n",
            " 22%|██▏       | 10/46 [00:11<00:40,  1.13s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 16 [320/1453 (22%)]\tLoss: 0.054037\n",
            " 43%|████▎     | 20/46 [00:22<00:29,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 16 [640/1453 (43%)]\tLoss: 0.059199\n",
            " 65%|██████▌   | 30/46 [00:34<00:18,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 16 [960/1453 (65%)]\tLoss: 0.078190\n",
            " 87%|████████▋ | 40/46 [00:46<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 16 [1280/1453 (87%)]\tLoss: 0.063781\n",
            "100%|██████████| 46/46 [00:53<00:00,  1.16s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 16\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.49it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 16 \tAverage loss: 0.1122\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0630 (train) | 0.1122 (val)\n",
            "Epoch 17 / 35\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 17 [0/1453 (0%)]\tLoss: 0.067172\n",
            " 22%|██▏       | 10/46 [00:11<00:40,  1.13s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 17 [320/1453 (22%)]\tLoss: 0.073697\n",
            " 43%|████▎     | 20/46 [00:22<00:29,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 17 [640/1453 (43%)]\tLoss: 0.057208\n",
            " 65%|██████▌   | 30/46 [00:34<00:18,  1.16s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 17 [960/1453 (65%)]\tLoss: 0.049033\n",
            " 87%|████████▋ | 40/46 [00:46<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 17 [1280/1453 (87%)]\tLoss: 0.050459\n",
            "100%|██████████| 46/46 [00:53<00:00,  1.16s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 17\n",
            "100%|██████████| 3/3 [00:01<00:00,  1.50it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 17 \tAverage loss: 0.1120\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0624 (train) | 0.1120 (val)\n",
            "Epoch 18 / 35\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 18 [0/1453 (0%)]\tLoss: 0.056424\n",
            " 22%|██▏       | 10/46 [00:11<00:40,  1.13s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 18 [320/1453 (22%)]\tLoss: 0.043834\n",
            " 43%|████▎     | 20/46 [00:22<00:29,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 18 [640/1453 (43%)]\tLoss: 0.051956\n",
            " 65%|██████▌   | 30/46 [00:34<00:18,  1.16s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 18 [960/1453 (65%)]\tLoss: 0.066805\n",
            " 87%|████████▋ | 40/46 [00:46<00:06,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 18 [1280/1453 (87%)]\tLoss: 0.056543\n",
            "100%|██████████| 46/46 [00:53<00:00,  1.16s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 18\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.49it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 18 \tAverage loss: 0.1120\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0623 (train) | 0.1120 (val)\n",
            "Epoch 19 / 35\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 19 [0/1453 (0%)]\tLoss: 0.048035\n",
            " 22%|██▏       | 10/46 [00:11<00:40,  1.13s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 19 [320/1453 (22%)]\tLoss: 0.064931\n",
            " 43%|████▎     | 20/46 [00:22<00:29,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 19 [640/1453 (43%)]\tLoss: 0.061526\n",
            " 65%|██████▌   | 30/46 [00:34<00:18,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 19 [960/1453 (65%)]\tLoss: 0.047246\n",
            " 87%|████████▋ | 40/46 [00:46<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 19 [1280/1453 (87%)]\tLoss: 0.045890\n",
            "100%|██████████| 46/46 [00:53<00:00,  1.15s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 19\n",
            "100%|██████████| 3/3 [00:01<00:00,  1.51it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 19 \tAverage loss: 0.1118\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0626 (train) | 0.1118 (val)\n",
            "Epoch 20 / 35\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 20 [0/1453 (0%)]\tLoss: 0.063404\n",
            " 22%|██▏       | 10/46 [00:11<00:40,  1.13s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 20 [320/1453 (22%)]\tLoss: 0.062003\n",
            " 43%|████▎     | 20/46 [00:22<00:29,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 20 [640/1453 (43%)]\tLoss: 0.061113\n",
            " 65%|██████▌   | 30/46 [00:34<00:18,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 20 [960/1453 (65%)]\tLoss: 0.070885\n",
            " 87%|████████▋ | 40/46 [00:46<00:06,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 20 [1280/1453 (87%)]\tLoss: 0.080840\n",
            "100%|██████████| 46/46 [00:53<00:00,  1.16s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 20\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.50it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 20 \tAverage loss: 0.1112\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0620 (train) | 0.1112 (val)\n",
            "Epoch 21 / 35\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 21 [0/1453 (0%)]\tLoss: 0.057664\n",
            " 22%|██▏       | 10/46 [00:11<00:40,  1.13s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 21 [320/1453 (22%)]\tLoss: 0.059543\n",
            " 43%|████▎     | 20/46 [00:22<00:29,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 21 [640/1453 (43%)]\tLoss: 0.044838\n",
            " 65%|██████▌   | 30/46 [00:34<00:18,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 21 [960/1453 (65%)]\tLoss: 0.058950\n",
            " 87%|████████▋ | 40/46 [00:46<00:06,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 21 [1280/1453 (87%)]\tLoss: 0.053143\n",
            "100%|██████████| 46/46 [00:53<00:00,  1.15s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 21\n",
            "100%|██████████| 3/3 [00:01<00:00,  1.50it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 21 \tAverage loss: 0.1109\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0626 (train) | 0.1109 (val)\n",
            "Epoch 22 / 35\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 22 [0/1453 (0%)]\tLoss: 0.051163\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 22 [320/1453 (22%)]\tLoss: 0.067854\n",
            " 43%|████▎     | 20/46 [00:22<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 22 [640/1453 (43%)]\tLoss: 0.068382\n",
            " 65%|██████▌   | 30/46 [00:34<00:18,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 22 [960/1453 (65%)]\tLoss: 0.050956\n",
            " 87%|████████▋ | 40/46 [00:46<00:06,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 22 [1280/1453 (87%)]\tLoss: 0.053810\n",
            "100%|██████████| 46/46 [00:53<00:00,  1.16s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 22\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.48it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 22 \tAverage loss: 0.1110\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0622 (train) | 0.1110 (val)\n",
            "Epoch 23 / 35\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 23 [0/1453 (0%)]\tLoss: 0.062825\n",
            " 22%|██▏       | 10/46 [00:11<00:40,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 23 [320/1453 (22%)]\tLoss: 0.069890\n",
            " 43%|████▎     | 20/46 [00:22<00:29,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 23 [640/1453 (43%)]\tLoss: 0.061197\n",
            " 65%|██████▌   | 30/46 [00:34<00:18,  1.16s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 23 [960/1453 (65%)]\tLoss: 0.058661\n",
            " 87%|████████▋ | 40/46 [00:46<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 23 [1280/1453 (87%)]\tLoss: 0.045763\n",
            "100%|██████████| 46/46 [00:53<00:00,  1.16s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 23\n",
            "100%|██████████| 3/3 [00:01<00:00,  1.51it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 23 \tAverage loss: 0.1112\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0618 (train) | 0.1112 (val)\n",
            "Epoch 24 / 35\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 24 [0/1453 (0%)]\tLoss: 0.064877\n",
            " 22%|██▏       | 10/46 [00:11<00:40,  1.13s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 24 [320/1453 (22%)]\tLoss: 0.068653\n",
            " 43%|████▎     | 20/46 [00:22<00:29,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 24 [640/1453 (43%)]\tLoss: 0.057907\n",
            " 65%|██████▌   | 30/46 [00:34<00:18,  1.16s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 24 [960/1453 (65%)]\tLoss: 0.051331\n",
            " 87%|████████▋ | 40/46 [00:46<00:06,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 24 [1280/1453 (87%)]\tLoss: 0.068230\n",
            "100%|██████████| 46/46 [00:53<00:00,  1.16s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 24\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.50it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 24 \tAverage loss: 0.1115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0617 (train) | 0.1115 (val)\n",
            "Epoch 25 / 35\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 25 [0/1453 (0%)]\tLoss: 0.075847\n",
            " 22%|██▏       | 10/46 [00:11<00:40,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 25 [320/1453 (22%)]\tLoss: 0.063595\n",
            " 43%|████▎     | 20/46 [00:22<00:29,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 25 [640/1453 (43%)]\tLoss: 0.089550\n",
            " 65%|██████▌   | 30/46 [00:34<00:18,  1.16s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 25 [960/1453 (65%)]\tLoss: 0.086079\n",
            " 87%|████████▋ | 40/46 [00:46<00:06,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 25 [1280/1453 (87%)]\tLoss: 0.051914\n",
            "100%|██████████| 46/46 [00:53<00:00,  1.16s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 25\n",
            "100%|██████████| 3/3 [00:01<00:00,  1.51it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 25 \tAverage loss: 0.1113\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0621 (train) | 0.1113 (val)\n",
            "Epoch 26 / 35\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 26 [0/1453 (0%)]\tLoss: 0.061421\n",
            " 22%|██▏       | 10/46 [00:11<00:40,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 26 [320/1453 (22%)]\tLoss: 0.058478\n",
            " 43%|████▎     | 20/46 [00:22<00:29,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 26 [640/1453 (43%)]\tLoss: 0.063866\n",
            " 65%|██████▌   | 30/46 [00:34<00:18,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 26 [960/1453 (65%)]\tLoss: 0.066612\n",
            " 87%|████████▋ | 40/46 [00:46<00:06,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 26 [1280/1453 (87%)]\tLoss: 0.059889\n",
            "100%|██████████| 46/46 [00:53<00:00,  1.16s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 26\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.49it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 26 \tAverage loss: 0.1111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0619 (train) | 0.1111 (val)\n",
            "Epoch 27 / 35\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 27 [0/1453 (0%)]\tLoss: 0.063841\n",
            " 22%|██▏       | 10/46 [00:11<00:40,  1.13s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 27 [320/1453 (22%)]\tLoss: 0.065833\n",
            " 43%|████▎     | 20/46 [00:22<00:29,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 27 [640/1453 (43%)]\tLoss: 0.052990\n",
            " 65%|██████▌   | 30/46 [00:34<00:18,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 27 [960/1453 (65%)]\tLoss: 0.048912\n",
            " 87%|████████▋ | 40/46 [00:46<00:06,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 27 [1280/1453 (87%)]\tLoss: 0.063747\n",
            "100%|██████████| 46/46 [00:53<00:00,  1.16s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 27\n",
            "100%|██████████| 3/3 [00:01<00:00,  1.50it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 27 \tAverage loss: 0.1108\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0616 (train) | 0.1108 (val)\n",
            "Epoch 28 / 35\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 28 [0/1453 (0%)]\tLoss: 0.076104\n",
            " 22%|██▏       | 10/46 [00:11<00:40,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 28 [320/1453 (22%)]\tLoss: 0.050683\n",
            " 43%|████▎     | 20/46 [00:22<00:29,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 28 [640/1453 (43%)]\tLoss: 0.062244\n",
            " 65%|██████▌   | 30/46 [00:34<00:18,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 28 [960/1453 (65%)]\tLoss: 0.057051\n",
            " 87%|████████▋ | 40/46 [00:46<00:06,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 28 [1280/1453 (87%)]\tLoss: 0.060703\n",
            "100%|██████████| 46/46 [00:53<00:00,  1.16s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 28\n",
            "100%|██████████| 3/3 [00:01<00:00,  1.50it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 28 \tAverage loss: 0.1111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0619 (train) | 0.1111 (val)\n",
            "Epoch 29 / 35\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 29 [0/1453 (0%)]\tLoss: 0.058218\n",
            " 22%|██▏       | 10/46 [00:11<00:40,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 29 [320/1453 (22%)]\tLoss: 0.080630\n",
            " 43%|████▎     | 20/46 [00:22<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 29 [640/1453 (43%)]\tLoss: 0.081330\n",
            " 65%|██████▌   | 30/46 [00:34<00:18,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 29 [960/1453 (65%)]\tLoss: 0.071255\n",
            " 87%|████████▋ | 40/46 [00:46<00:06,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 29 [1280/1453 (87%)]\tLoss: 0.065741\n",
            "100%|██████████| 46/46 [00:53<00:00,  1.16s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 29\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.47it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 29 \tAverage loss: 0.1110\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0622 (train) | 0.1110 (val)\n",
            "Epoch 30 / 35\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 30 [0/1453 (0%)]\tLoss: 0.070124\n",
            " 22%|██▏       | 10/46 [00:11<00:40,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 30 [320/1453 (22%)]\tLoss: 0.066837\n",
            " 43%|████▎     | 20/46 [00:22<00:29,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 30 [640/1453 (43%)]\tLoss: 0.055405\n",
            " 65%|██████▌   | 30/46 [00:34<00:18,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 30 [960/1453 (65%)]\tLoss: 0.068088\n",
            " 87%|████████▋ | 40/46 [00:46<00:06,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 30 [1280/1453 (87%)]\tLoss: 0.052501\n",
            "100%|██████████| 46/46 [00:53<00:00,  1.16s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 30\n",
            "100%|██████████| 3/3 [00:01<00:00,  1.51it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 30 \tAverage loss: 0.1107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0615 (train) | 0.1107 (val)\n",
            "Epoch 31 / 35\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 31 [0/1453 (0%)]\tLoss: 0.090350\n",
            " 22%|██▏       | 10/46 [00:11<00:40,  1.13s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 31 [320/1453 (22%)]\tLoss: 0.060291\n",
            " 43%|████▎     | 20/46 [00:22<00:29,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 31 [640/1453 (43%)]\tLoss: 0.057026\n",
            " 65%|██████▌   | 30/46 [00:34<00:18,  1.16s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 31 [960/1453 (65%)]\tLoss: 0.056732\n",
            " 87%|████████▋ | 40/46 [00:46<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 31 [1280/1453 (87%)]\tLoss: 0.062917\n",
            "100%|██████████| 46/46 [00:53<00:00,  1.15s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 31\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.50it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 31 \tAverage loss: 0.1112\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0616 (train) | 0.1112 (val)\n",
            "Epoch 32 / 35\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 32 [0/1453 (0%)]\tLoss: 0.058861\n",
            " 22%|██▏       | 10/46 [00:11<00:40,  1.13s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 32 [320/1453 (22%)]\tLoss: 0.064756\n",
            " 43%|████▎     | 20/46 [00:22<00:29,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 32 [640/1453 (43%)]\tLoss: 0.057076\n",
            " 65%|██████▌   | 30/46 [00:34<00:18,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 32 [960/1453 (65%)]\tLoss: 0.057587\n",
            " 87%|████████▋ | 40/46 [00:46<00:06,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 32 [1280/1453 (87%)]\tLoss: 0.062506\n",
            "100%|██████████| 46/46 [00:53<00:00,  1.16s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 32\n",
            "100%|██████████| 3/3 [00:01<00:00,  1.53it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 32 \tAverage loss: 0.1117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0619 (train) | 0.1117 (val)\n",
            "Epoch 33 / 35\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 33 [0/1453 (0%)]\tLoss: 0.055859\n",
            " 22%|██▏       | 10/46 [00:11<00:40,  1.13s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 33 [320/1453 (22%)]\tLoss: 0.067134\n",
            " 43%|████▎     | 20/46 [00:22<00:29,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 33 [640/1453 (43%)]\tLoss: 0.057819\n",
            " 65%|██████▌   | 30/46 [00:34<00:18,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 33 [960/1453 (65%)]\tLoss: 0.053333\n",
            " 87%|████████▋ | 40/46 [00:46<00:06,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 33 [1280/1453 (87%)]\tLoss: 0.062534\n",
            "100%|██████████| 46/46 [00:53<00:00,  1.16s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 33\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.49it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 33 \tAverage loss: 0.1114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0619 (train) | 0.1114 (val)\n",
            "Epoch 34 / 35\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 34 [0/1453 (0%)]\tLoss: 0.049683\n",
            " 22%|██▏       | 10/46 [00:11<00:40,  1.13s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 34 [320/1453 (22%)]\tLoss: 0.076388\n",
            " 43%|████▎     | 20/46 [00:22<00:29,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 34 [640/1453 (43%)]\tLoss: 0.056779\n",
            " 65%|██████▌   | 30/46 [00:34<00:18,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 34 [960/1453 (65%)]\tLoss: 0.068250\n",
            " 87%|████████▋ | 40/46 [00:46<00:06,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 34 [1280/1453 (87%)]\tLoss: 0.053979\n",
            "100%|██████████| 46/46 [00:53<00:00,  1.16s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 34\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.47it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 34 \tAverage loss: 0.1112\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0617 (train) | 0.1112 (val)\n",
            "Epoch 35 / 35\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 35 [0/1453 (0%)]\tLoss: 0.071548\n",
            " 22%|██▏       | 10/46 [00:11<00:40,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 35 [320/1453 (22%)]\tLoss: 0.056910\n",
            " 43%|████▎     | 20/46 [00:22<00:29,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 35 [640/1453 (43%)]\tLoss: 0.059291\n",
            " 65%|██████▌   | 30/46 [00:34<00:18,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 35 [960/1453 (65%)]\tLoss: 0.054586\n",
            " 87%|████████▋ | 40/46 [00:46<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 35 [1280/1453 (87%)]\tLoss: 0.063771\n",
            "100%|██████████| 46/46 [00:53<00:00,  1.16s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 35\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.48it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 35 \tAverage loss: 0.1110\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0615 (train) | 0.1110 (val)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': {'loss': [0.8295200085196919,\n",
              "   0.3025659064403995,\n",
              "   0.16396001860222484,\n",
              "   0.11811087670731528,\n",
              "   0.09531932225718796,\n",
              "   0.08292783692181725,\n",
              "   0.07707919269102816,\n",
              "   0.07462675264393635,\n",
              "   0.07207524951826845,\n",
              "   0.07010624047012388,\n",
              "   0.06608697267970953,\n",
              "   0.06520390255711281,\n",
              "   0.06434988293537827,\n",
              "   0.06398085798332384,\n",
              "   0.06359853408076971,\n",
              "   0.06301429880563751,\n",
              "   0.062441296244766985,\n",
              "   0.06234117600418171,\n",
              "   0.06257430852197896,\n",
              "   0.061998228399571434,\n",
              "   0.0625529672824877,\n",
              "   0.06218699117672501,\n",
              "   0.061794787832501835,\n",
              "   0.06174231268829587,\n",
              "   0.06214735210352082,\n",
              "   0.06188053142148629,\n",
              "   0.06159811331047654,\n",
              "   0.061856805155257565,\n",
              "   0.062180163191358384,\n",
              "   0.061466460418102255,\n",
              "   0.06162265734843197,\n",
              "   0.06185568675798789,\n",
              "   0.06190455272147677,\n",
              "   0.061703895707726886,\n",
              "   0.061530219399961866]},\n",
              " 'val': {'loss': [0.6058417161305746,\n",
              "   0.3149687945842743,\n",
              "   0.22653631369272867,\n",
              "   0.19596164425214133,\n",
              "   0.1607591857512792,\n",
              "   0.1366692135731379,\n",
              "   0.13865318894386292,\n",
              "   0.13354641695817313,\n",
              "   0.13259869565566382,\n",
              "   0.12358329941829045,\n",
              "   0.11658458411693573,\n",
              "   0.11720849573612213,\n",
              "   0.11438791205485661,\n",
              "   0.11449216802914937,\n",
              "   0.11312618106603622,\n",
              "   0.11215139925479889,\n",
              "   0.11198837558428447,\n",
              "   0.11195216079552968,\n",
              "   0.11175157378117244,\n",
              "   0.11120281120141347,\n",
              "   0.11093448102474213,\n",
              "   0.11098096519708633,\n",
              "   0.11123203734556834,\n",
              "   0.11148762454589208,\n",
              "   0.11128056794404984,\n",
              "   0.11107083906730016,\n",
              "   0.1107974424958229,\n",
              "   0.11105632781982422,\n",
              "   0.11100761592388153,\n",
              "   0.11073843638102214,\n",
              "   0.11123137176036835,\n",
              "   0.11169175058603287,\n",
              "   0.11141497890154521,\n",
              "   0.11119165023167928,\n",
              "   0.11096848299105962]}}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Se inicializa el entrenamiento del modelo.\n",
        "modelhandler.train_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "k55JhgMyG09V",
        "outputId": "4541bafe-c787-463b-c757-307c1a1a8e42"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Se visualiza el proceso de entrenamiento.\n",
        "# Esta función traza la pérdida del modelo durante el entrenamiento.\n",
        "modelhandler.plot_loss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E52bTEXnG09W",
        "outputId": "e665f5a2-4bc9-448f-a47b-029ce8493633"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# Se busca la pérdida mínima en la validación, que corresponde al mejor modelo.\n",
        "# 'np.argmin' devuelve el índice de la pérdida mínima en el conjunto de validación.\n",
        "# Se suma 1 porque los índices en Python comienzan en 0, pero las épocas comienzan en 1.\n",
        "np.argmin(modelhandler.running_record['val']['loss'])+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "kH5xVXQyG09W",
        "outputId": "d6dcfaac-0d25-4377-bd5d-db55f384ffcb",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Entrenamiento/checkpoints/epoch_30/unetv13.pt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-7aac965efa01>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Se carga el mejor modelo entrenado y se verifica su rendimiento en el conjunto de prueba.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Se emplea `load_model` para cargar el modelo entrenado. Este método toma el nombre del archivo de punto de control.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodelhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Entrenamiento/checkpoints/epoch_30/unetv13.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pv_vision/nn/modelhandler.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;34m\"\"\" Load model from path \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         self.model.load_state_dict(\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         )\n\u001b[1;32m    347\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 997\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    998\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Entrenamiento/checkpoints/epoch_30/unetv13.pt'"
          ]
        }
      ],
      "source": [
        "# Se carga el mejor modelo entrenado y se verifica su rendimiento en el conjunto de prueba.\n",
        "# Se emplea `load_model` para cargar el modelo entrenado. Este método toma el nombre del archivo de punto de control.\n",
        "modelhandler.load_model('/content/drive/MyDrive/Entrenamiento/checkpoints/epoch_30/unetv13.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xa-Fdu8ZG09W"
      },
      "source": [
        "El siguiente código prueba el modelo en el conjunto de prueba y almacena la salida en 'testset_output'. También se hace un comentario sobre la puntuación de la prueba y la puntuación de la validación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1q3LEUNaG09W",
        "outputId": "2dbbfcbe-da9f-415e-a87d-98392575059a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing mode\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [04:18<00:00, 21.52s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Test set: Average loss: 0.1105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.1105\n"
          ]
        }
      ],
      "source": [
        "# Se evalúa el modelo en el conjunto de prueba. `test_model` es una función de ModelHandler\n",
        "# que evalúa el modelo en el conjunto de prueba y almacena la salida en la caché.\n",
        "_ = modelhandler.test_model(cache_output='testset_outputv13')\n",
        "\n",
        "# La salida del modelo se almacena en self.cache['testset_output']"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
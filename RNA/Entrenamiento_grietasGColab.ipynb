{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Franklingo13/PVDefectDetect/blob/main/RNA/Entrenamiento_grietasGColab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psMYf9fJG09O"
      },
      "source": [
        "Notebook para entrenamiento de redes neuronales convolucionales para clasificación de defectos en imágenes de celdas fotovoltaicas.\n",
        "Pensado para correr en Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "gbQ5zjRCG09Q",
        "outputId": "36ac4f16-9e2c-4149-d1f5-219f7b857a61"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-98d4ae4ff33e>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Conexión con Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         )\n\u001b[0;32m--> 283\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "# Conexión con Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OhRFEtnDGxpJ"
      },
      "outputs": [],
      "source": [
        "# SPDX-License-Identifier: Apache-2.0\n",
        "#\n",
        "# Copyright (C) 2021 Supervisely\n",
        "#\n",
        "# This file is part of the Supervisely project and has been taken\n",
        "# from the Supervisely repository (https://github.com/supervisely/supervisely/blob/master/plugins/nn/unet_v2/src/unet.py).\n",
        "# It is being redistributed under the Apache License 2.0.\n",
        "#\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.models.vgg import vgg16_bn\n",
        "\n",
        "\n",
        "class ConvBNAct(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.seq = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels,\n",
        "                      kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return self.seq(inputs)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, src_channels, dst_channels):\n",
        "        super().__init__()\n",
        "        self.seq1 = ConvBNAct(src_channels, dst_channels)\n",
        "        self.seq2 = ConvBNAct(dst_channels, dst_channels)\n",
        "        self.seq3 = ConvBNAct(dst_channels, dst_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        result = self.seq1(x)\n",
        "        result = self.seq2(result)\n",
        "        result = self.seq3(result)\n",
        "        return result\n",
        "\n",
        "\n",
        "class UNetUp(nn.Module):\n",
        "    def __init__(self, down_channels,  right_channels):\n",
        "        super().__init__()\n",
        "        self.bottom_up = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.conv = nn.Conv2d(down_channels, right_channels, kernel_size=1, stride=1)\n",
        "\n",
        "    def forward(self, left, bottom):\n",
        "        from_bottom = self.bottom_up(bottom)\n",
        "        from_bottom = self.conv(from_bottom)\n",
        "        result = torch.cat([left, from_bottom], 1)\n",
        "        return result\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), dilation=2, padding=2)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(3, 3), dilation=2, padding=2)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = self.bn(out)\n",
        "        out = self.conv2(self.relu(out))\n",
        "        out = self.bn2(out)\n",
        "        return torch.cat((x, self.relu2(out)), dim=1)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder_blocks,  encoder_channels, n_cls):\n",
        "        self.encoder_channels = encoder_channels\n",
        "        self.depth = len(self.encoder_channels)\n",
        "        assert len(encoder_blocks) == self.depth\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder_blocks = nn.ModuleList(encoder_blocks)\n",
        "\n",
        "        self.blocks = nn.ModuleList()\n",
        "        # add bottleneck\n",
        "        self.blocks.append(Block(\n",
        "            self.encoder_channels[-1],\n",
        "            self.encoder_channels[-1]\n",
        "        ))\n",
        "\n",
        "        self.ups = nn.ModuleList()\n",
        "        for i in range(1, self.depth):\n",
        "            bottom_channels = self.encoder_channels[self.depth - i]\n",
        "            left_channels = self.encoder_channels[self.depth - i - 1]\n",
        "            right_channels = left_channels\n",
        "            self.ups.append(UNetUp(bottom_channels,  right_channels))\n",
        "            self.blocks.append(Block(\n",
        "                left_channels + right_channels,\n",
        "                right_channels\n",
        "            ))\n",
        "        self.last_conv = nn.Conv2d(encoder_channels[0], n_cls, 1)\n",
        "        # self.dropout = nn.Dropout2d(p=0.1)\n",
        "        self.bottle = Bottleneck(512, 512)\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoder_outputs = []\n",
        "        for encoder_block in self.encoder_blocks:\n",
        "            x = encoder_block(x)\n",
        "            encoder_outputs.append(x)\n",
        "        x = self.bottle(encoder_outputs[self.depth - 1])\n",
        "        for i in range(self.depth):\n",
        "            if i > 0:\n",
        "                encoder_output = encoder_outputs[self.depth - i - 1]\n",
        "                x = self.ups[i - 1](encoder_output, x)\n",
        "                x = self.blocks[i](x)\n",
        "        # x = self.dropout(x)\n",
        "        x = self.last_conv(x)\n",
        "        return x  # no softmax or log_softmax\n",
        "\n",
        "\n",
        "def _get_encoder_blocks(model):\n",
        "    # last modules (ReLUs) of VGG blocks\n",
        "    layers_last_module_names = ['5', '12', '22', '32', '42']\n",
        "    result = []\n",
        "    cur_block = nn.Sequential()\n",
        "    for name, child in model.named_children():\n",
        "        if name == 'features':\n",
        "            for name2, child2 in child.named_children():\n",
        "                cur_block.add_module(name2, child2)\n",
        "                if name2 in layers_last_module_names:\n",
        "                    result.append(cur_block)\n",
        "                    cur_block = nn.Sequential()\n",
        "            break\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def construct_unet(n_cls, pretrain=False):  # no weights inited\n",
        "    model = vgg16_bn(weights='DEFAULT')\n",
        "    encoder_blocks = _get_encoder_blocks(model)\n",
        "    encoder_channels = [64, 128, 256, 512, 1024]  # vgg16 channels\n",
        "    # prev_channels = encoder_channels[-1]\n",
        "\n",
        "    return UNet(encoder_blocks, encoder_channels, n_cls)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "U_8l2-gnG09S"
      },
      "outputs": [],
      "source": [
        "# Importación de librerías\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torch.nn import DataParallel\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torchvision.io import read_image, ImageReadMode\n",
        "from torchvision.datasets.vision import VisionDataset\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50, DeepLabV3_ResNet50_Weights\n",
        "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
        "from torchvision.utils import draw_segmentation_masks\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "\n",
        "import requests\n",
        "import copy\n",
        "#from unet_model import construct_unet\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from imutils.paths import list_images\n",
        "import os\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "u-13tOJejCxA",
        "outputId": "f7532efe-3513-4c65-c4a7-b20ead6a00f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pv-vision\n",
            "  Downloading pv_vision-0.2.8-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: imutils>=0.5.4 in /usr/local/lib/python3.10/dist-packages (from pv-vision) (0.5.4)\n",
            "Collecting ipywidgets>=8.1.2 (from pv-vision)\n",
            "  Downloading ipywidgets-8.1.3-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pv-vision) (1.4.2)\n",
            "Collecting matplotlib>=3.8.0 (from pv-vision)\n",
            "  Downloading matplotlib-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: opencv-python>=4.7.0.72 in /usr/local/lib/python3.10/dist-packages (from pv-vision) (4.10.0.84)\n",
            "Requirement already satisfied: scikit-learn>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pv-vision) (1.3.2)\n",
            "Requirement already satisfied: setuptools>=68.2.2 in /usr/local/lib/python3.10/dist-packages (from pv-vision) (71.0.4)\n",
            "Requirement already satisfied: torch>=2.2.0.post100 in /usr/local/lib/python3.10/dist-packages (from pv-vision) (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.15.2a0 in /usr/local/lib/python3.10/dist-packages (from pv-vision) (0.18.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from pv-vision) (4.66.5)\n",
            "Collecting comm>=0.1.3 (from ipywidgets>=8.1.2->pv-vision)\n",
            "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.1.2->pv-vision) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.1.2->pv-vision) (5.7.1)\n",
            "Collecting widgetsnbextension~=4.0.11 (from ipywidgets>=8.1.2->pv-vision)\n",
            "  Downloading widgetsnbextension-4.0.11-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.1.2->pv-vision) (3.0.11)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.8.0->pv-vision) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.8.0->pv-vision) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.8.0->pv-vision) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.8.0->pv-vision) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.8.0->pv-vision) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.8.0->pv-vision) (24.1)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.8.0->pv-vision) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.8.0->pv-vision) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.8.0->pv-vision) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->pv-vision) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->pv-vision) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0.post100->pv-vision) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0.post100->pv-vision) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0.post100->pv-vision) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0.post100->pv-vision) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0.post100->pv-vision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0.post100->pv-vision) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.2.0.post100->pv-vision)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.2.0.post100->pv-vision)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.2.0.post100->pv-vision)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.2.0.post100->pv-vision)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.2.0.post100->pv-vision)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.2.0.post100->pv-vision)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.2.0.post100->pv-vision)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.2.0.post100->pv-vision)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.2.0.post100->pv-vision)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2.2.0.post100->pv-vision)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.2.0.post100->pv-vision)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0.post100->pv-vision) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.2.0.post100->pv-vision)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting jedi>=0.16 (from ipython>=6.1.0->ipywidgets>=8.1.2->pv-vision)\n",
            "  Using cached jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->pv-vision) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->pv-vision) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->pv-vision) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->pv-vision) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->pv-vision) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->pv-vision) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->pv-vision) (4.9.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.8.0->pv-vision) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.2.0.post100->pv-vision) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.2.0.post100->pv-vision) (1.3.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.1.2->pv-vision) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.1.2->pv-vision) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.1.2->pv-vision) (0.2.13)\n",
            "Downloading pv_vision-0.2.8-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.3/48.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipywidgets-8.1.3-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Downloading widgetsnbextension-4.0.11-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: widgetsnbextension, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jedi, comm, nvidia-cusparse-cu12, nvidia-cudnn-cu12, matplotlib, nvidia-cusolver-cu12, ipywidgets, pv-vision\n",
            "  Attempting uninstall: widgetsnbextension\n",
            "    Found existing installation: widgetsnbextension 3.6.8\n",
            "    Uninstalling widgetsnbextension-3.6.8:\n",
            "      Successfully uninstalled widgetsnbextension-3.6.8\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.7.1\n",
            "    Uninstalling matplotlib-3.7.1:\n",
            "      Successfully uninstalled matplotlib-3.7.1\n",
            "  Attempting uninstall: ipywidgets\n",
            "    Found existing installation: ipywidgets 7.7.1\n",
            "    Uninstalling ipywidgets-7.7.1:\n",
            "      Successfully uninstalled ipywidgets-7.7.1\n",
            "Successfully installed comm-0.2.2 ipywidgets-8.1.3 jedi-0.19.1 matplotlib-3.9.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 pv-vision-0.2.8 widgetsnbextension-4.0.11\n"
          ]
        }
      ],
      "source": [
        "# Importación de la librería de pv-vision\n",
        "!pip install pv-vision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YVtXGzixG09T"
      },
      "outputs": [],
      "source": [
        "# Importar el manejador de modelo: ModelHandler\n",
        "from pv_vision.nn import ModelHandler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ia6yr7DDG09T"
      },
      "outputs": [],
      "source": [
        "# Definición de una clase para el conjunto de datos solar,\n",
        "# que hereda de la clase VisionDataset de PyTorch.\n",
        "class SolarDataset(VisionDataset):\n",
        "    \"\"\"Un conjunto de datos que lee directamente las imágenes y las máscaras desde una carpeta.\"\"\"\n",
        "\n",
        "    # Se definió el método de inicialización para la clase.\n",
        "    def __init__(self,\n",
        "                 root,\n",
        "                 image_folder,\n",
        "                 mask_folder,\n",
        "                 transforms,\n",
        "                 mode = \"train\",\n",
        "                 random_seed=42):\n",
        "        # Se llamó al método de inicialización de la clase padre.\n",
        "        super().__init__(root, transforms)\n",
        "        # Se establecieron las rutas a las carpetas de imágenes y máscaras.\n",
        "        self.image_path = Path(self.root) / image_folder\n",
        "        self.mask_path = Path(self.root) / mask_folder\n",
        "\n",
        "        # Se verificó que las carpetas de imágenes y máscaras existan.\n",
        "        if not os.path.exists(self.image_path):\n",
        "            raise OSError(f\"{self.image_path} no encontrado.\")\n",
        "\n",
        "        if not os.path.exists(self.mask_path):\n",
        "            raise OSError(f\"{self.mask_path} no encontrado.\")\n",
        "\n",
        "        # Se obtuvieron las listas de imágenes y máscaras y se ordenaron.\n",
        "        self.image_list = sorted(list(list_images(self.image_path)))\n",
        "        self.mask_list = sorted(list(list_images(self.mask_path)))\n",
        "\n",
        "        # Se convirtieron las listas de imágenes y máscaras a arrays de numpy.\n",
        "        self.image_list = np.array(self.image_list)\n",
        "        self.mask_list = np.array(self.mask_list)\n",
        "\n",
        "        # Se estableció la semilla para la generación de números aleatorios y se mezclaron las imágenes y las máscaras.\n",
        "        np.random.seed(random_seed)\n",
        "        index = np.arange(len(self.image_list))\n",
        "        np.random.shuffle(index)\n",
        "        self.image_list = self.image_list[index]\n",
        "        self.mask_list = self.mask_list[index]\n",
        "\n",
        "    # Se definió el método para obtener la longitud del conjunto de datos.\n",
        "    def __len__(self):\n",
        "        return len(self.image_list)\n",
        "\n",
        "    # Se definió un método para obtener el nombre de una imagen o máscara.\n",
        "    def __getname__(self, index):\n",
        "        image_name = os.path.splitext(os.path.split(self.image_list[index])[-1])[0]\n",
        "        mask_name = os.path.splitext(os.path.split(self.mask_list[index])[-1])[0]\n",
        "\n",
        "        if image_name == mask_name:\n",
        "            return image_name\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    # Se definió un método para obtener una imagen y su máscara correspondiente.\n",
        "    def __getraw__(self, index):\n",
        "        if not self.__getname__(index):\n",
        "            raise ValueError(\"{}: La imagen no coincide con la máscara\".format(os.path.split(self.image_list[index])[-1]))\n",
        "        image = Image.open(self.image_list[index])\n",
        "        mask = Image.open(self.mask_list[index]).convert('L')\n",
        "        mask = np.array(mask)\n",
        "        mask = Image.fromarray(mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "    # Se definió el método para obtener un elemento del conjunto de datos.\n",
        "    def __getitem__(self, index):\n",
        "        image, mask = self.__getraw__(index)\n",
        "        image, mask = self.transforms(image, mask)\n",
        "\n",
        "        return image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "t1nDW9d6G09T"
      },
      "outputs": [],
      "source": [
        "# Definición de una clase para componer varias transformaciones.\n",
        "class Compose:\n",
        "    def __init__(self, transforms):\n",
        "        \"\"\"\n",
        "        transforms: una lista de transformaciones\n",
        "        \"\"\"\n",
        "        self.transforms = transforms\n",
        "\n",
        "    # Se definió el método para aplicar las transformaciones a la imagen y la máscara.\n",
        "    def __call__(self, image, target):\n",
        "        \"\"\"\n",
        "        image: imagen de entrada\n",
        "        target: máscara de entrada\n",
        "        \"\"\"\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target\n",
        "\n",
        "# Se definió una clase para redimensionar la imagen y la máscara a un tamaño fijo.\n",
        "class FixResize:\n",
        "    # UNet requiere que el tamaño de entrada sea múltiplo de 16\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "\n",
        "    # Se definió el método para redimensionar la imagen y la máscara.\n",
        "    def __call__(self, image, target):\n",
        "        image = F.resize(image, (self.size, self.size), interpolation=transforms.InterpolationMode.BILINEAR)\n",
        "        target = F.resize(target, (self.size, self.size), interpolation=transforms.InterpolationMode.NEAREST)\n",
        "        return image, target\n",
        "\n",
        "# Se definió una clase para transformar la imagen y la máscara a tensores.\n",
        "class ToTensor:\n",
        "    \"\"\"Transforma la imagen a tensor. Escala la imagen a [0,1] float32.\n",
        "    Transforma la máscara a tensor.\n",
        "    \"\"\"\n",
        "    def __call__(self, image, target):\n",
        "        image = transforms.ToTensor()(image)\n",
        "        target = torch.as_tensor(np.array(target), dtype=torch.int64)\n",
        "        return image, target\n",
        "\n",
        "# Se definió una clase para transformar la imagen a tensor manteniendo el tipo original.\n",
        "class PILToTensor:\n",
        "    \"\"\"Transforma la imagen a tensor. Mantiene el tipo original.\"\"\"\n",
        "    def __call__(self, image, target):\n",
        "        image = F.pil_to_tensor(image)\n",
        "        target = torch.as_tensor(np.array(target), dtype=torch.int64)\n",
        "        return image, target\n",
        "\n",
        "# Se definió una clase para normalizar la imagen.\n",
        "class Normalize:\n",
        "    def __init__(self, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        # Verifica si la imagen es en escala de grises (1 canal) y la convierte a RGB (3 canales) si es necesario\n",
        "        if image.shape[0] == 1:\n",
        "            image = image.repeat(3, 1, 1)  # Repite el canal existente 3 veces\n",
        "        image = F.normalize(image, mean=self.mean, std=self.std)\n",
        "        return image, target"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definición de una clase personalizada para la rotación de imagen y máscara\n",
        "class RandomRotation:\n",
        "    def __init__(self, degrees):\n",
        "        self.degrees = degrees\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        # Rotar la imagen\n",
        "        angle = transforms.RandomRotation.get_params([-self.degrees, self.degrees])\n",
        "        image = F.rotate(image, angle, interpolation=transforms.InterpolationMode.BILINEAR)\n",
        "        # Rotar la máscara\n",
        "        target = F.rotate(target, angle, interpolation=transforms.InterpolationMode.NEAREST)\n",
        "        return image, target\n"
      ],
      "metadata": {
        "id": "iggYqFz__qYf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "pRAdQ8o1G09U",
        "outputId": "8187ff5b-dbb6-4d40-af79-1e8760cfa7bb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "/content/drive/MyDrive/Entrenamiento/train/img no encontrado.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-dde3bb5b92a3>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#transformers = Compose([FixResize(256), ToTensor(), Normalize()])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Se crean los conjuntos de datos de entrenamiento, validación y prueba.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m trainset = SolarDataset(root, image_folder=\"train/img\",\n\u001b[0m\u001b[1;32m     18\u001b[0m         mask_folder=\"train/ann\", transforms=transformers)\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-e6bb7c4996e8>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, image_folder, mask_folder, transforms, mode, random_seed)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Se verificó que las carpetas de imágenes y máscaras existan.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.image_path} no encontrado.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: /content/drive/MyDrive/Entrenamiento/train/img no encontrado."
          ]
        }
      ],
      "source": [
        "# Ruta al directorio que contiene las imágenes y las máscaras.\n",
        "# root = Path(\n",
        "#     '/content/drive/MyDrive/Trabajo de titulación/PV_vision/Entrenamiento')\n",
        "\n",
        "root = Path(\n",
        "    '/content/drive/MyDrive/Entrenamiento')\n",
        "\n",
        "# Se definen las transformaciones a aplicar a las imágenes y las etiquetas.\n",
        "transformers = Compose([\n",
        "    #RandomRotation(degrees=10),\n",
        "    FixResize(256),\n",
        "    ToTensor(),\n",
        "    Normalize()\n",
        "])\n",
        "#transformers = Compose([FixResize(256), ToTensor(), Normalize()])\n",
        "# Se crean los conjuntos de datos de entrenamiento, validación y prueba.\n",
        "trainset = SolarDataset(root, image_folder=\"train/img\",\n",
        "        mask_folder=\"train/ann\", transforms=transformers)\n",
        "\n",
        "valset = SolarDataset(root, image_folder=\"val/img\",\n",
        "        mask_folder=\"val/ann\", transforms=transformers)\n",
        "\n",
        "testset = SolarDataset(root, image_folder=\"test/img\",\n",
        "        mask_folder=\"test/ann\", transforms=transformers)\n",
        "\n",
        "# Verificación de que la carpeta haya sido establecida correctamente\n",
        "print(f\"El conjunto de datos de entrenamiento contiene {len(trainset)} elementos.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhN5cKIpjCxD"
      },
      "outputs": [],
      "source": [
        "class Accuracy:\n",
        "    \"\"\"Calcular la precisión de un modelo\"\"\"\n",
        "    def __init__(self):\n",
        "        self.__name__ = \"accuracy\"\n",
        "        self._base = [0, 0]\n",
        "\n",
        "    def calc(self, outputs, targets, reduction='mean'):\n",
        "        \"\"\" Calcular la precisión.\n",
        "        Argumentos:\n",
        "        -----------\n",
        "        outputs: torch.Tensor\n",
        "        La salida del modelo, forma (batch_size, num_classes, H, W)\n",
        "\n",
        "        targets: torch.Tensor\n",
        "        La etiqueta verdadera, forma (batch_size, H, W)\n",
        "\n",
        "        reduction: str\n",
        "        El método de reducción, 'mean' o 'sum'\n",
        "        Si es 'mean', devuelve la precisión media del lote\n",
        "        Si es 'sum', devuelve la suma de predicciones correctas del lote\n",
        "\n",
        "        Retorna:\n",
        "        --------\n",
        "        accuracy: torch.Tensor\n",
        "        \"\"\"\n",
        "        # Asegúrate de que las dimensiones de outputs y targets sean compatibles\n",
        "        if outputs.dim() == 4 and targets.dim() == 3:\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct = torch.sum(preds == targets)\n",
        "\n",
        "            if reduction == 'mean':\n",
        "                return correct.float() / targets.numel()\n",
        "            elif reduction == 'sum':\n",
        "                return correct\n",
        "            else:\n",
        "                raise ValueError(\"reduction debe ser 'mean' o 'sum'\")\n",
        "        else:\n",
        "            raise ValueError(\"Las dimensiones de outputs y targets no son compatibles\")\n",
        "\n",
        "    def accumulate(self, outputs, targets):\n",
        "        \"\"\" Acumular la métrica a lo largo de varios lotes.\"\"\"\n",
        "        if outputs.dim() == 4 and targets.dim() == 3:\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct = torch.sum(preds == targets)\n",
        "            self._base[0] += correct\n",
        "            self._base[1] += targets.numel()\n",
        "        else:\n",
        "            raise ValueError(\"Las dimensiones de outputs y targets no son compatibles\")\n",
        "\n",
        "    def reset(self):\n",
        "        self._base = [0, 0]\n",
        "\n",
        "    def accumulated_score(self):\n",
        "        \"\"\" Devolver la puntuación acumulada en una época.\"\"\"\n",
        "        if self._base[1] == 0:\n",
        "            # advertencia de división por cero\n",
        "            warnings.warn(\"El denominador es cero, devuelve 0\", RuntimeWarning)\n",
        "            return 0\n",
        "        return self._base[0].float() / self._base[1]\n",
        "\n",
        "    def __call__(self, outputs, targets, reduction='mean'):\n",
        "        return self.calc(outputs, targets, reduction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zB7a3yuP4Dps"
      },
      "outputs": [],
      "source": [
        "## Definición de una clase para calcular el Índice de Jaccard, o Intersección sobre Unión (IoU)\n",
        "class JaccardIndex:\n",
        "    \"\"\"Calcular el índice de Jaccard (IoU) de un modelo\"\"\"\n",
        "    def __init__(self):\n",
        "        self.__name__ = \"jaccard_index\"\n",
        "        self._base_intersection = 0\n",
        "        self._base_union = 0\n",
        "\n",
        "    def calc(self, outputs, targets, reduction='mean'):\n",
        "        \"\"\"\n",
        "        Calcular el índice de Jaccard.\n",
        "\n",
        "        Argumentos:\n",
        "        -----------\n",
        "        outputs: torch.Tensor\n",
        "            La salida del modelo, forma (batch_size, num_classes, H, W)\n",
        "        targets: torch.Tensor\n",
        "            La etiqueta verdadera, forma (batch_size, H, W)\n",
        "        reduction: str\n",
        "            El método de reducción, 'mean' o 'sum'.\n",
        "\n",
        "        Retorna:\n",
        "        --------\n",
        "        iou: torch.Tensor\n",
        "        \"\"\"\n",
        "\n",
        "        if outputs.dim() == 4 and targets.dim() == 3:\n",
        "            # Convertir targets a formato one-hot\n",
        "            targets = torch.nn.functional.one_hot(targets, num_classes=outputs.shape[1])\n",
        "            targets = targets.permute(0, 3, 1, 2).float()  # Cambiar dimensiones para coincidir con outputs\n",
        "\n",
        "            # Obtener la predicción con la mayor probabilidad por clase\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            preds = torch.nn.functional.one_hot(preds, num_classes=outputs.shape[1])\n",
        "            preds = preds.permute(0, 3, 1, 2).float()\n",
        "\n",
        "            intersection = torch.sum(preds * targets, dim=(0, 2, 3))\n",
        "            union = torch.sum(preds + targets, dim=(0, 2, 3)) - intersection\n",
        "\n",
        "            iou_per_class = intersection / (union + 1e-8)  # Evitar división por cero\n",
        "\n",
        "            if reduction == 'mean':\n",
        "                return torch.mean(iou_per_class).item()\n",
        "            elif reduction == 'sum':\n",
        "                return torch.sum(iou_per_class).item()\n",
        "            else:\n",
        "                raise ValueError(\"reduction debe ser 'mean' o 'sum'\")\n",
        "        else:\n",
        "            raise ValueError(\"Las dimensiones de outputs y targets no son compatibles\")\n",
        "\n",
        "    def accumulate(self, outputs, targets):\n",
        "        \"\"\" Acumular la métrica a lo largo de varios lotes.\"\"\"\n",
        "        if outputs.dim() == 4 and targets.dim() == 3:\n",
        "            targets = torch.nn.functional.one_hot(targets, num_classes=outputs.shape[1])\n",
        "            targets = targets.permute(0, 3, 1, 2).float()\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            preds = torch.nn.functional.one_hot(preds, num_classes=outputs.shape[1])\n",
        "            preds = preds.permute(0, 3, 1, 2).float()\n",
        "\n",
        "            intersection = torch.sum(preds * targets, dim=(0, 2, 3))\n",
        "            union = torch.sum(preds + targets, dim=(0, 2, 3)) - intersection\n",
        "\n",
        "            self._base_intersection += intersection\n",
        "            self._base_union += union\n",
        "        else:\n",
        "            raise ValueError(\"Las dimensiones de outputs y targets no son compatibles\")\n",
        "\n",
        "    def reset(self):\n",
        "        self._base_intersection = 0\n",
        "        self._base_union = 0\n",
        "\n",
        "    def accumulated_score(self):\n",
        "        \"\"\" Devolver la puntuación acumulada en una época.\"\"\"\n",
        "        if self._base_union.sum() == 0:\n",
        "            # advertencia de división por cero\n",
        "            warnings.warn(\"El denominador es cero, devuelve 0\", RuntimeWarning)\n",
        "            return 0\n",
        "        return torch.mean(self._base_intersection.float() / (self._base_union + 1e-8)).item()\n",
        "\n",
        "    def __call__(self, outputs, targets, reduction='mean'):\n",
        "        return self.calc(outputs, targets, reduction)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaZs0hwDG09U"
      },
      "outputs": [],
      "source": [
        "# Se define una función para crear un modelo DeepLab preentrenado.\n",
        "def DeepLab_pretrained(num_classes):\n",
        "    # Se carga el modelo DeepLab con una arquitectura ResNet50 preentrenada.\n",
        "    deeplab = deeplabv3_resnet50(weights=DeepLabV3_ResNet50_Weights.DEFAULT)\n",
        "\n",
        "    # Se reemplaza el clasificador del modelo con un nuevo clasificador DeepLabHead.\n",
        "    # El nuevo clasificador tiene 2048 características de entrada y 'num_classes' características de salida.\n",
        "    deeplab.classifier = DeepLabHead(2048, num_classes)\n",
        "\n",
        "    # Se devuelve el modelo modificado.\n",
        "    return deeplab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZFPZp57F3wK"
      },
      "outputs": [],
      "source": [
        "# Crea una instancia del modelo U-Net con 5 canales de salida.\n",
        "# Número de canales de salida = al número de clases\n",
        "unet = construct_unet(5)\n",
        "# Se \"envuelve\" el modelo en un objeto DataParallel.\n",
        "# Esto permite que el modelo se ejecute en paralelo en múltiples GPUs, si están disponibles.\n",
        "unet = DataParallel(unet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qnmr0nyOG09U",
        "outputId": "3bc25514-7857-4e33-bac1-bb8846ed4a7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dispositivo utilizado: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Se define el dispositivo en el que se ejecutará el modelo.\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Se imprime el dispositivo utilizado.\n",
        "print(f\"Dispositivo utilizado: {device}\")\n",
        "\n",
        "# Se crea el modelo utilizando la función DeepLab_pretrained definida anteriormente.\n",
        "# El modelo se envuelve en un objeto DataParallel para permitir el entrenamiento en múltiples GPUs si están disponibles.\n",
        "#model = DataParallel(DeepLab_pretrained(5))\n",
        "\n",
        "# Se define la función de pérdida a utilizar durante el entrenamiento.\n",
        "# En este caso, se utiliza la pérdida de entropía cruzada.\n",
        "criterion = CrossEntropyLoss()\n",
        "\n",
        "# Se define el optimizador a utilizar durante el entrenamiento. En este caso, se utiliza Adam con una tasa de aprendizaje de 0.01.\n",
        "#optimizer = Adam(model.parameters(), lr=0.01)\n",
        "optimizer = Adam(unet.parameters(), lr=0.0015)\n",
        "\n",
        "# Se define el programador de la tasa de aprendizaje a utilizar durante el entrenamiento.\n",
        "# En este caso, se utiliza un programador de paso que disminuye la tasa de aprendizaje en un factor de 0.2 cada 5 épocas.\n",
        "lr_scheduler = StepLR(optimizer, step_size=8, gamma=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qouTmOWmA8ng",
        "outputId": "ba5e0818-c500-4b04-c7d0-aeece8f9f86a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Cargar los pesos del modelo preentrenado\n",
        "\n",
        "weight_path = '/content/drive/MyDrive/Entrenamiento/unetv25.pt'\n",
        "unet.load_state_dict(torch.load(weight_path, map_location=device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjJv6uo4G09V",
        "outputId": "f5608115-33e0-457e-98c5-b939288342fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pv_vision.nn.modelhandler:ModelHandler initialized.\n"
          ]
        }
      ],
      "source": [
        "# Se inicializa el manejador del modelo.\n",
        "# La salida se almacena en la carpeta de salida.\n",
        "modelhandler = ModelHandler(\n",
        "    # Se pasa el modelo que se va a entrenar.\n",
        "    #model=model,\n",
        "    model = unet,\n",
        "    # Se especifica el nombre de la carpeta de salida.\n",
        "    #model_output='out_unet',\n",
        "    # Se pasan los conjuntos de datos de entrenamiento, validación y prueba.\n",
        "    train_dataset=trainset,\n",
        "    val_dataset=valset,\n",
        "    test_dataset=testset,\n",
        "    # Se especifica el tamaño del lote para el entrenamiento y la validación.\n",
        "    batch_size_train=16,\n",
        "    batch_size_val=16,\n",
        "    # Se pasa el programador de la tasa de aprendizaje.\n",
        "    lr_scheduler=lr_scheduler,\n",
        "    # Se especifica el número de épocas para el entrenamiento.\n",
        "    num_epochs=42,\n",
        "    # Se pasa la función de pérdida y el optimizador.\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    # Se pasa el dispositivo en el que se ejecutará el entrenamiento.\n",
        "    device=device,\n",
        "    #evaluate_metric= JaccardIndex,\n",
        "    # Se especifica el directorio donde se guardarán los puntos de control del modelo.\n",
        "    save_dir='/content/drive/MyDrive/Entrenamiento/checkpoints',\n",
        "    # Se especifica el nombre del archivo de punto de control.\n",
        "    save_name='unetv28.pt'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "X1SfRwQCG09V",
        "outputId": "bbf0a183-2dd6-4cf5-9a64-86ee46e4a4da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 1 [0/1453 (0%)]\tLoss: 0.080259\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 1 [320/1453 (22%)]\tLoss: 0.080451\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 1 [640/1453 (43%)]\tLoss: 0.061422\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 1 [960/1453 (65%)]\tLoss: 0.054443\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 1 [1280/1453 (87%)]\tLoss: 0.062055\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.19s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 1\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.25it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 1 \tAverage loss: 0.1123\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0652 (train) | 0.1123 (val)\n",
            "Epoch 2 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 2 [0/1453 (0%)]\tLoss: 0.055648\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.16s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 2 [320/1453 (22%)]\tLoss: 0.035349\n",
            " 43%|████▎     | 20/46 [00:23<00:30,  1.16s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 2 [640/1453 (43%)]\tLoss: 0.054989\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.16s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 2 [960/1453 (65%)]\tLoss: 0.057757\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 2 [1280/1453 (87%)]\tLoss: 0.056520\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.19s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 2\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.27it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 2 \tAverage loss: 0.1043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0559 (train) | 0.1043 (val)\n",
            "Epoch 3 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 3 [0/1453 (0%)]\tLoss: 0.056071\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 3 [320/1453 (22%)]\tLoss: 0.061719\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 3 [640/1453 (43%)]\tLoss: 0.053497\n",
            " 65%|██████▌   | 30/46 [00:35<00:19,  1.19s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 3 [960/1453 (65%)]\tLoss: 0.047765\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 3 [1280/1453 (87%)]\tLoss: 0.071607\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.19s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 3\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.28it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 3 \tAverage loss: 0.0978\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0542 (train) | 0.0978 (val)\n",
            "Epoch 4 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 4 [0/1453 (0%)]\tLoss: 0.050060\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 4 [320/1453 (22%)]\tLoss: 0.037157\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 4 [640/1453 (43%)]\tLoss: 0.040780\n",
            " 65%|██████▌   | 30/46 [00:35<00:19,  1.20s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 4 [960/1453 (65%)]\tLoss: 0.044557\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 4 [1280/1453 (87%)]\tLoss: 0.046523\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.19s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 4\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.29it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 4 \tAverage loss: 0.0986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0527 (train) | 0.0986 (val)\n",
            "Epoch 5 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 5 [0/1453 (0%)]\tLoss: 0.054097\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 5 [320/1453 (22%)]\tLoss: 0.074632\n",
            " 43%|████▎     | 20/46 [00:23<00:30,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 5 [640/1453 (43%)]\tLoss: 0.055898\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 5 [960/1453 (65%)]\tLoss: 0.066649\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 5 [1280/1453 (87%)]\tLoss: 0.047344\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.18s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 5\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.28it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 5 \tAverage loss: 0.0982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0523 (train) | 0.0982 (val)\n",
            "Epoch 6 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 6 [0/1453 (0%)]\tLoss: 0.042918\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 6 [320/1453 (22%)]\tLoss: 0.047620\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 6 [640/1453 (43%)]\tLoss: 0.050751\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 6 [960/1453 (65%)]\tLoss: 0.047484\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 6 [1280/1453 (87%)]\tLoss: 0.057454\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.18s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 6\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.24it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 6 \tAverage loss: 0.1142\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0516 (train) | 0.1142 (val)\n",
            "Epoch 7 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 7 [0/1453 (0%)]\tLoss: 0.051005\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 7 [320/1453 (22%)]\tLoss: 0.063688\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 7 [640/1453 (43%)]\tLoss: 0.055936\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.16s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 7 [960/1453 (65%)]\tLoss: 0.053044\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 7 [1280/1453 (87%)]\tLoss: 0.039895\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.18s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 7\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.29it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 7 \tAverage loss: 0.0976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0531 (train) | 0.0976 (val)\n",
            "Epoch 8 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 8 [0/1453 (0%)]\tLoss: 0.049765\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 8 [320/1453 (22%)]\tLoss: 0.046612\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 8 [640/1453 (43%)]\tLoss: 0.050105\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.16s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 8 [960/1453 (65%)]\tLoss: 0.052363\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 8 [1280/1453 (87%)]\tLoss: 0.047726\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.19s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 8\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.29it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 8 \tAverage loss: 0.1020\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0513 (train) | 0.1020 (val)\n",
            "Epoch 9 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 9 [0/1453 (0%)]\tLoss: 0.054376\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 9 [320/1453 (22%)]\tLoss: 0.033160\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 9 [640/1453 (43%)]\tLoss: 0.062201\n",
            " 65%|██████▌   | 30/46 [00:35<00:19,  1.19s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 9 [960/1453 (65%)]\tLoss: 0.038929\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 9 [1280/1453 (87%)]\tLoss: 0.055873\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.18s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 9\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.24it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 9 \tAverage loss: 0.0884\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0493 (train) | 0.0884 (val)\n",
            "Epoch 10 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 10 [0/1453 (0%)]\tLoss: 0.041031\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 10 [320/1453 (22%)]\tLoss: 0.034905\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 10 [640/1453 (43%)]\tLoss: 0.041721\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 10 [960/1453 (65%)]\tLoss: 0.060992\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 10 [1280/1453 (87%)]\tLoss: 0.045913\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.19s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 10\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.28it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 10 \tAverage loss: 0.0881\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0478 (train) | 0.0881 (val)\n",
            "Epoch 11 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 11 [0/1453 (0%)]\tLoss: 0.046635\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 11 [320/1453 (22%)]\tLoss: 0.043634\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 11 [640/1453 (43%)]\tLoss: 0.042452\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 11 [960/1453 (65%)]\tLoss: 0.039601\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 11 [1280/1453 (87%)]\tLoss: 0.038329\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.18s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 11\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.27it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 11 \tAverage loss: 0.0863\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0471 (train) | 0.0863 (val)\n",
            "Epoch 12 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 12 [0/1453 (0%)]\tLoss: 0.039916\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 12 [320/1453 (22%)]\tLoss: 0.038437\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 12 [640/1453 (43%)]\tLoss: 0.053159\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 12 [960/1453 (65%)]\tLoss: 0.045817\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 12 [1280/1453 (87%)]\tLoss: 0.049851\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.18s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 12\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.27it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 12 \tAverage loss: 0.0859\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0473 (train) | 0.0859 (val)\n",
            "Epoch 13 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 13 [0/1453 (0%)]\tLoss: 0.043873\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 13 [320/1453 (22%)]\tLoss: 0.049200\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 13 [640/1453 (43%)]\tLoss: 0.063937\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.16s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 13 [960/1453 (65%)]\tLoss: 0.052746\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 13 [1280/1453 (87%)]\tLoss: 0.041432\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.18s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 13\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.26it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 13 \tAverage loss: 0.0851\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0471 (train) | 0.0851 (val)\n",
            "Epoch 14 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 14 [0/1453 (0%)]\tLoss: 0.054949\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 14 [320/1453 (22%)]\tLoss: 0.055088\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 14 [640/1453 (43%)]\tLoss: 0.046925\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.16s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 14 [960/1453 (65%)]\tLoss: 0.037032\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 14 [1280/1453 (87%)]\tLoss: 0.041828\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.19s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 14\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.28it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 14 \tAverage loss: 0.0850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0467 (train) | 0.0850 (val)\n",
            "Epoch 15 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 15 [0/1453 (0%)]\tLoss: 0.044653\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 15 [320/1453 (22%)]\tLoss: 0.044282\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 15 [640/1453 (43%)]\tLoss: 0.044470\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 15 [960/1453 (65%)]\tLoss: 0.044651\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 15 [1280/1453 (87%)]\tLoss: 0.048896\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.18s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 15\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.25it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 15 \tAverage loss: 0.0844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0467 (train) | 0.0844 (val)\n",
            "Epoch 16 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 16 [0/1453 (0%)]\tLoss: 0.039193\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 16 [320/1453 (22%)]\tLoss: 0.057288\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 16 [640/1453 (43%)]\tLoss: 0.048214\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 16 [960/1453 (65%)]\tLoss: 0.047453\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 16 [1280/1453 (87%)]\tLoss: 0.041637\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.18s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 16\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.26it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 16 \tAverage loss: 0.0847\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0466 (train) | 0.0847 (val)\n",
            "Epoch 17 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 17 [0/1453 (0%)]\tLoss: 0.058535\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 17 [320/1453 (22%)]\tLoss: 0.042618\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 17 [640/1453 (43%)]\tLoss: 0.052768\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 17 [960/1453 (65%)]\tLoss: 0.040758\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 17 [1280/1453 (87%)]\tLoss: 0.052119\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.19s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 17\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.27it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 17 \tAverage loss: 0.0842\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0464 (train) | 0.0842 (val)\n",
            "Epoch 18 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 18 [0/1453 (0%)]\tLoss: 0.042662\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 18 [320/1453 (22%)]\tLoss: 0.055573\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 18 [640/1453 (43%)]\tLoss: 0.044416\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 18 [960/1453 (65%)]\tLoss: 0.039518\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 18 [1280/1453 (87%)]\tLoss: 0.042819\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.19s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 18\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.30it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 18 \tAverage loss: 0.0843\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0467 (train) | 0.0843 (val)\n",
            "Epoch 19 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 19 [0/1453 (0%)]\tLoss: 0.044746\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 19 [320/1453 (22%)]\tLoss: 0.043892\n",
            " 43%|████▎     | 20/46 [00:23<00:30,  1.16s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 19 [640/1453 (43%)]\tLoss: 0.040256\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 19 [960/1453 (65%)]\tLoss: 0.040720\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 19 [1280/1453 (87%)]\tLoss: 0.048823\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.19s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 19\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.26it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 19 \tAverage loss: 0.0837\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0463 (train) | 0.0837 (val)\n",
            "Epoch 20 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 20 [0/1453 (0%)]\tLoss: 0.048171\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 20 [320/1453 (22%)]\tLoss: 0.052626\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 20 [640/1453 (43%)]\tLoss: 0.056777\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 20 [960/1453 (65%)]\tLoss: 0.045611\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 20 [1280/1453 (87%)]\tLoss: 0.048835\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.18s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 20\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.27it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 20 \tAverage loss: 0.0842\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0464 (train) | 0.0842 (val)\n",
            "Epoch 21 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 21 [0/1453 (0%)]\tLoss: 0.057701\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 21 [320/1453 (22%)]\tLoss: 0.045291\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 21 [640/1453 (43%)]\tLoss: 0.047743\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 21 [960/1453 (65%)]\tLoss: 0.050399\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 21 [1280/1453 (87%)]\tLoss: 0.044207\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.19s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 21\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.26it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 21 \tAverage loss: 0.0845\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0462 (train) | 0.0845 (val)\n",
            "Epoch 22 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 22 [0/1453 (0%)]\tLoss: 0.036661\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 22 [320/1453 (22%)]\tLoss: 0.046435\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 22 [640/1453 (43%)]\tLoss: 0.053285\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 22 [960/1453 (65%)]\tLoss: 0.049942\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 22 [1280/1453 (87%)]\tLoss: 0.049097\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.18s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 22\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.26it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 22 \tAverage loss: 0.0839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0464 (train) | 0.0839 (val)\n",
            "Epoch 23 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 23 [0/1453 (0%)]\tLoss: 0.043878\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 23 [320/1453 (22%)]\tLoss: 0.042035\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 23 [640/1453 (43%)]\tLoss: 0.040897\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.16s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 23 [960/1453 (65%)]\tLoss: 0.043369\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 23 [1280/1453 (87%)]\tLoss: 0.046276\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.19s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 23\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.25it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 23 \tAverage loss: 0.0841\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0465 (train) | 0.0841 (val)\n",
            "Epoch 24 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 24 [0/1453 (0%)]\tLoss: 0.042714\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 24 [320/1453 (22%)]\tLoss: 0.057736\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 24 [640/1453 (43%)]\tLoss: 0.052119\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.16s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 24 [960/1453 (65%)]\tLoss: 0.054256\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 24 [1280/1453 (87%)]\tLoss: 0.042358\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.19s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 24\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.28it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 24 \tAverage loss: 0.0849\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0464 (train) | 0.0849 (val)\n",
            "Epoch 25 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 25 [0/1453 (0%)]\tLoss: 0.046018\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 25 [320/1453 (22%)]\tLoss: 0.047405\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 25 [640/1453 (43%)]\tLoss: 0.047679\n",
            " 65%|██████▌   | 30/46 [00:35<00:19,  1.19s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 25 [960/1453 (65%)]\tLoss: 0.044006\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 25 [1280/1453 (87%)]\tLoss: 0.043087\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.19s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 25\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.23it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 25 \tAverage loss: 0.0839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0464 (train) | 0.0839 (val)\n",
            "Epoch 26 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 26 [0/1453 (0%)]\tLoss: 0.050834\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 26 [320/1453 (22%)]\tLoss: 0.036446\n",
            " 43%|████▎     | 20/46 [00:23<00:30,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 26 [640/1453 (43%)]\tLoss: 0.036274\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 26 [960/1453 (65%)]\tLoss: 0.035388\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 26 [1280/1453 (87%)]\tLoss: 0.054036\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.18s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 26\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.28it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 26 \tAverage loss: 0.0840\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0462 (train) | 0.0840 (val)\n",
            "Epoch 27 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 27 [0/1453 (0%)]\tLoss: 0.066901\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 27 [320/1453 (22%)]\tLoss: 0.035371\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 27 [640/1453 (43%)]\tLoss: 0.037638\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 27 [960/1453 (65%)]\tLoss: 0.039310\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 27 [1280/1453 (87%)]\tLoss: 0.044996\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.18s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 27\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.26it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 27 \tAverage loss: 0.0843\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0464 (train) | 0.0843 (val)\n",
            "Epoch 28 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 28 [0/1453 (0%)]\tLoss: 0.045074\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 28 [320/1453 (22%)]\tLoss: 0.046261\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 28 [640/1453 (43%)]\tLoss: 0.053887\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 28 [960/1453 (65%)]\tLoss: 0.046201\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 28 [1280/1453 (87%)]\tLoss: 0.042706\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.19s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 28\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.27it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 28 \tAverage loss: 0.0842\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0463 (train) | 0.0842 (val)\n",
            "Epoch 29 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 29 [0/1453 (0%)]\tLoss: 0.051280\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 29 [320/1453 (22%)]\tLoss: 0.062401\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 29 [640/1453 (43%)]\tLoss: 0.038644\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.16s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 29 [960/1453 (65%)]\tLoss: 0.042326\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 29 [1280/1453 (87%)]\tLoss: 0.036296\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.18s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 29\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.28it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 29 \tAverage loss: 0.0844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0463 (train) | 0.0844 (val)\n",
            "Epoch 30 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 30 [0/1453 (0%)]\tLoss: 0.036178\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 30 [320/1453 (22%)]\tLoss: 0.043659\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 30 [640/1453 (43%)]\tLoss: 0.063864\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 30 [960/1453 (65%)]\tLoss: 0.038513\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 30 [1280/1453 (87%)]\tLoss: 0.048498\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.18s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 30\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.26it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 30 \tAverage loss: 0.0836\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0465 (train) | 0.0836 (val)\n",
            "Epoch 31 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 31 [0/1453 (0%)]\tLoss: 0.044703\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 31 [320/1453 (22%)]\tLoss: 0.053371\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 31 [640/1453 (43%)]\tLoss: 0.054860\n",
            " 65%|██████▌   | 30/46 [00:35<00:19,  1.19s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 31 [960/1453 (65%)]\tLoss: 0.042632\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 31 [1280/1453 (87%)]\tLoss: 0.034467\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.18s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 31\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.26it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 31 \tAverage loss: 0.0840\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0463 (train) | 0.0840 (val)\n",
            "Epoch 32 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 32 [0/1453 (0%)]\tLoss: 0.043737\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 32 [320/1453 (22%)]\tLoss: 0.051129\n",
            " 43%|████▎     | 20/46 [00:23<00:30,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 32 [640/1453 (43%)]\tLoss: 0.046763\n",
            " 65%|██████▌   | 30/46 [00:35<00:19,  1.19s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 32 [960/1453 (65%)]\tLoss: 0.038869\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 32 [1280/1453 (87%)]\tLoss: 0.028596\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.19s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 32\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.29it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 32 \tAverage loss: 0.0848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0462 (train) | 0.0848 (val)\n",
            "Epoch 33 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 33 [0/1453 (0%)]\tLoss: 0.054863\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 33 [320/1453 (22%)]\tLoss: 0.055895\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 33 [640/1453 (43%)]\tLoss: 0.042315\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 33 [960/1453 (65%)]\tLoss: 0.060566\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 33 [1280/1453 (87%)]\tLoss: 0.043440\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.18s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 33\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.30it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 33 \tAverage loss: 0.0835\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0464 (train) | 0.0835 (val)\n",
            "Epoch 34 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 34 [0/1453 (0%)]\tLoss: 0.043292\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 34 [320/1453 (22%)]\tLoss: 0.052332\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 34 [640/1453 (43%)]\tLoss: 0.053536\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 34 [960/1453 (65%)]\tLoss: 0.041014\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 34 [1280/1453 (87%)]\tLoss: 0.054948\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.18s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 34\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.25it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 34 \tAverage loss: 0.0839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0463 (train) | 0.0839 (val)\n",
            "Epoch 35 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 35 [0/1453 (0%)]\tLoss: 0.045213\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 35 [320/1453 (22%)]\tLoss: 0.038596\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 35 [640/1453 (43%)]\tLoss: 0.034286\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.16s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 35 [960/1453 (65%)]\tLoss: 0.045700\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 35 [1280/1453 (87%)]\tLoss: 0.041286\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.18s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 35\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.25it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 35 \tAverage loss: 0.0848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0463 (train) | 0.0848 (val)\n",
            "Epoch 36 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 36 [0/1453 (0%)]\tLoss: 0.048601\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 36 [320/1453 (22%)]\tLoss: 0.035568\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 36 [640/1453 (43%)]\tLoss: 0.052120\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.16s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 36 [960/1453 (65%)]\tLoss: 0.041014\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 36 [1280/1453 (87%)]\tLoss: 0.045862\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.19s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 36\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.26it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 36 \tAverage loss: 0.0840\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0465 (train) | 0.0840 (val)\n",
            "Epoch 37 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 37 [0/1453 (0%)]\tLoss: 0.041925\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 37 [320/1453 (22%)]\tLoss: 0.041179\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 37 [640/1453 (43%)]\tLoss: 0.059047\n",
            " 65%|██████▌   | 30/46 [00:35<00:19,  1.19s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 37 [960/1453 (65%)]\tLoss: 0.052208\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 37 [1280/1453 (87%)]\tLoss: 0.049361\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.19s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 37\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.27it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 37 \tAverage loss: 0.0846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0465 (train) | 0.0846 (val)\n",
            "Epoch 38 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 38 [0/1453 (0%)]\tLoss: 0.034448\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.16s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 38 [320/1453 (22%)]\tLoss: 0.045965\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 38 [640/1453 (43%)]\tLoss: 0.047060\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 38 [960/1453 (65%)]\tLoss: 0.051390\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 38 [1280/1453 (87%)]\tLoss: 0.040428\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.18s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 38\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.27it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 38 \tAverage loss: 0.0837\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0463 (train) | 0.0837 (val)\n",
            "Epoch 39 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 39 [0/1453 (0%)]\tLoss: 0.033267\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 39 [320/1453 (22%)]\tLoss: 0.058216\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 39 [640/1453 (43%)]\tLoss: 0.054941\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 39 [960/1453 (65%)]\tLoss: 0.050412\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 39 [1280/1453 (87%)]\tLoss: 0.048150\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.18s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 39\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.26it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 39 \tAverage loss: 0.0841\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0463 (train) | 0.0841 (val)\n",
            "Epoch 40 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 40 [0/1453 (0%)]\tLoss: 0.036754\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.14s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 40 [320/1453 (22%)]\tLoss: 0.040872\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 40 [640/1453 (43%)]\tLoss: 0.034568\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 40 [960/1453 (65%)]\tLoss: 0.050446\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 40 [1280/1453 (87%)]\tLoss: 0.043332\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.18s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 40\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.22it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 40 \tAverage loss: 0.0841\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0465 (train) | 0.0841 (val)\n",
            "Epoch 41 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 41 [0/1453 (0%)]\tLoss: 0.056760\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 41 [320/1453 (22%)]\tLoss: 0.047264\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 41 [640/1453 (43%)]\tLoss: 0.047801\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 41 [960/1453 (65%)]\tLoss: 0.041235\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 41 [1280/1453 (87%)]\tLoss: 0.051418\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.18s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 41\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.27it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 41 \tAverage loss: 0.0849\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0464 (train) | 0.0849 (val)\n",
            "Epoch 42 / 42\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 42 [0/1453 (0%)]\tLoss: 0.039351\n",
            " 22%|██▏       | 10/46 [00:11<00:41,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 42 [320/1453 (22%)]\tLoss: 0.037854\n",
            " 43%|████▎     | 20/46 [00:23<00:29,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 42 [640/1453 (43%)]\tLoss: 0.040365\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.16s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 42 [960/1453 (65%)]\tLoss: 0.050480\n",
            " 87%|████████▋ | 40/46 [00:47<00:06,  1.15s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 42 [1280/1453 (87%)]\tLoss: 0.035157\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.19s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 42\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.26it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 42 \tAverage loss: 0.0840\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0462 (train) | 0.0840 (val)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': {'loss': [0.06522080486323698,\n",
              "   0.055914750144511194,\n",
              "   0.05424404321706303,\n",
              "   0.05271866004106429,\n",
              "   0.05229005487526767,\n",
              "   0.051581521692230056,\n",
              "   0.05314865997554677,\n",
              "   0.05129974427474669,\n",
              "   0.04925886364442188,\n",
              "   0.047791516845462574,\n",
              "   0.047131369373692535,\n",
              "   0.04725211304442602,\n",
              "   0.047104462986351785,\n",
              "   0.046659965101432076,\n",
              "   0.046749551410684896,\n",
              "   0.04657376631684165,\n",
              "   0.0463704926802098,\n",
              "   0.04667727225149244,\n",
              "   0.04634283431494802,\n",
              "   0.046444506418403475,\n",
              "   0.04622440824396677,\n",
              "   0.0463903228568358,\n",
              "   0.0464971008603522,\n",
              "   0.04641150927244181,\n",
              "   0.04641410776626137,\n",
              "   0.046217202824903866,\n",
              "   0.046448274518774035,\n",
              "   0.046334759470047164,\n",
              "   0.04625593471172261,\n",
              "   0.04650191464560736,\n",
              "   0.046349362195235486,\n",
              "   0.046230674002778506,\n",
              "   0.04635138221018411,\n",
              "   0.046347349315512536,\n",
              "   0.046315222053038035,\n",
              "   0.0465270994283664,\n",
              "   0.04646099755170012,\n",
              "   0.046348039937671594,\n",
              "   0.04630372312537818,\n",
              "   0.04647394406615006,\n",
              "   0.04635599134291195,\n",
              "   0.04624035722958738]},\n",
              " 'val': {'loss': [0.11228397488594055,\n",
              "   0.10427644352118175,\n",
              "   0.09780437747637431,\n",
              "   0.09855812291304271,\n",
              "   0.09815900027751923,\n",
              "   0.11422661195198695,\n",
              "   0.0976360763112704,\n",
              "   0.10204129169384639,\n",
              "   0.08837259809176128,\n",
              "   0.08807128916184108,\n",
              "   0.08629353592793147,\n",
              "   0.08589889605840047,\n",
              "   0.08512144535779953,\n",
              "   0.08501521994670232,\n",
              "   0.08436164259910583,\n",
              "   0.08466627945502599,\n",
              "   0.08424766113360722,\n",
              "   0.0842512771487236,\n",
              "   0.0837000161409378,\n",
              "   0.08418549845616023,\n",
              "   0.08446051677068074,\n",
              "   0.08393947283426921,\n",
              "   0.08411608636379242,\n",
              "   0.08494827896356583,\n",
              "   0.08391280720631282,\n",
              "   0.08397361636161804,\n",
              "   0.08426050593455632,\n",
              "   0.08418662597735722,\n",
              "   0.08438774198293686,\n",
              "   0.08364196866750717,\n",
              "   0.0840130125482877,\n",
              "   0.08484414716561635,\n",
              "   0.0834889014561971,\n",
              "   0.08390574902296066,\n",
              "   0.08475665499766667,\n",
              "   0.08398936192194621,\n",
              "   0.08459095160166423,\n",
              "   0.08374912043412526,\n",
              "   0.08405932039022446,\n",
              "   0.08413411180178325,\n",
              "   0.08492746949195862,\n",
              "   0.08395615220069885]}}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Se inicializa el entrenamiento del modelo.\n",
        "modelhandler.train_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "k55JhgMyG09V",
        "outputId": "5a362526-bc41-4746-899a-7149dbc419a8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Se visualiza el proceso de entrenamiento.\n",
        "# Esta función traza la pérdida del modelo durante el entrenamiento.\n",
        "modelhandler.plot_loss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E52bTEXnG09W",
        "outputId": "67b8d89b-a624-4958-8ee9-d9045fa30869"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# Se busca la pérdida mínima en la validación, que corresponde al mejor modelo.\n",
        "# 'np.argmin' devuelve el índice de la pérdida mínima en el conjunto de validación.\n",
        "# Se suma 1 porque los índices en Python comienzan en 0, pero las épocas comienzan en 1.\n",
        "np.argmin(modelhandler.running_record['val']['loss'])+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kH5xVXQyG09W",
        "outputId": "764992de-9a15-42fb-af92-c7648046e3bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pv_vision.nn.modelhandler:Loaded model from /content/drive/MyDrive/Entrenamiento/checkpoints/epoch_33/unetv28.pt\n"
          ]
        }
      ],
      "source": [
        "# Se carga el mejor modelo entrenado y se verifica su rendimiento en el conjunto de prueba.\n",
        "# Se emplea `load_model` para cargar el modelo entrenado. Este método toma el nombre del archivo de punto de control.\n",
        "modelhandler.load_model('/content/drive/MyDrive/Entrenamiento/checkpoints/epoch_33/unetv28.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xa-Fdu8ZG09W"
      },
      "source": [
        "El siguiente código prueba el modelo en el conjunto de prueba y almacena la salida en 'testset_output'. También se hace un comentario sobre la puntuación de la prueba y la puntuación de la validación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1q3LEUNaG09W",
        "outputId": "3663db8b-f181-4ea1-e952-9fbc8117ea54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing mode\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:09<00:00,  1.22it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Test set: Average loss: 0.1326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.1326\n"
          ]
        }
      ],
      "source": [
        "# Se evalúa el modelo en el conjunto de prueba. `test_model` es una función de ModelHandler\n",
        "# que evalúa el modelo en el conjunto de prueba y almacena la salida en la caché.\n",
        "_ = modelhandler.test_model(cache_output='testset_outputv28')\n",
        "\n",
        "# La salida del modelo se almacena en self.cache['testset_output']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Libera la caché de la GPU\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "CxrtcIqePCUe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
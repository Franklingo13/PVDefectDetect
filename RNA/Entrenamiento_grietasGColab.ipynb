{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Franklingo13/PVDefectDetect/blob/main/RNA/Entrenamiento_grietasGColab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psMYf9fJG09O"
      },
      "source": [
        "Notebook para entrenamiento de redes neuronales convolucionales para clasificación de defectos en imágenes de celdas fotovoltaicas.\n",
        "Pensado para correr en Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbQ5zjRCG09Q",
        "outputId": "973fccca-1d53-453a-e5c6-74b4c590f2b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Conexión con Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OhRFEtnDGxpJ"
      },
      "outputs": [],
      "source": [
        "# SPDX-License-Identifier: Apache-2.0\n",
        "#\n",
        "# Copyright (C) 2021 Supervisely\n",
        "#\n",
        "# This file is part of the Supervisely project and has been taken\n",
        "# from the Supervisely repository (https://github.com/supervisely/supervisely/blob/master/plugins/nn/unet_v2/src/unet.py).\n",
        "# It is being redistributed under the Apache License 2.0.\n",
        "#\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.models.vgg import vgg16_bn\n",
        "\n",
        "\n",
        "class ConvBNAct(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.seq = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels,\n",
        "                      kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return self.seq(inputs)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, src_channels, dst_channels):\n",
        "        super().__init__()\n",
        "        self.seq1 = ConvBNAct(src_channels, dst_channels)\n",
        "        self.seq2 = ConvBNAct(dst_channels, dst_channels)\n",
        "        self.seq3 = ConvBNAct(dst_channels, dst_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        result = self.seq1(x)\n",
        "        result = self.seq2(result)\n",
        "        result = self.seq3(result)\n",
        "        return result\n",
        "\n",
        "\n",
        "class UNetUp(nn.Module):\n",
        "    def __init__(self, down_channels,  right_channels):\n",
        "        super().__init__()\n",
        "        self.bottom_up = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.conv = nn.Conv2d(down_channels, right_channels, kernel_size=1, stride=1)\n",
        "\n",
        "    def forward(self, left, bottom):\n",
        "        from_bottom = self.bottom_up(bottom)\n",
        "        from_bottom = self.conv(from_bottom)\n",
        "        result = torch.cat([left, from_bottom], 1)\n",
        "        return result\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), dilation=2, padding=2)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(3, 3), dilation=2, padding=2)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = self.bn(out)\n",
        "        out = self.conv2(self.relu(out))\n",
        "        out = self.bn2(out)\n",
        "        return torch.cat((x, self.relu2(out)), dim=1)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder_blocks,  encoder_channels, n_cls):\n",
        "        self.encoder_channels = encoder_channels\n",
        "        self.depth = len(self.encoder_channels)\n",
        "        assert len(encoder_blocks) == self.depth\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder_blocks = nn.ModuleList(encoder_blocks)\n",
        "\n",
        "        self.blocks = nn.ModuleList()\n",
        "        # add bottleneck\n",
        "        self.blocks.append(Block(\n",
        "            self.encoder_channels[-1],\n",
        "            self.encoder_channels[-1]\n",
        "        ))\n",
        "\n",
        "        self.ups = nn.ModuleList()\n",
        "        for i in range(1, self.depth):\n",
        "            bottom_channels = self.encoder_channels[self.depth - i]\n",
        "            left_channels = self.encoder_channels[self.depth - i - 1]\n",
        "            right_channels = left_channels\n",
        "            self.ups.append(UNetUp(bottom_channels,  right_channels))\n",
        "            self.blocks.append(Block(\n",
        "                left_channels + right_channels,\n",
        "                right_channels\n",
        "            ))\n",
        "        self.last_conv = nn.Conv2d(encoder_channels[0], n_cls, 1)\n",
        "        # self.dropout = nn.Dropout2d(p=0.1)\n",
        "        self.bottle = Bottleneck(512, 512)\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoder_outputs = []\n",
        "        for encoder_block in self.encoder_blocks:\n",
        "            x = encoder_block(x)\n",
        "            encoder_outputs.append(x)\n",
        "        x = self.bottle(encoder_outputs[self.depth - 1])\n",
        "        for i in range(self.depth):\n",
        "            if i > 0:\n",
        "                encoder_output = encoder_outputs[self.depth - i - 1]\n",
        "                x = self.ups[i - 1](encoder_output, x)\n",
        "                x = self.blocks[i](x)\n",
        "        # x = self.dropout(x)\n",
        "        x = self.last_conv(x)\n",
        "        return x  # no softmax or log_softmax\n",
        "\n",
        "\n",
        "def _get_encoder_blocks(model):\n",
        "    # last modules (ReLUs) of VGG blocks\n",
        "    layers_last_module_names = ['5', '12', '22', '32', '42']\n",
        "    result = []\n",
        "    cur_block = nn.Sequential()\n",
        "    for name, child in model.named_children():\n",
        "        if name == 'features':\n",
        "            for name2, child2 in child.named_children():\n",
        "                cur_block.add_module(name2, child2)\n",
        "                if name2 in layers_last_module_names:\n",
        "                    result.append(cur_block)\n",
        "                    cur_block = nn.Sequential()\n",
        "            break\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def construct_unet(n_cls, pretrain=False):  # no weights inited\n",
        "    model = vgg16_bn(weights='DEFAULT')\n",
        "    encoder_blocks = _get_encoder_blocks(model)\n",
        "    encoder_channels = [64, 128, 256, 512, 1024]  # vgg16 channels\n",
        "    # prev_channels = encoder_channels[-1]\n",
        "\n",
        "    return UNet(encoder_blocks, encoder_channels, n_cls)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "U_8l2-gnG09S"
      },
      "outputs": [],
      "source": [
        "# Importación de librerías\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torch.nn import DataParallel\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torchvision.io import read_image, ImageReadMode\n",
        "from torchvision.datasets.vision import VisionDataset\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50, DeepLabV3_ResNet50_Weights\n",
        "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
        "from torchvision.utils import draw_segmentation_masks\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "\n",
        "import requests\n",
        "import copy\n",
        "#from unet_model import construct_unet\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from imutils.paths import list_images\n",
        "import os\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "u-13tOJejCxA",
        "outputId": "8527a4ce-360c-4614-a7c6-49a9590de203"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pv-vision\n",
            "  Downloading pv_vision-0.2.8-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: imutils>=0.5.4 in /usr/local/lib/python3.10/dist-packages (from pv-vision) (0.5.4)\n",
            "Collecting ipywidgets>=8.1.2 (from pv-vision)\n",
            "  Downloading ipywidgets-8.1.3-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pv-vision) (1.4.2)\n",
            "Collecting matplotlib>=3.8.0 (from pv-vision)\n",
            "  Downloading matplotlib-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: opencv-python>=4.7.0.72 in /usr/local/lib/python3.10/dist-packages (from pv-vision) (4.10.0.84)\n",
            "Requirement already satisfied: scikit-learn>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pv-vision) (1.3.2)\n",
            "Requirement already satisfied: setuptools>=68.2.2 in /usr/local/lib/python3.10/dist-packages (from pv-vision) (71.0.4)\n",
            "Requirement already satisfied: torch>=2.2.0.post100 in /usr/local/lib/python3.10/dist-packages (from pv-vision) (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.15.2a0 in /usr/local/lib/python3.10/dist-packages (from pv-vision) (0.18.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from pv-vision) (4.66.4)\n",
            "Collecting comm>=0.1.3 (from ipywidgets>=8.1.2->pv-vision)\n",
            "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.1.2->pv-vision) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.1.2->pv-vision) (5.7.1)\n",
            "Collecting widgetsnbextension~=4.0.11 (from ipywidgets>=8.1.2->pv-vision)\n",
            "  Downloading widgetsnbextension-4.0.11-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.1.2->pv-vision) (3.0.11)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.8.0->pv-vision) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.8.0->pv-vision) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.8.0->pv-vision) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.8.0->pv-vision) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.8.0->pv-vision) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.8.0->pv-vision) (24.1)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.8.0->pv-vision) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.8.0->pv-vision) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.8.0->pv-vision) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->pv-vision) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->pv-vision) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0.post100->pv-vision) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0.post100->pv-vision) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0.post100->pv-vision) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0.post100->pv-vision) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0.post100->pv-vision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0.post100->pv-vision) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.2.0.post100->pv-vision)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.2.0.post100->pv-vision)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.2.0.post100->pv-vision)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.2.0.post100->pv-vision)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.2.0.post100->pv-vision)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.2.0.post100->pv-vision)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.2.0.post100->pv-vision)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.2.0.post100->pv-vision)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.2.0.post100->pv-vision)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2.2.0.post100->pv-vision)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.2.0.post100->pv-vision)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0.post100->pv-vision) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.2.0.post100->pv-vision)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting jedi>=0.16 (from ipython>=6.1.0->ipywidgets>=8.1.2->pv-vision)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->pv-vision) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->pv-vision) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->pv-vision) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->pv-vision) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->pv-vision) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->pv-vision) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->pv-vision) (4.9.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.8.0->pv-vision) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.2.0.post100->pv-vision) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.2.0.post100->pv-vision) (1.3.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.1.2->pv-vision) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.1.2->pv-vision) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.1.2->pv-vision) (0.2.13)\n",
            "Downloading pv_vision-0.2.8-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.3/48.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipywidgets-8.1.3-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Downloading widgetsnbextension-4.0.11-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: widgetsnbextension, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jedi, comm, nvidia-cusparse-cu12, nvidia-cudnn-cu12, matplotlib, nvidia-cusolver-cu12, ipywidgets, pv-vision\n",
            "  Attempting uninstall: widgetsnbextension\n",
            "    Found existing installation: widgetsnbextension 3.6.7\n",
            "    Uninstalling widgetsnbextension-3.6.7:\n",
            "      Successfully uninstalled widgetsnbextension-3.6.7\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.7.1\n",
            "    Uninstalling matplotlib-3.7.1:\n",
            "      Successfully uninstalled matplotlib-3.7.1\n",
            "  Attempting uninstall: ipywidgets\n",
            "    Found existing installation: ipywidgets 7.7.1\n",
            "    Uninstalling ipywidgets-7.7.1:\n",
            "      Successfully uninstalled ipywidgets-7.7.1\n",
            "Successfully installed comm-0.2.2 ipywidgets-8.1.3 jedi-0.19.1 matplotlib-3.9.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 pv-vision-0.2.8 widgetsnbextension-4.0.11\n"
          ]
        }
      ],
      "source": [
        "# Importación de la librería de pv-vision\n",
        "!pip install pv-vision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YVtXGzixG09T"
      },
      "outputs": [],
      "source": [
        "# Importar el manejador de modelo: ModelHandler\n",
        "from pv_vision.nn import ModelHandler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ia6yr7DDG09T"
      },
      "outputs": [],
      "source": [
        "# Definición de una clase para el conjunto de datos solar,\n",
        "# que hereda de la clase VisionDataset de PyTorch.\n",
        "class SolarDataset(VisionDataset):\n",
        "    \"\"\"Un conjunto de datos que lee directamente las imágenes y las máscaras desde una carpeta.\"\"\"\n",
        "\n",
        "    # Se definió el método de inicialización para la clase.\n",
        "    def __init__(self,\n",
        "                 root,\n",
        "                 image_folder,\n",
        "                 mask_folder,\n",
        "                 transforms,\n",
        "                 mode = \"train\",\n",
        "                 random_seed=42):\n",
        "        # Se llamó al método de inicialización de la clase padre.\n",
        "        super().__init__(root, transforms)\n",
        "        # Se establecieron las rutas a las carpetas de imágenes y máscaras.\n",
        "        self.image_path = Path(self.root) / image_folder\n",
        "        self.mask_path = Path(self.root) / mask_folder\n",
        "\n",
        "        # Se verificó que las carpetas de imágenes y máscaras existan.\n",
        "        if not os.path.exists(self.image_path):\n",
        "            raise OSError(f\"{self.image_path} no encontrado.\")\n",
        "\n",
        "        if not os.path.exists(self.mask_path):\n",
        "            raise OSError(f\"{self.mask_path} no encontrado.\")\n",
        "\n",
        "        # Se obtuvieron las listas de imágenes y máscaras y se ordenaron.\n",
        "        self.image_list = sorted(list(list_images(self.image_path)))\n",
        "        self.mask_list = sorted(list(list_images(self.mask_path)))\n",
        "\n",
        "        # Se convirtieron las listas de imágenes y máscaras a arrays de numpy.\n",
        "        self.image_list = np.array(self.image_list)\n",
        "        self.mask_list = np.array(self.mask_list)\n",
        "\n",
        "        # Se estableció la semilla para la generación de números aleatorios y se mezclaron las imágenes y las máscaras.\n",
        "        np.random.seed(random_seed)\n",
        "        index = np.arange(len(self.image_list))\n",
        "        np.random.shuffle(index)\n",
        "        self.image_list = self.image_list[index]\n",
        "        self.mask_list = self.mask_list[index]\n",
        "\n",
        "    # Se definió el método para obtener la longitud del conjunto de datos.\n",
        "    def __len__(self):\n",
        "        return len(self.image_list)\n",
        "\n",
        "    # Se definió un método para obtener el nombre de una imagen o máscara.\n",
        "    def __getname__(self, index):\n",
        "        image_name = os.path.splitext(os.path.split(self.image_list[index])[-1])[0]\n",
        "        mask_name = os.path.splitext(os.path.split(self.mask_list[index])[-1])[0]\n",
        "\n",
        "        if image_name == mask_name:\n",
        "            return image_name\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    # Se definió un método para obtener una imagen y su máscara correspondiente.\n",
        "    def __getraw__(self, index):\n",
        "        if not self.__getname__(index):\n",
        "            raise ValueError(\"{}: La imagen no coincide con la máscara\".format(os.path.split(self.image_list[index])[-1]))\n",
        "        image = Image.open(self.image_list[index])\n",
        "        mask = Image.open(self.mask_list[index]).convert('L')\n",
        "        mask = np.array(mask)\n",
        "        mask = Image.fromarray(mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "    # Se definió el método para obtener un elemento del conjunto de datos.\n",
        "    def __getitem__(self, index):\n",
        "        image, mask = self.__getraw__(index)\n",
        "        image, mask = self.transforms(image, mask)\n",
        "\n",
        "        return image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "t1nDW9d6G09T"
      },
      "outputs": [],
      "source": [
        "# Definición de una clase para componer varias transformaciones.\n",
        "class Compose:\n",
        "    def __init__(self, transforms):\n",
        "        \"\"\"\n",
        "        transforms: una lista de transformaciones\n",
        "        \"\"\"\n",
        "        self.transforms = transforms\n",
        "\n",
        "    # Se definió el método para aplicar las transformaciones a la imagen y la máscara.\n",
        "    def __call__(self, image, target):\n",
        "        \"\"\"\n",
        "        image: imagen de entrada\n",
        "        target: máscara de entrada\n",
        "        \"\"\"\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target\n",
        "\n",
        "# Se definió una clase para redimensionar la imagen y la máscara a un tamaño fijo.\n",
        "class FixResize:\n",
        "    # UNet requiere que el tamaño de entrada sea múltiplo de 16\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "\n",
        "    # Se definió el método para redimensionar la imagen y la máscara.\n",
        "    def __call__(self, image, target):\n",
        "        image = F.resize(image, (self.size, self.size), interpolation=transforms.InterpolationMode.BILINEAR)\n",
        "        target = F.resize(target, (self.size, self.size), interpolation=transforms.InterpolationMode.NEAREST)\n",
        "        return image, target\n",
        "\n",
        "# Se definió una clase para transformar la imagen y la máscara a tensores.\n",
        "class ToTensor:\n",
        "    \"\"\"Transforma la imagen a tensor. Escala la imagen a [0,1] float32.\n",
        "    Transforma la máscara a tensor.\n",
        "    \"\"\"\n",
        "    def __call__(self, image, target):\n",
        "        image = transforms.ToTensor()(image)\n",
        "        target = torch.as_tensor(np.array(target), dtype=torch.int64)\n",
        "        return image, target\n",
        "\n",
        "# Se definió una clase para transformar la imagen a tensor manteniendo el tipo original.\n",
        "class PILToTensor:\n",
        "    \"\"\"Transforma la imagen a tensor. Mantiene el tipo original.\"\"\"\n",
        "    def __call__(self, image, target):\n",
        "        image = F.pil_to_tensor(image)\n",
        "        target = torch.as_tensor(np.array(target), dtype=torch.int64)\n",
        "        return image, target\n",
        "\n",
        "# Se definió una clase para normalizar la imagen.\n",
        "class Normalize:\n",
        "    def __init__(self, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        # Verifica si la imagen es en escala de grises (1 canal) y la convierte a RGB (3 canales) si es necesario\n",
        "        if image.shape[0] == 1:\n",
        "            image = image.repeat(3, 1, 1)  # Repite el canal existente 3 veces\n",
        "        image = F.normalize(image, mean=self.mean, std=self.std)\n",
        "        return image, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRAdQ8o1G09U",
        "outputId": "755665a8-0616-4654-e9f6-8cfb7ca40f30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El conjunto de datos de entrenamiento contiene 1453 elementos.\n"
          ]
        }
      ],
      "source": [
        "# Ruta al directorio que contiene las imágenes y las máscaras.\n",
        "# root = Path(\n",
        "#     '/content/drive/MyDrive/Trabajo de titulación/PV_vision/Entrenamiento')\n",
        "\n",
        "root = Path(\n",
        "    '/content/drive/MyDrive/Entrenamiento')\n",
        "\n",
        "# Se definen las transformaciones a aplicar a las imágenes y las etiquetas.\n",
        "#transformers = Compose([transforms.RandomRotation(degrees=30), FixResize(256), ToTensor(), Normalize()])\n",
        "transformers = Compose([FixResize(256), ToTensor(), Normalize()])\n",
        "# Se crean los conjuntos de datos de entrenamiento, validación y prueba.\n",
        "trainset = SolarDataset(root, image_folder=\"train/img\",\n",
        "        mask_folder=\"train/ann\", transforms=transformers)\n",
        "\n",
        "valset = SolarDataset(root, image_folder=\"val/img\",\n",
        "        mask_folder=\"val/ann\", transforms=transformers)\n",
        "\n",
        "testset = SolarDataset(root, image_folder=\"test/img\",\n",
        "        mask_folder=\"test/ann\", transforms=transformers)\n",
        "\n",
        "# Verificación de que la carpeta haya sido establecida correctamente\n",
        "print(f\"El conjunto de datos de entrenamiento contiene {len(trainset)} elementos.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhN5cKIpjCxD"
      },
      "outputs": [],
      "source": [
        "class Accuracy:\n",
        "    \"\"\"Calcular la precisión de un modelo\"\"\"\n",
        "    def __init__(self):\n",
        "        self.__name__ = \"accuracy\"\n",
        "        self._base = [0, 0]\n",
        "\n",
        "    def calc(self, outputs, targets, reduction='mean'):\n",
        "        \"\"\" Calcular la precisión.\n",
        "        Argumentos:\n",
        "        -----------\n",
        "        outputs: torch.Tensor\n",
        "        La salida del modelo, forma (batch_size, num_classes, H, W)\n",
        "\n",
        "        targets: torch.Tensor\n",
        "        La etiqueta verdadera, forma (batch_size, H, W)\n",
        "\n",
        "        reduction: str\n",
        "        El método de reducción, 'mean' o 'sum'\n",
        "        Si es 'mean', devuelve la precisión media del lote\n",
        "        Si es 'sum', devuelve la suma de predicciones correctas del lote\n",
        "\n",
        "        Retorna:\n",
        "        --------\n",
        "        accuracy: torch.Tensor\n",
        "        \"\"\"\n",
        "        # Asegúrate de que las dimensiones de outputs y targets sean compatibles\n",
        "        if outputs.dim() == 4 and targets.dim() == 3:\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct = torch.sum(preds == targets)\n",
        "\n",
        "            if reduction == 'mean':\n",
        "                return correct.float() / targets.numel()\n",
        "            elif reduction == 'sum':\n",
        "                return correct\n",
        "            else:\n",
        "                raise ValueError(\"reduction debe ser 'mean' o 'sum'\")\n",
        "        else:\n",
        "            raise ValueError(\"Las dimensiones de outputs y targets no son compatibles\")\n",
        "\n",
        "    def accumulate(self, outputs, targets):\n",
        "        \"\"\" Acumular la métrica a lo largo de varios lotes.\"\"\"\n",
        "        if outputs.dim() == 4 and targets.dim() == 3:\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct = torch.sum(preds == targets)\n",
        "            self._base[0] += correct\n",
        "            self._base[1] += targets.numel()\n",
        "        else:\n",
        "            raise ValueError(\"Las dimensiones de outputs y targets no son compatibles\")\n",
        "\n",
        "    def reset(self):\n",
        "        self._base = [0, 0]\n",
        "\n",
        "    def accumulated_score(self):\n",
        "        \"\"\" Devolver la puntuación acumulada en una época.\"\"\"\n",
        "        if self._base[1] == 0:\n",
        "            # advertencia de división por cero\n",
        "            warnings.warn(\"El denominador es cero, devuelve 0\", RuntimeWarning)\n",
        "            return 0\n",
        "        return self._base[0].float() / self._base[1]\n",
        "\n",
        "    def __call__(self, outputs, targets, reduction='mean'):\n",
        "        return self.calc(outputs, targets, reduction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaZs0hwDG09U"
      },
      "outputs": [],
      "source": [
        "# Se define una función para crear un modelo DeepLab preentrenado.\n",
        "def DeepLab_pretrained(num_classes):\n",
        "    # Se carga el modelo DeepLab con una arquitectura ResNet50 preentrenada.\n",
        "    deeplab = deeplabv3_resnet50(weights=DeepLabV3_ResNet50_Weights.DEFAULT)\n",
        "\n",
        "    # Se reemplaza el clasificador del modelo con un nuevo clasificador DeepLabHead.\n",
        "    # El nuevo clasificador tiene 2048 características de entrada y 'num_classes' características de salida.\n",
        "    deeplab.classifier = DeepLabHead(2048, num_classes)\n",
        "\n",
        "    # Se devuelve el modelo modificado.\n",
        "    return deeplab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TZFPZp57F3wK",
        "outputId": "21f15670-fc59-4ac8-d87d-9e0a65f025ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\" to /root/.cache/torch/hub/checkpoints/vgg16_bn-6c64b313.pth\n",
            "100%|██████████| 528M/528M [00:03<00:00, 170MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Crea una instancia del modelo U-Net con 5 canales de salida.\n",
        "# Número de canales de salida = al número de clases\n",
        "unet = construct_unet(5)\n",
        "# Se \"envuelve\" el modelo en un objeto DataParallel.\n",
        "# Esto permite que el modelo se ejecute en paralelo en múltiples GPUs, si están disponibles.\n",
        "unet = DataParallel(unet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qnmr0nyOG09U",
        "outputId": "0f7cb93d-93f2-43cf-a714-77c1c5474d14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dispositivo utilizado: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Se define el dispositivo en el que se ejecutará el modelo.\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Se imprime el dispositivo utilizado.\n",
        "print(f\"Dispositivo utilizado: {device}\")\n",
        "\n",
        "# Se crea el modelo utilizando la función DeepLab_pretrained definida anteriormente.\n",
        "# El modelo se envuelve en un objeto DataParallel para permitir el entrenamiento en múltiples GPUs si están disponibles.\n",
        "#model = DataParallel(DeepLab_pretrained(5))\n",
        "\n",
        "# Se define la función de pérdida a utilizar durante el entrenamiento.\n",
        "# En este caso, se utiliza la pérdida de entropía cruzada.\n",
        "criterion = CrossEntropyLoss()\n",
        "\n",
        "# Se define el optimizador a utilizar durante el entrenamiento. En este caso, se utiliza Adam con una tasa de aprendizaje de 0.01.\n",
        "#optimizer = Adam(model.parameters(), lr=0.01)\n",
        "optimizer = Adam(unet.parameters(), lr=0.0001)\n",
        "\n",
        "# Se define el programador de la tasa de aprendizaje a utilizar durante el entrenamiento.\n",
        "# En este caso, se utiliza un programador de paso que disminuye la tasa de aprendizaje en un factor de 0.2 cada 5 épocas.\n",
        "lr_scheduler = StepLR(optimizer, step_size=2, gamma=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qouTmOWmA8ng",
        "outputId": "ed833328-4e25-4d27-fdb5-ad360734de87"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Cargar los pesos del modelo preentrenado\n",
        "\n",
        "weight_path = '/content/drive/MyDrive/Entrenamiento/unetv16.pt'\n",
        "unet.load_state_dict(torch.load(weight_path, map_location=device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjJv6uo4G09V",
        "outputId": "17e7bdb8-9fd4-470d-c89a-ab097b88103f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pv_vision.nn.modelhandler:ModelHandler initialized.\n"
          ]
        }
      ],
      "source": [
        "# Se inicializa el manejador del modelo.\n",
        "# La salida se almacena en la carpeta de salida.\n",
        "modelhandler = ModelHandler(\n",
        "    # Se pasa el modelo que se va a entrenar.\n",
        "    #model=model,\n",
        "    model = unet,\n",
        "    # Se especifica el nombre de la carpeta de salida.\n",
        "    #model_output='out_unet',\n",
        "    # Se pasan los conjuntos de datos de entrenamiento, validación y prueba.\n",
        "    train_dataset=trainset,\n",
        "    val_dataset=valset,\n",
        "    test_dataset=testset,\n",
        "    # Se especifica el tamaño del lote para el entrenamiento y la validación.\n",
        "    batch_size_train=32,\n",
        "    batch_size_val=32,\n",
        "    # Se pasa el programador de la tasa de aprendizaje.\n",
        "    lr_scheduler=lr_scheduler,\n",
        "    # Se especifica el número de épocas para el entrenamiento.\n",
        "    num_epochs=30,\n",
        "    # Se pasa la función de pérdida y el optimizador.\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    # Se pasa el dispositivo en el que se ejecutará el entrenamiento.\n",
        "    device=device,\n",
        "    #evaluate_metric= Precision,\n",
        "    # Se especifica el directorio donde se guardarán los puntos de control del modelo.\n",
        "    save_dir='/content/drive/MyDrive/Entrenamiento/checkpoints',\n",
        "    # Se especifica el nombre del archivo de punto de control.\n",
        "    save_name='unetv20.pt'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1SfRwQCG09V",
        "outputId": "ce10ead4-3713-44fd-b053-7718de86ffa7",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 30\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 1 [0/1453 (0%)]\tLoss: 0.042293\n",
            " 22%|██▏       | 10/46 [02:12<06:45, 11.26s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 1 [320/1453 (22%)]\tLoss: 0.050394\n",
            " 43%|████▎     | 20/46 [04:09<05:03, 11.66s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 1 [640/1453 (43%)]\tLoss: 0.045130\n",
            " 65%|██████▌   | 30/46 [06:08<03:06, 11.68s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 1 [960/1453 (65%)]\tLoss: 0.037558\n",
            " 87%|████████▋ | 40/46 [08:04<01:09, 11.58s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 1 [1280/1453 (87%)]\tLoss: 0.042031\n",
            "100%|██████████| 46/46 [09:08<00:00, 11.92s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 1\n",
            "100%|██████████| 3/3 [01:07<00:00, 22.37s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 1 \tAverage loss: 0.0861\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0445 (train) | 0.0861 (val)\n",
            "Epoch 2 / 30\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 2 [0/1453 (0%)]\tLoss: 0.040744\n",
            " 22%|██▏       | 10/46 [00:11<00:42,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 2 [320/1453 (22%)]\tLoss: 0.034457\n",
            " 43%|████▎     | 20/46 [00:23<00:30,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 2 [640/1453 (43%)]\tLoss: 0.040248\n",
            " 65%|██████▌   | 30/46 [00:35<00:19,  1.19s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 2 [960/1453 (65%)]\tLoss: 0.045510\n",
            " 87%|████████▋ | 40/46 [00:47<00:07,  1.20s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 2 [1280/1453 (87%)]\tLoss: 0.048837\n",
            "100%|██████████| 46/46 [00:55<00:00,  1.20s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 2\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.47it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 2 \tAverage loss: 0.0858\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0446 (train) | 0.0858 (val)\n",
            "Epoch 3 / 30\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 3 [0/1453 (0%)]\tLoss: 0.050889\n",
            " 22%|██▏       | 10/46 [00:11<00:42,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 3 [320/1453 (22%)]\tLoss: 0.035583\n",
            " 43%|████▎     | 20/46 [00:23<00:30,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 3 [640/1453 (43%)]\tLoss: 0.048593\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 3 [960/1453 (65%)]\tLoss: 0.038737\n",
            " 87%|████████▋ | 40/46 [00:47<00:07,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 3 [1280/1453 (87%)]\tLoss: 0.040902\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.19s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 3\n",
            "100%|██████████| 3/3 [00:01<00:00,  1.51it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 3 \tAverage loss: 0.0858\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0440 (train) | 0.0858 (val)\n",
            "Epoch 4 / 30\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 4 [0/1453 (0%)]\tLoss: 0.044474\n",
            " 22%|██▏       | 10/46 [00:11<00:42,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 4 [320/1453 (22%)]\tLoss: 0.061229\n",
            " 43%|████▎     | 20/46 [00:23<00:30,  1.19s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 4 [640/1453 (43%)]\tLoss: 0.039589\n",
            " 65%|██████▌   | 30/46 [00:35<00:19,  1.19s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 4 [960/1453 (65%)]\tLoss: 0.033150\n",
            " 87%|████████▋ | 40/46 [00:48<00:07,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 4 [1280/1453 (87%)]\tLoss: 0.046214\n",
            "100%|██████████| 46/46 [00:55<00:00,  1.20s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 4\n",
            "100%|██████████| 3/3 [00:01<00:00,  1.52it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 4 \tAverage loss: 0.0857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0439 (train) | 0.0857 (val)\n",
            "Epoch 5 / 30\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 5 [0/1453 (0%)]\tLoss: 0.038694\n",
            " 22%|██▏       | 10/46 [00:11<00:42,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 5 [320/1453 (22%)]\tLoss: 0.042379\n",
            " 43%|████▎     | 20/46 [00:23<00:30,  1.19s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 5 [640/1453 (43%)]\tLoss: 0.039986\n",
            " 65%|██████▌   | 30/46 [00:35<00:19,  1.20s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 5 [960/1453 (65%)]\tLoss: 0.031503\n",
            " 87%|████████▋ | 40/46 [00:47<00:07,  1.19s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 5 [1280/1453 (87%)]\tLoss: 0.031464\n",
            "100%|██████████| 46/46 [00:55<00:00,  1.20s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 5\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.50it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 5 \tAverage loss: 0.0858\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0441 (train) | 0.0858 (val)\n",
            "Epoch 6 / 30\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 6 [0/1453 (0%)]\tLoss: 0.037694\n",
            " 22%|██▏       | 10/46 [00:11<00:42,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 6 [320/1453 (22%)]\tLoss: 0.031930\n",
            " 43%|████▎     | 20/46 [00:23<00:30,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 6 [640/1453 (43%)]\tLoss: 0.043109\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.19s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 6 [960/1453 (65%)]\tLoss: 0.039626\n",
            " 87%|████████▋ | 40/46 [00:47<00:07,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 6 [1280/1453 (87%)]\tLoss: 0.036351\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.19s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 6\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.50it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 6 \tAverage loss: 0.0857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0440 (train) | 0.0857 (val)\n",
            "Epoch 7 / 30\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 7 [0/1453 (0%)]\tLoss: 0.034660\n",
            " 22%|██▏       | 10/46 [00:11<00:42,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 7 [320/1453 (22%)]\tLoss: 0.045872\n",
            " 43%|████▎     | 20/46 [00:23<00:30,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 7 [640/1453 (43%)]\tLoss: 0.036650\n",
            " 65%|██████▌   | 30/46 [00:35<00:19,  1.19s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 7 [960/1453 (65%)]\tLoss: 0.035644\n",
            " 87%|████████▋ | 40/46 [00:47<00:07,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 7 [1280/1453 (87%)]\tLoss: 0.042439\n",
            "100%|██████████| 46/46 [00:55<00:00,  1.20s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 7\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.49it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 7 \tAverage loss: 0.0858\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0439 (train) | 0.0858 (val)\n",
            "Epoch 8 / 30\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 8 [0/1453 (0%)]\tLoss: 0.052110\n",
            " 22%|██▏       | 10/46 [00:11<00:42,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 8 [320/1453 (22%)]\tLoss: 0.035632\n",
            " 43%|████▎     | 20/46 [00:23<00:30,  1.19s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 8 [640/1453 (43%)]\tLoss: 0.041429\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 8 [960/1453 (65%)]\tLoss: 0.040502\n",
            " 87%|████████▋ | 40/46 [00:47<00:07,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 8 [1280/1453 (87%)]\tLoss: 0.039579\n",
            "100%|██████████| 46/46 [00:55<00:00,  1.20s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 8\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.48it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 8 \tAverage loss: 0.0856\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0441 (train) | 0.0856 (val)\n",
            "Epoch 9 / 30\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 9 [0/1453 (0%)]\tLoss: 0.043054\n",
            " 22%|██▏       | 10/46 [00:11<00:42,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 9 [320/1453 (22%)]\tLoss: 0.052024\n",
            " 43%|████▎     | 20/46 [00:23<00:30,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 9 [640/1453 (43%)]\tLoss: 0.038794\n",
            " 65%|██████▌   | 30/46 [00:35<00:19,  1.20s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 9 [960/1453 (65%)]\tLoss: 0.051298\n",
            " 87%|████████▋ | 40/46 [00:47<00:07,  1.19s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 9 [1280/1453 (87%)]\tLoss: 0.030773\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.19s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 9\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.46it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 9 \tAverage loss: 0.0856\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0440 (train) | 0.0856 (val)\n",
            "Epoch 10 / 30\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 10 [0/1453 (0%)]\tLoss: 0.037224\n",
            " 22%|██▏       | 10/46 [00:11<00:42,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 10 [320/1453 (22%)]\tLoss: 0.050625\n",
            " 43%|████▎     | 20/46 [00:23<00:30,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 10 [640/1453 (43%)]\tLoss: 0.039545\n",
            " 65%|██████▌   | 30/46 [00:35<00:19,  1.19s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 10 [960/1453 (65%)]\tLoss: 0.043853\n",
            " 87%|████████▋ | 40/46 [00:47<00:07,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 10 [1280/1453 (87%)]\tLoss: 0.037857\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.19s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 10\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.49it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 10 \tAverage loss: 0.0856\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0442 (train) | 0.0856 (val)\n",
            "Epoch 11 / 30\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 11 [0/1453 (0%)]\tLoss: 0.051730\n",
            " 22%|██▏       | 10/46 [00:11<00:42,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 11 [320/1453 (22%)]\tLoss: 0.049040\n",
            " 43%|████▎     | 20/46 [00:23<00:30,  1.19s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 11 [640/1453 (43%)]\tLoss: 0.039724\n",
            " 65%|██████▌   | 30/46 [00:35<00:19,  1.19s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 11 [960/1453 (65%)]\tLoss: 0.031468\n",
            " 87%|████████▋ | 40/46 [00:47<00:07,  1.19s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 11 [1280/1453 (87%)]\tLoss: 0.043101\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.20s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 11\n",
            "100%|██████████| 3/3 [00:01<00:00,  1.50it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 11 \tAverage loss: 0.0857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0441 (train) | 0.0857 (val)\n",
            "Epoch 12 / 30\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 12 [0/1453 (0%)]\tLoss: 0.038309\n",
            " 22%|██▏       | 10/46 [00:11<00:42,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 12 [320/1453 (22%)]\tLoss: 0.051501\n",
            " 43%|████▎     | 20/46 [00:23<00:30,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 12 [640/1453 (43%)]\tLoss: 0.045557\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 12 [960/1453 (65%)]\tLoss: 0.035033\n",
            " 87%|████████▋ | 40/46 [00:47<00:07,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 12 [1280/1453 (87%)]\tLoss: 0.036395\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.19s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 12\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.50it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 12 \tAverage loss: 0.0857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0440 (train) | 0.0857 (val)\n",
            "Epoch 13 / 30\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 13 [0/1453 (0%)]\tLoss: 0.049608\n",
            " 22%|██▏       | 10/46 [00:11<00:42,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 13 [320/1453 (22%)]\tLoss: 0.042917\n",
            " 43%|████▎     | 20/46 [00:23<00:30,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 13 [640/1453 (43%)]\tLoss: 0.045217\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.19s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 13 [960/1453 (65%)]\tLoss: 0.029331\n",
            " 87%|████████▋ | 40/46 [00:47<00:07,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 13 [1280/1453 (87%)]\tLoss: 0.042570\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.19s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 13\n",
            "100%|██████████| 3/3 [00:01<00:00,  1.50it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 13 \tAverage loss: 0.0858\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0440 (train) | 0.0858 (val)\n",
            "Epoch 14 / 30\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 14 [0/1453 (0%)]\tLoss: 0.046591\n",
            " 22%|██▏       | 10/46 [00:11<00:42,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 14 [320/1453 (22%)]\tLoss: 0.049328\n",
            " 43%|████▎     | 20/46 [00:23<00:30,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 14 [640/1453 (43%)]\tLoss: 0.035893\n",
            " 65%|██████▌   | 30/46 [00:35<00:19,  1.19s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 14 [960/1453 (65%)]\tLoss: 0.053473\n",
            " 87%|████████▋ | 40/46 [00:47<00:07,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 14 [1280/1453 (87%)]\tLoss: 0.036181\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.20s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 14\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.48it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 14 \tAverage loss: 0.0859\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0439 (train) | 0.0859 (val)\n",
            "Epoch 15 / 30\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 15 [0/1453 (0%)]\tLoss: 0.043857\n",
            " 22%|██▏       | 10/46 [00:11<00:42,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 15 [320/1453 (22%)]\tLoss: 0.034596\n",
            " 43%|████▎     | 20/46 [00:23<00:30,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 15 [640/1453 (43%)]\tLoss: 0.061662\n",
            " 65%|██████▌   | 30/46 [00:35<00:19,  1.19s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 15 [960/1453 (65%)]\tLoss: 0.041125\n",
            " 87%|████████▋ | 40/46 [00:47<00:07,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 15 [1280/1453 (87%)]\tLoss: 0.055154\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.19s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 15\n",
            "100%|██████████| 3/3 [00:01<00:00,  1.50it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 15 \tAverage loss: 0.0857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0441 (train) | 0.0857 (val)\n",
            "Epoch 16 / 30\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 16 [0/1453 (0%)]\tLoss: 0.047883\n",
            " 22%|██▏       | 10/46 [00:11<00:42,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 16 [320/1453 (22%)]\tLoss: 0.031502\n",
            " 43%|████▎     | 20/46 [00:23<00:30,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 16 [640/1453 (43%)]\tLoss: 0.039822\n",
            " 65%|██████▌   | 30/46 [00:35<00:19,  1.19s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 16 [960/1453 (65%)]\tLoss: 0.052194\n",
            " 87%|████████▋ | 40/46 [00:47<00:07,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 16 [1280/1453 (87%)]\tLoss: 0.045180\n",
            "100%|██████████| 46/46 [00:55<00:00,  1.20s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 16\n",
            "100%|██████████| 3/3 [00:01<00:00,  1.51it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 16 \tAverage loss: 0.0855\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0439 (train) | 0.0855 (val)\n",
            "Epoch 17 / 30\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 17 [0/1453 (0%)]\tLoss: 0.039248\n",
            " 22%|██▏       | 10/46 [00:11<00:42,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 17 [320/1453 (22%)]\tLoss: 0.039934\n",
            " 43%|████▎     | 20/46 [00:23<00:30,  1.19s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 17 [640/1453 (43%)]\tLoss: 0.048707\n",
            " 65%|██████▌   | 30/46 [00:35<00:19,  1.19s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 17 [960/1453 (65%)]\tLoss: 0.051430\n",
            " 87%|████████▋ | 40/46 [00:47<00:07,  1.19s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 17 [1280/1453 (87%)]\tLoss: 0.047959\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.20s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 17\n",
            "100%|██████████| 3/3 [00:01<00:00,  1.52it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 17 \tAverage loss: 0.0856\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0440 (train) | 0.0856 (val)\n",
            "Epoch 18 / 30\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 18 [0/1453 (0%)]\tLoss: 0.032256\n",
            " 22%|██▏       | 10/46 [00:11<00:42,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 18 [320/1453 (22%)]\tLoss: 0.050025\n",
            " 43%|████▎     | 20/46 [00:23<00:30,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 18 [640/1453 (43%)]\tLoss: 0.044809\n",
            " 65%|██████▌   | 30/46 [00:35<00:19,  1.20s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 18 [960/1453 (65%)]\tLoss: 0.040933\n",
            " 87%|████████▋ | 40/46 [00:47<00:07,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 18 [1280/1453 (87%)]\tLoss: 0.054045\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.20s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 18\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.49it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 18 \tAverage loss: 0.0856\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0440 (train) | 0.0856 (val)\n",
            "Epoch 19 / 30\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 19 [0/1453 (0%)]\tLoss: 0.038541\n",
            " 22%|██▏       | 10/46 [00:11<00:42,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 19 [320/1453 (22%)]\tLoss: 0.044588\n",
            " 43%|████▎     | 20/46 [00:23<00:30,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 19 [640/1453 (43%)]\tLoss: 0.039068\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 19 [960/1453 (65%)]\tLoss: 0.037999\n",
            " 87%|████████▋ | 40/46 [00:47<00:07,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 19 [1280/1453 (87%)]\tLoss: 0.045303\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.19s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 19\n",
            "100%|██████████| 3/3 [00:01<00:00,  1.50it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 19 \tAverage loss: 0.0860\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0443 (train) | 0.0860 (val)\n",
            "Epoch 20 / 30\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 20 [0/1453 (0%)]\tLoss: 0.039929\n",
            " 22%|██▏       | 10/46 [00:11<00:42,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 20 [320/1453 (22%)]\tLoss: 0.051341\n",
            " 43%|████▎     | 20/46 [00:23<00:30,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 20 [640/1453 (43%)]\tLoss: 0.045735\n",
            " 65%|██████▌   | 30/46 [00:35<00:19,  1.20s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 20 [960/1453 (65%)]\tLoss: 0.045114\n",
            " 87%|████████▋ | 40/46 [00:47<00:07,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 20 [1280/1453 (87%)]\tLoss: 0.047426\n",
            "100%|██████████| 46/46 [00:55<00:00,  1.20s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 20\n",
            "100%|██████████| 3/3 [00:01<00:00,  1.50it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 20 \tAverage loss: 0.0858\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0441 (train) | 0.0858 (val)\n",
            "Epoch 21 / 30\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 21 [0/1453 (0%)]\tLoss: 0.050622\n",
            " 22%|██▏       | 10/46 [00:11<00:42,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 21 [320/1453 (22%)]\tLoss: 0.042261\n",
            " 43%|████▎     | 20/46 [00:23<00:30,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 21 [640/1453 (43%)]\tLoss: 0.053993\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 21 [960/1453 (65%)]\tLoss: 0.054850\n",
            " 87%|████████▋ | 40/46 [00:47<00:07,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 21 [1280/1453 (87%)]\tLoss: 0.037391\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.19s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 21\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.50it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 21 \tAverage loss: 0.0856\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0440 (train) | 0.0856 (val)\n",
            "Epoch 22 / 30\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 22 [0/1453 (0%)]\tLoss: 0.044950\n",
            " 22%|██▏       | 10/46 [00:11<00:42,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 22 [320/1453 (22%)]\tLoss: 0.045990\n",
            " 43%|████▎     | 20/46 [00:23<00:30,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 22 [640/1453 (43%)]\tLoss: 0.043122\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.19s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 22 [960/1453 (65%)]\tLoss: 0.040978\n",
            " 87%|████████▋ | 40/46 [00:47<00:07,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 22 [1280/1453 (87%)]\tLoss: 0.034572\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.19s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 22\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.47it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 22 \tAverage loss: 0.0857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0438 (train) | 0.0857 (val)\n",
            "Epoch 23 / 30\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 23 [0/1453 (0%)]\tLoss: 0.047876\n",
            " 22%|██▏       | 10/46 [00:11<00:42,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 23 [320/1453 (22%)]\tLoss: 0.041805\n",
            " 43%|████▎     | 20/46 [00:23<00:30,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 23 [640/1453 (43%)]\tLoss: 0.042155\n",
            " 65%|██████▌   | 30/46 [00:35<00:19,  1.20s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 23 [960/1453 (65%)]\tLoss: 0.036205\n",
            " 87%|████████▋ | 40/46 [00:47<00:07,  1.19s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 23 [1280/1453 (87%)]\tLoss: 0.049388\n",
            "100%|██████████| 46/46 [00:55<00:00,  1.20s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 23\n",
            "100%|██████████| 3/3 [00:01<00:00,  1.51it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 23 \tAverage loss: 0.0857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0439 (train) | 0.0857 (val)\n",
            "Epoch 24 / 30\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 24 [0/1453 (0%)]\tLoss: 0.037363\n",
            " 22%|██▏       | 10/46 [00:11<00:42,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 24 [320/1453 (22%)]\tLoss: 0.052819\n",
            " 43%|████▎     | 20/46 [00:23<00:30,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 24 [640/1453 (43%)]\tLoss: 0.054562\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 24 [960/1453 (65%)]\tLoss: 0.042745\n",
            " 87%|████████▋ | 40/46 [00:47<00:07,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 24 [1280/1453 (87%)]\tLoss: 0.039498\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.19s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 24\n",
            "100%|██████████| 3/3 [00:01<00:00,  1.52it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 24 \tAverage loss: 0.0857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0440 (train) | 0.0857 (val)\n",
            "Epoch 25 / 30\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 25 [0/1453 (0%)]\tLoss: 0.038180\n",
            " 22%|██▏       | 10/46 [00:11<00:42,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 25 [320/1453 (22%)]\tLoss: 0.049649\n",
            " 43%|████▎     | 20/46 [00:23<00:30,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 25 [640/1453 (43%)]\tLoss: 0.038883\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.19s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 25 [960/1453 (65%)]\tLoss: 0.060322\n",
            " 87%|████████▋ | 40/46 [00:47<00:07,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 25 [1280/1453 (87%)]\tLoss: 0.033906\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.19s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 25\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.49it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 25 \tAverage loss: 0.0857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0441 (train) | 0.0857 (val)\n",
            "Epoch 26 / 30\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 26 [0/1453 (0%)]\tLoss: 0.043423\n",
            " 22%|██▏       | 10/46 [00:11<00:42,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 26 [320/1453 (22%)]\tLoss: 0.038152\n",
            " 43%|████▎     | 20/46 [00:23<00:30,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 26 [640/1453 (43%)]\tLoss: 0.043808\n",
            " 65%|██████▌   | 30/46 [00:35<00:19,  1.19s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 26 [960/1453 (65%)]\tLoss: 0.047639\n",
            " 87%|████████▋ | 40/46 [00:47<00:07,  1.19s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 26 [1280/1453 (87%)]\tLoss: 0.027521\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.19s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 26\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.50it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 26 \tAverage loss: 0.0857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0439 (train) | 0.0857 (val)\n",
            "Epoch 27 / 30\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 27 [0/1453 (0%)]\tLoss: 0.043036\n",
            " 22%|██▏       | 10/46 [00:11<00:42,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 27 [320/1453 (22%)]\tLoss: 0.062542\n",
            " 43%|████▎     | 20/46 [00:23<00:30,  1.19s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 27 [640/1453 (43%)]\tLoss: 0.042848\n",
            " 65%|██████▌   | 30/46 [00:35<00:18,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 27 [960/1453 (65%)]\tLoss: 0.045194\n",
            " 87%|████████▋ | 40/46 [00:47<00:07,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 27 [1280/1453 (87%)]\tLoss: 0.038559\n",
            "100%|██████████| 46/46 [00:55<00:00,  1.20s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 27\n",
            "100%|██████████| 3/3 [00:01<00:00,  1.51it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 27 \tAverage loss: 0.0857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0439 (train) | 0.0857 (val)\n",
            "Epoch 28 / 30\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 28 [0/1453 (0%)]\tLoss: 0.037678\n",
            " 22%|██▏       | 10/46 [00:11<00:42,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 28 [320/1453 (22%)]\tLoss: 0.028634\n",
            " 43%|████▎     | 20/46 [00:23<00:30,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 28 [640/1453 (43%)]\tLoss: 0.036882\n",
            " 65%|██████▌   | 30/46 [00:35<00:19,  1.19s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 28 [960/1453 (65%)]\tLoss: 0.050801\n",
            " 87%|████████▋ | 40/46 [00:47<00:07,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 28 [1280/1453 (87%)]\tLoss: 0.034602\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.19s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 28\n",
            "100%|██████████| 3/3 [00:01<00:00,  1.52it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 28 \tAverage loss: 0.0857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0440 (train) | 0.0857 (val)\n",
            "Epoch 29 / 30\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 29 [0/1453 (0%)]\tLoss: 0.040170\n",
            " 22%|██▏       | 10/46 [00:11<00:42,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 29 [320/1453 (22%)]\tLoss: 0.042443\n",
            " 43%|████▎     | 20/46 [00:23<00:30,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 29 [640/1453 (43%)]\tLoss: 0.043225\n",
            " 65%|██████▌   | 30/46 [00:35<00:19,  1.19s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 29 [960/1453 (65%)]\tLoss: 0.052614\n",
            " 87%|████████▋ | 40/46 [00:47<00:07,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 29 [1280/1453 (87%)]\tLoss: 0.045958\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.19s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 29\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.49it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 29 \tAverage loss: 0.0856\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0442 (train) | 0.0856 (val)\n",
            "Epoch 30 / 30\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/46 [00:00<?, ?it/s]INFO:pv_vision.nn.modelhandler:Train Epoch: 30 [0/1453 (0%)]\tLoss: 0.045958\n",
            " 22%|██▏       | 10/46 [00:11<00:42,  1.17s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 30 [320/1453 (22%)]\tLoss: 0.048943\n",
            " 43%|████▎     | 20/46 [00:23<00:30,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 30 [640/1453 (43%)]\tLoss: 0.036306\n",
            " 65%|██████▌   | 30/46 [00:35<00:19,  1.19s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 30 [960/1453 (65%)]\tLoss: 0.042848\n",
            " 87%|████████▋ | 40/46 [00:47<00:07,  1.18s/it]INFO:pv_vision.nn.modelhandler:Train Epoch: 30 [1280/1453 (87%)]\tLoss: 0.046562\n",
            "100%|██████████| 46/46 [00:54<00:00,  1.19s/it]\n",
            "INFO:pv_vision.nn.modelhandler:Saved model at epoch 30\n",
            "100%|██████████| 3/3 [00:02<00:00,  1.48it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Val epoch: 30 \tAverage loss: 0.0858\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0440 (train) | 0.0858 (val)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': {'loss': [0.044490440800550146,\n",
              "   0.04458105814093897,\n",
              "   0.04397191569152694,\n",
              "   0.04392251401821745,\n",
              "   0.044096569115961326,\n",
              "   0.04398720790949347,\n",
              "   0.04387692428217702,\n",
              "   0.04410132423238104,\n",
              "   0.04397506191454833,\n",
              "   0.04416253898197917,\n",
              "   0.044112969589844626,\n",
              "   0.044035711243020526,\n",
              "   0.044003363555144705,\n",
              "   0.04392337262712506,\n",
              "   0.04412755178106873,\n",
              "   0.04388588273619588,\n",
              "   0.0439532007060622,\n",
              "   0.04403238003613288,\n",
              "   0.04427537038994098,\n",
              "   0.04411728105316142,\n",
              "   0.04397725867439531,\n",
              "   0.04378457361010544,\n",
              "   0.04392923114735425,\n",
              "   0.044004837313829745,\n",
              "   0.04405103689501881,\n",
              "   0.0438691745502573,\n",
              "   0.04392560277357236,\n",
              "   0.04397557918379738,\n",
              "   0.04417428813992659,\n",
              "   0.043993725429217895]},\n",
              " 'val': {'loss': [0.08607834577560425,\n",
              "   0.08575832843780518,\n",
              "   0.08580511808395386,\n",
              "   0.08574531227350235,\n",
              "   0.08579817414283752,\n",
              "   0.0856501782933871,\n",
              "   0.08576500912507375,\n",
              "   0.08564803500970204,\n",
              "   0.08555196473995845,\n",
              "   0.08563197404146194,\n",
              "   0.08565634985764821,\n",
              "   0.08570839464664459,\n",
              "   0.08575982103745143,\n",
              "   0.085898756980896,\n",
              "   0.08571778486172359,\n",
              "   0.08553891877333324,\n",
              "   0.08559803664684296,\n",
              "   0.08562295138835907,\n",
              "   0.08596973369518916,\n",
              "   0.08576517800490062,\n",
              "   0.08561690151691437,\n",
              "   0.0856765906016032,\n",
              "   0.08565697073936462,\n",
              "   0.08569660037755966,\n",
              "   0.08571861932675044,\n",
              "   0.08569747457901637,\n",
              "   0.08569006125132243,\n",
              "   0.08572075267632802,\n",
              "   0.08555953701337178,\n",
              "   0.08580023050308228]}}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Se inicializa el entrenamiento del modelo.\n",
        "modelhandler.train_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "k55JhgMyG09V",
        "outputId": "cff76bff-8f7d-4e9d-9f34-0112e25216f2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Se visualiza el proceso de entrenamiento.\n",
        "# Esta función traza la pérdida del modelo durante el entrenamiento.\n",
        "modelhandler.plot_loss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E52bTEXnG09W",
        "outputId": "350ea686-00d1-4a18-c6bf-42a566692553"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# Se busca la pérdida mínima en la validación, que corresponde al mejor modelo.\n",
        "# 'np.argmin' devuelve el índice de la pérdida mínima en el conjunto de validación.\n",
        "# Se suma 1 porque los índices en Python comienzan en 0, pero las épocas comienzan en 1.\n",
        "np.argmin(modelhandler.running_record['val']['loss'])+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kH5xVXQyG09W",
        "outputId": "9be98c58-b4eb-46a2-e862-14fc78bc633f",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pv_vision.nn.modelhandler:Loaded model from /content/drive/MyDrive/Entrenamiento/checkpoints/epoch_15/unetv19.pt\n"
          ]
        }
      ],
      "source": [
        "# Se carga el mejor modelo entrenado y se verifica su rendimiento en el conjunto de prueba.\n",
        "# Se emplea `load_model` para cargar el modelo entrenado. Este método toma el nombre del archivo de punto de control.\n",
        "modelhandler.load_model('/content/drive/MyDrive/Entrenamiento/checkpoints/epoch_15/unetv20.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xa-Fdu8ZG09W"
      },
      "source": [
        "El siguiente código prueba el modelo en el conjunto de prueba y almacena la salida en 'testset_output'. También se hace un comentario sobre la puntuación de la prueba y la puntuación de la validación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1q3LEUNaG09W",
        "outputId": "1673aadd-6024-4dd2-d17d-25cc07a74366"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing mode\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:07<00:00,  1.54it/s]\n",
            "INFO:pv_vision.nn.modelhandler:Test set: Average loss: 0.1086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.1086\n"
          ]
        }
      ],
      "source": [
        "# Se evalúa el modelo en el conjunto de prueba. `test_model` es una función de ModelHandler\n",
        "# que evalúa el modelo en el conjunto de prueba y almacena la salida en la caché.\n",
        "_ = modelhandler.test_model(cache_output='testset_outputv19')\n",
        "\n",
        "# La salida del modelo se almacena en self.cache['testset_output']"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}